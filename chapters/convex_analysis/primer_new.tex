\subsection*{My Contribution}
I present the relevant facts from Convex analysis.
I prove some results that I did not find in the literature, but likely are folklore.

Throughout this section let $n\in\mathbb{N}$.
\subsection*{Sets}
A subset $C\subseteq \R^n$ is called \textbf{convex set}, 
if for all $x,y\in C$ and all $\theta\in [0,1]$,
we have 
$
  \theta x + (1-\theta)y 
  \in
  C
$.
Many set operations preserve convexity. Among them
forming the 
\textbf{Cartesian product} of two convex sets, 
\textbf{intersection} of a collection of convex sets and 
taking the \textbf{inverse image under linear functions}.

The classical theory evolves around the question 
if convex sets can be separated.
\begin{definition*}
  Let 
  $C_1$ and $C_2$
  be two non-empty convex sets in $\R^n$. 
  A hyperplane $H$ is said to \textbf{separate}
  $C_1$ and $C_2$
  if $C_1$ is contained in one of the closed half-spaces associated with
  $H$ and $C_2$ lies in the opposite closed half-space. It is said to separate 
  $C_1$ and $C_2$
  \textbf{properly} if 
  $C_1$ and $C_2$
  are not both contained in $H$.
\end{definition*}

We need a refined concept of interiors, since some convex sets have empty interior. To this end, 
  we call a set
  \index{affine set}
  $A\subseteq \R^n$ 
  \textbf{affine set}, if
  $
    \alpha x + (1-\alpha)y \in A
    \quad
    \text{for all}
    \ 
    x,y \in A
    \ 
    \text{and all}
    \ 
    \alpha \in \R
  $.
  The \textbf{affine hull} 
  \index{$\mathrm{aff(\cdot)}$, affine hull}
  $\mathrm{aff}(\Omega)$
  of a set 
  $\Omega\subseteq \R^n$
  is the smallest affine set that includes $\Omega$.
  We define the \textbf{relative interior}
  \index{$\mathrm{ri}(\cdot)$, relative interior}
  $\mathrm{ri}\,\Omega$ 
  of a set 
  $\Omega\subseteq \R^n$
  to be the interior relative to the affine hull, that is,
    \begin{gather}
    \mathrm{ri}(\Omega)
    \ 
    :=
    \ 
    \left\{ 
      x \in \Omega 
      \ 
      |
      \ 
      \exists
      \,
      \varepsilon > 0\ 
      \colon
      (
      x+\varepsilon B_{\R^n}
      )
      \cap
      \mathrm{aff}(\Omega)
      \ 
      \subset
      \ 
      \Omega
      \,
    \right\}
    \,.
  \end{gather}

\begin{ftheorem}
  \label{cv:primer:sep}
  \emph{(Convex separation in finite dimension)}
  Let $C_1$ and $C_2$ be two non-empty convex sets in $\R^n$. 
  Then $C_1$ and $C_2$ can be properly separated if and only if 
  $\mathrm{ri}(C_1)\cap\mathrm{ri}(C_2)=\emptyset.$
\end{ftheorem}
\begin{proof}
  \cite[Theorem~11.3]{Rockafellar1970}
\end{proof}
We collect some useful 
properties of relative interiors
before we get on to convex functions.
\begin{proposition}
  \label{cv:primer:prop}
  Let $C$ be a non-empty convex set in $\R^n.$ The following holds:
\begin{enumerate}[label={(\roman*)}]
  \item
    $
      \mathrm{ri}(C)
      \ 
      \neq
      \ 
      \emptyset
      $
if and only if
      $
      C
      \ 
      \neq
      \ 
      \emptyset
    $
  \item
    $
      \mathrm{cl}(\mathrm{ri}\,C)
      \ 
      =
      \ 
      \mathrm{cl}\,C
      $
      and
      $
      \mathrm{ri}(\mathrm{cl}\,C)
      \ 
      =
      \ 
      \mathrm{ri}(C)
    $
  \item
    $
    \mathrm{ri}(C)
      \ 
    =
      \ 
    \left\{ 
      z \in C
      \colon
      \text{for all}\ 
      x \in C \ 
      \text{there exists}\ 
      t > 0 \ 
      \text{such that}\ 
      z + t (z-x)
      \in C
    \right\}
    $
  \item
    Suppose
    $
      \bigcap_{i\in I} C_i
      \ 
      \neq
      \ 
      \emptyset
    $
    for a finite index set $I$.
    Then
    $
      \mathrm{ri}
      \left( 
        \bigcap_{i\in I} C_i
      \right)
      \ 
      =
      \ 
      \bigcap_{i\in I}  
      \mathrm{ri}(C_i)
    $.
    \item
      Let 
      $
        L\,:\,\R^n \to\  \R^m
      $
      be a linear function. Then
      $
        \mathrm{ri}\,L(C)
        \ 
        =
        \ 
        L(\mathrm{ri}\,C)
      $.
      If it also holds
      $
        L^{\!-1}(\mathrm{ri}\,C)
        \ 
        \neq
        \ 
        \emptyset
      $,
      we have
      $
      \mathrm{ri}\,L^{\!-1}(C)
      \ 
        =
      \ 
        L^{\!-1}(\mathrm{ri}\,C)
      $.
      \item
        $
          \mathrm{ri}(C_1\!\times C_2)
          \ 
          = 
          \ 
          \mathrm{ri}\,C_1
          \! 
          \times
          \mathrm{ri}\,C_2
        $
\end{enumerate}

\end{proposition}


\begin{proof}
  For a proof of (i)-(v) we refer to~\cite[Theorem 6.2 - 6.7]{Rockafellar1970}.

To prove (vi) we use (iii).
Let
  $
  (z_1, z_2)
  \in 
  \mathrm{ri}(C_1\!\times C_2).
  $
  Then for all 
  $
  (x_1, x_2)
  \in 
  C_1\!\times C_2
  $
  there exists
  $t>0$
  such that
  \begin{gather}
    \label{cv:primer:prop:1}
      z_i + t (z_i-x_i)
      \in C_i
      \qquad
      \text{for all}\ 
      i\in \left\{ 1,2 \right\}.
  \end{gather}
  Using (iii) again, we get
  $
  \mathrm{ri}(C_1\!\times C_2)
  \, 
  \subseteq
  \,
          \mathrm{ri}\,C_1
          \! 
          \times
          \mathrm{ri}\,C_2
  $.
  Suppose 
  $
  (z_1,z_2)
    \in
    \mathrm{ri}\,C_1
    \!
    \times
    \mathrm{ri}\,C_2
  $.
  By (iii), for all
  $
    (x_1,x_2)\in C_1\times C_2
  $
  there exist
  $
    (t_1,t_2)>0
  $
  such that
  \begin{gather}
    \label{cv:primer:prop:2}
      z_i + t_i (z_i-x_i)
      \in C_i
      \qquad
      \text{for all}\ 
      i\in \left\{ 1,2 \right\}.
  \end{gather}
  If $t_1=t_2$
  we recover
  \eqref{cv:primer:prop:1}
  from
  \eqref{cv:primer:prop:2}.
  By (iii) it holds
  $
  (z_1,z_2)
    \in
    \mathrm{ri}
    (C_1
    \!
    \times
    C_2)
  $.
  If
  $t_1<t_2$
  we
  define $\theta:=\frac{t_1}{t_2}\in (0,1).$
  Consider  
  \eqref{cv:primer:prop:2} with $i=2$,
  together with $z_2 \in C_2$
  and
  the convexity of $C_2$.
  It follows
  \begin{gather}
    \label{cv:primer:prop:3}
    z_2 + t_1 (z_2 - x_2)
    \ 
    =
    \ 
    \theta
    \cdot
    (
    z_2 + t_2 (z_2 - x_2)
    )
    \ 
    +
    \ 
    (1-\theta)
    \cdot
    z_2
    \in C_2
    \,.
  \end{gather}
  Now we consider
  \eqref{cv:primer:prop:3} and
  \eqref{cv:primer:prop:2} with $i=1$.
  This gives \eqref{cv:primer:prop:1} with $t=t_1$.
  As before, it follows
  $
  (z_1,z_2)\in\mathrm{ri}(C_1\!\times C_2)
  $.
  If 
  $t_1>t_2$
  similar arguments lead to the same result.
  We have proven 
  $
  \mathrm{ri}(C_1\!\times C_2)
  \, 
  \supseteq
  \,
          \mathrm{ri}\,C_1
          \! 
          \times
          \mathrm{ri}\,C_2
  $
  and equality.
\end{proof}
\subsection*{Functions}
A function 
$
f
\colon
\R^n
\to
\overline{\R}
$
is called \textbf{convex function}, if the area above its graph, that is, its epigraph(cf.\cite[ยง2.4.1]{Mordukhovich2022}), is convex. We shall often use an equivalent definition.
To this end, 
a function $f$ is convex if and only if 
\begin{gather}
  \label{cv:cf}
  f(\theta x + (1-\theta)y)
  \ 
  \le
  \ 
  \theta f(x)
  +
  (1-\theta)f(y)
  \qquad
  \text{for all}\ 
  x,y\in \R^n
  \ 
  \text{and all}\ 
  \theta\in[0,1]
  \,.
\end{gather}
This definition extends to convex cominbinations
$
  \theta_1,\ldots,\theta_m\in[0,1]
$
with
$
  \sum_{i=1}^{m} 
  \theta_i
  =1
$, that is, 
a function $f$ is convex if and only if 
\begin{gather}
  f
  \left( 
    \sum_{i=1}^{m} 
    \theta_i
    x_i
  \right)
  \ 
  \le
  \ 
    \sum_{i=1}^{m} 
    \theta_i
    f(x_i)
  \qquad
  \text{for all}\ 
  x_1,\ldots,x_m\in \R^n
  \,.
\end{gather}
We call a function \textbf{strictly convex} if the inequality in
\eqref{cv:cf} is strict.

We define the \textbf{domain} $\mathrm{dom}\,f$
of a convex function $f$ to be the set where $f$ is finite, that is,
\begin{gather}
  \mathrm{dom}\,f
  \ 
  :=
  \ 
  \left\{ 
x\in\R^n
:
f(x)<\infty
  \right\}
  \,.
\end{gather}
The domain of a convex function is convex. 
We say that $f$ is a \textbf{proper function} if  $\mathrm{dom}\,f\neq\emptyset$. 

For any $\overline{x}\in\mathrm{dom}\,f$ we call $x^*\in\R^n$ a 
\textbf{subgradient} of $f$ at $\overline{x}$ if for all 
$x\in\R^n$ it holds
\begin{gather}
  \inner{x^*}{x-\overline{x}}
  \le
  f(x)
  -
  f(\overline{x})
  \,.
\end{gather}
We denote the collection of all subgradients at $\overline{x}$, that is, the \textbf{subdifferential} of $f$ at $\overline{x}$, as
$
\partial f(\overline{x})
$.
If $f$ is differentiable at $\overline{x}$ it holds
$
\partial f(\overline{x})
=
\left\{ 
  \nabla
  f(\overline{x})
\right\}
$
and thus
\begin{gather}
  \label{cv:primer:mvthe}
  \inner{
  \nabla
  f(\overline{x})
}{x-\overline{x}}
  \le
  f(x)
  -
  f(\overline{x})
  \,.
\end{gather}
%We call a differentiable function $f$ \textbf{strongly convex} with parameter
%$m>0$ if for all
%$x,y\in\mathrm{dom}\,f$ it holds
%\begin{gather}
%  f(y)-f(x)
%  \ 
%  \ge
%  \ 
%  \inner
%  {
%  \nabla
%  f(x)
%  }
%  {y-x}
%  \ 
%  +
%  \ 
%  \frac{m}{2}
%  \norm{y-x}^2
%  \,.
%\end{gather}
%If $f$ is twice continuously differentiable, then it is strongly 
%convex with parameter $m>0$ if and only if the matrix
%\begin{gather}
%\nabla^2f(x)
%-m\cdot\mathbf{I}
%\qquad
%\text{
%is positive semi-definite for all
%}\ 
%x\in\mathrm{dom}\,f
%\,,
%\end{gather}
% where 
%$
%\nabla^2f
%$
%is the Hessian Matrix.
%

\begin{definition*}
  Given a nonempty subset 
  $\Omega \subseteq \R^n$,
  we define
  the \textbf{support function} 
  $
  $
  of $\Omega$
  to be
  \begin{gather*}
  \sigma_\Omega 
  \,
  :
  \,
  \R^n \to\  \overline{\R}
  \,,
  \qquad
  x^*
  \ 
  \mapsto
  \ 
    \sup_{x \in \Omega}
    \ 
    \inner{x^*\!}{x}
    \,.
  \end{gather*}
\end{definition*}


\begin{definition}
  Given functions
  $
    f_i\,:\,
    \R^n \to\  \overline{\R}
  $
  for $ i = 1, \ldots, m $,
  we define the \textbf{infimal convolution} of these functions to be
  \begin{gather*}
    f_1 \square \cdots \square f_m
    \ 
    \colon
    \ 
    \R^n
    \to
    \ 
    \overline{\R}
    \,,
    \quad
    x
    \ 
    \mapsto
    \ 
    \inf
    \left\{ 
    \sum_{i = 1}^{m}
      f_i(x_i)
      \ 
      \colon
      \ 
      x_i \in \R^n 
      \ 
      \mathrm{and}\ 
      \sum_{i = 1}^{m} 
        x_i
      =
      x
    \right\}
    \,.
  \end{gather*}
\end{definition}
 
The next result establishes a connection between the support function of the intersection of two convex sets and the infimal convolution of the support functions of the sets taken by themselfes.
The proof translates the geometric concept of convex separation to the world of convex functions.

\begin{lemma}
  \label{cv:primer:lem}
  Let $C_1$ and $C_2$ be two non-empty convex sets in $\R^n$.
  For any
  $ x^* \in \mathrm{dom}\, \sigma_{C_1\cap C_2} $
  the sets
  \begin{align*}
    \Theta_1
    &
    \ :=\ 
    C_1 \times [\,0,\infty)
    \,,
    \\
    \Theta_2
    (x^*)
    &
    \ :=\ 
    \left\{ 
      (x,\lambda)\in \R^n
      \ 
      \colon
      \ 
      x \in C_2
      \ 
      \text{and}
      \ 
      \lambda
      \,
      \le
      \,
      \inner{x^*\!}{x} 
      \ 
      -
      \ 
      \sigma_{C_1\cap C_2}(x^*)
    \right\}
  \end{align*}
  can by properly separated.
\end{lemma}
\begin{proof}
  We fix 
  $ x^* \in \mathrm{dom}\, \sigma_{C_1\cap C_2} $
  and write
  $ 
  \alpha
  \ 
  :=
  \ 
  \sigma_{C_1\cap C_2}(x^*)
  $.
  In order to apply convex separation in finite dimension 
  (Theorem~\ref{cv:primer:sep})
  to
  the sets
  $ \Theta_1 $ and $ \Theta_2(x^*) $,
  it suffics to show
  their convexity and
  $
    \mathrm{ri}\, 
    \Theta_1
    \cap
    \mathrm{ri}\, 
    \Theta_2(x^*)
    =
    \emptyset
  $.
  \subsubsection*{Convexity of 
  $ \Theta_1 $ and $ \Theta_2(x^*) $
  }
  Clearly, 
  $ \Theta_1 $ is convex by the convexity of 
  $ C_1 $ and $ [0,\infty) $.
 To see that $\Theta_2(x^*)$ is convex consider the linear function
 \begin{gather*}
    L
    \,
    :
    \,
    \R^n\times\,  \R 
    \ 
    \to
    \ 
    \R
    \,,
    \qquad 
    (x,\lambda)
    \ 
    \mapsto
    \ 
    \inner{x^*\!}{x} - \lambda
    \,.
 \end{gather*}
 From the definitions of $L$ and $\Theta_2(x^*)$ we get 
  \begin{gather*}
 \Theta_2
 (x^*)
    \ 
    =
    \ 
    (
    C_2\!\times\R
    )
    \ 
    \cap
    \ 
    L^{\!-1}
    [\,\alpha,\infty)
    \,
    .
  \end{gather*}
  Thus,
  by
  Proposition~\ref{cv:primer:prop}~(v)
  and the convexity of $C_2$ we get the convexity of
  $
    L^{\!-1}
    [\,\alpha,\infty)
  $ and with it that of $\Theta_2(x^*)$.

  \subsubsection*{Relative interiors of
  $ \Theta_1 $ and $ \Theta_2(x^*) $
  are disjoint}
  We start by calculating the relative interiors. It holds
  \begin{alignat*}{3}
    \mathrm{ri}\,
    \Theta_1
    &
    \ 
    =
    \ 
    \mathrm{ri}
    ( C_1\times [0,\infty) )
    &&
    \ 
    =
    \ 
    \mathrm{ri}\,
    C_1
    \!
    \times
    \mathrm{ri}\,
    [0,\infty)
    \ 
    =
    \ 
    \mathrm{ri}\,
    C_1
    \!
    \times
    (0,\infty)
    \,,
    %%%%%%%%%%%%%%%%%
    \\
    \mathrm{ri}\,
    \Theta_2(x^*)
    & 
    \ 
    =
    \ 
    \mathrm{ri}
    (
    L^{\!-1}
    [\,\alpha,\infty)
    )
    &&
    \ 
    =
    \ 
    L^{\!-1}
    (
    \mathrm{ri}\,
    [\,\alpha,\infty)
    )
    \ 
    \ 
    =
    \ 
    L^{\!-1}
    (\alpha,\infty)
    \,.
  \end{alignat*}
  Suppose there exists
  $ (\lambda,x) \in
    \mathrm{ri}\, 
    \Theta_1
    \cap
    \,
    \mathrm{ri}\, 
    \Theta_2(x^*)
  $.
  Then it holds 
  $ x \in C_1\!\times C_2 $
  and 
  $ \lambda >0 $.
  We also note, that
  \begin{gather*}
  \alpha
  \ 
  =
  \ 
  \sigma_{C_1\cap\, C_2}(x^*)
    \ 
  =
    \ 
  \sup_{z \in C_1\cap\, C_2}
  \inner
  {x^*}
  {z}
  \ 
  \ge
  \ 
  \inner
  {x^*}
  {x}
  \,.
  \end{gather*}
  Then it follows
  \begin{gather*}
    \alpha
    \ 
    <
    \ 
  \inner
  {x^*}
  {x}
  - \lambda
    \ 
  \le
    \ 
  \alpha\,,
  \end{gather*}
  a contradiction.
  Thus, the relative interiors of
  $ \Theta_1 $ and $ \Theta_2(x^*) $
  are disjoint.

  Applying Theorem~\ref{cv:primer:sep} finishes the proof.
\end{proof}


\begin{theorem*}
  Let $C_1$ and $C_2$ be two non-empty convex sets in $\R^n$ with
  $\mathrm{ri}\,C_1\cap\mathrm{ri}\,C_2\neq\emptyset.$
  Then the support function of the intersection 
  $
    C_1\! \cap C_2
  $
  is represented as
  \begin{gather}
    (\sigma_{
    C_1 \cap\, C_2
    })
    (x^*)
    =
    (\sigma_{C_1}\square \,\sigma_{C_2})
    (x^*)
    \qquad
    \text{for all}\ 
    x^* \in \R^n.
  \end{gather}
  Furthermore, for any
  $
  x^*\in \mathrm{dom}
    (\sigma_{
    C_1 \cap\, C_2
    })
  $
  there exist dual elements 
  $
    x_1^*
    ,
    x_2^*
    \in \R^n
  $ 
  such that 
  $
    x^*
    =
    x_1^*
    +
    x_2^*.
  $
  and
  \begin{gather}
    (\sigma_{
    C_1 \cap\, C_2
    })
    (x^*)
    =
    \sigma_{C_1}(x_1^*)
    +
    \sigma_{C_2}(x_2^*).
  \end{gather}
\end{theorem*}
\begin{proof}
  Using Lemma~\ref{cv:primer:lem}
  the rest of the proof is as that of
  \emph{\cite[Theorem~4.23(b)]{Mordukhovich2022}}.
\end{proof}

\begin{takeaways}
  The support function intersection rule connects the geometric 
  property of convex separation to an identity of support functions
  This result is central to the analysis of convex conjugates.
\end{takeaways}
One important application of convex functions is in optimization.
There we often analyse a dual problem instead, which relies on the notion of \textbf{convex conjugate} 
$
    f^*:
    \R^n \to \overline{\R}
  $
  of $f$ defined by
  \begin{gather}
    \label{def:convex_conjugate}
    f^*(x^*)
    :=
    \sup_{ x \in \R^n }
    \inner
    {x^*}{x}
    - f(x)
    \,.
  \end{gather}
  Even for arbitrary functions, the convex conjugate is convex(cf.
  \cite[Proposition~4.2]{Mordukhovich2022}
  ).
  Like in differential calculus, there exist sum and chain rule for computing the convex conjugate.
\begin{theorem}
  Let
  $
    f,g:
    \R^n \to (-\infty, \infty]
  $
  be proper convex functions 
  and
  $
  \text{ri}\left( \text{dom}(f) \right)
  \cap
  \text{ri}\left( \text{dom}(g) \right)
  \neq 
  \emptyset
  .
  $
  Then we have the 
  \textbf{
  conjugate sum rule
  }
  \begin{gather}
    ( f + g )^*(x^*)
    =
    ( f^* \square g^*)(x^*)
  \end{gather}
  for all $x^* \in \R^n$.
  Moreover, the infimum in 
  $
    ( f^* \square g^*)(x^*)
  $
  is attained, i.e., for any
  $
    x^* \in \text{dom}(f+g)^*
  $
  there exists vectors $x_1^*, x_2^*$
  for which
  \begin{gather}
    (f+g)^*(x^*)
    =
    f^*(x_1^*)
    +
    g^*(x_2^*),
    \quad
    x^* = x_1^* + x_2^*.
  \end{gather}
\end{theorem}
\begin{proof}
  \cite[Theorem~4.27(c)]{Mordukhovich2022}
\end{proof}



% conjugate chain rule %
 %%%%%%%%%%%%%%%%%%%%%%
\begin{theorem}
  \label{cvxa_conjugate_chain_rule}
  Let 
  $
    A:
      \R^m \to \R^n
  $
  be a linear map (matrix)
  and
  $
    g:
      \R^n \to (-\infty, \infty]
  $
  a proper convex function. If
  $
    \text{Im}(A) \cap \text{ri}(\text{dom}(g))
    \neq
    \emptyset
  $
  it follows
  the 
  \textbf{conjugate chain rule}
  \begin{gather}
    ( g \circ A )^* ( x^* )
    =
    \inf_
          { y^* \in ( A^* )^{ -1 } ( x^* )}
                                          g^*( y^* )
                                          .
  \end{gather}
  Furthermore, 
    for any 
      $
        x^* \in \text{dom}( g \circ A)^*
      $
        there exists
          $
            y^* \in ( A^* )^{ -1 } ( x^* )
          $
            such that
              $
                ( g \circ A)^* ( x^* )
                =
                g^*( y^* )
              $.
\end{theorem}
\begin{proof}
  \cite[Theorem~4.28(c)]{Mordukhovich2022}
\end{proof}
%%%%%%%%%%%%%%%%%
%%%% EXAMPLE %%%%
%%%%%%%%%%%%%%%%%
\begin{example}
  \label{cv:cc:ex}
  Let 
  $
    f:\R\to \overline{\R}
  $
  be a proper convex function, that is, 
  $
    \mathrm{dom}\,f
    \neq
    \emptyset
  $
  and $f$ is convex.
  In steps we apply the conjugate chain and sum rule, together with mathematical induction,
  to prove the conjugate relationship 
  \begin{align*}
    &S_{f,n}:\R^n \to \overline{\R},
    \qquad
    (x_1,\ldots,x_n)
    \mapsto
    \sum_{i=1}^{n} 
    f(x_i)
    ,
    \\
    &S_{f,n}^*:\R^n \to \overline{\R},
    \qquad
    (x^*_1,\ldots,x^*_n)
    \mapsto
    \sum_{i=1}^{n} 
    f^*(x^*_i)
    \,.
  \end{align*}
  This relationship is very natural and the ensuing calculations serve to confirm our intuition.

  First, we work in the projections on the coordinates. 
  For the $i$-th coordinate, where $i=1,\ldots,n$, this is 
  \begin{gather}
    p_i:\R^n\to \R
    ,
    \quad
    (x_1,\ldots,x_n)
    \mapsto
    x_i\,.
  \end{gather}
  All projections 
  $p_i$
  are linear function with matrix representation
  $
    e_i^\top
  $,
  where $e_i$ is $i$-the coordinate vector.
  The adjoint of $p_i$ is therefore
  \begin{gather}
    p^*_i:\R\to \R^n
    ,
    \quad
    x
    \mapsto
    e_i\cdot x
    \,.
  \end{gather}
  For the inverse image of the adjoint of $p_i$ it holds
  \begin{gather}
    (p_i^*)^{-1}
    \left\{ 
    (x_1^*,\ldots,x_n^*)
    \right\}
    \ 
    =
    \ 
    \begin{cases}
      \left\{ x_i^* \right\},
      \quad
      &\text{if}\ 
      x_j^*=0\ \text{for all}\ j\neq i\,,
      \\
      \ \ \emptyset
      \quad
      &\text{else.}
    \end{cases}
  \end{gather}
  Throughout this example we use the asterisk character $^*$ somewhat inconsistently. 
  Note that $f^*$ is the convex conjugate 
  of the function $f$ and $p_i^*$ is the adjoint linear function of the projection on the $i$-th coordinate. Likewise, we denote dual variables, that is, the arguments of convex conjugates, as $x^*$.

  Next, we employ the conjugate chain rule to establish the conjugate relationship 
  \begin{align*}
    f_i&:\R^n\to \overline{\R}
    ,
    \quad
    (x_1,\ldots,x_n)
    \mapsto x_i \mapsto f(x_i)
    \,,
    \\
    f^*_i&:\R^n\to \overline{\R}
    ,
    \quad
    (x^*_1,\ldots,x^*_n)\mapsto 
    \begin{cases}
      f^*(x_i^*),
      \quad
      &\text{if}\ 
      x_j^*=0\ \text{for all}\ j\neq i\,,
      \\
      \infty
      \quad
      &\text{else.}
    \end{cases}
  \end{align*}
  Note, that 
  $
    f_i
    =
    (f\circ p_i)
  $
  and
  $
    f^*_i
    =
    (f\circ p_i)^*
  $.
  Since 
  $
    \mathrm{Im}\,p_i=\R
  $
  and 
  $
    \mathrm{dom}\, f
    \neq
    \emptyset
  $,
  it holds
  $
    \mathrm{Im}\, p_i
    \cap
    \mathrm{ri}(
    \mathrm{dom}\, f
    )
    \neq
    \emptyset
  $.
  Then $f$ and $p_i$ conform with the demands of the conjugate chain rule.
  It follows
  \begin{align*}
    &f_i^*
    (x^*_1,\ldots,x^*_n) 
    \ 
    =
    \ 
    (f\circ p_i)^*
    (x^*_1,\ldots,x^*_n) 
    \ =
    \ 
    \inf
    \left\{ 
    f^*(y)
    \ 
    |
    \ 
    y\in 
    (p_i^*)^{-1}
    \left\{ 
    (x_1^*,\ldots,x_n^*)
    \right\}
    \right\}
    \\
    &\quad=
    \ 
    \begin{cases}
      f^*(x_i^*),
      \quad
      &\text{if}\ 
      x_j^*=0\ \text{for all}\ j\neq i\,,
      \\
      \infty
      \quad
      &\text{else,}
    \end{cases}
  \end{align*}
  where we keep to the convention $\inf\emptyset=\infty$.
  In the same way it follows
  \begin{gather}
    \left( 
      S_{f,n}
      \circ
      p_{\left\{ 1,\ldots,n \right\}}
    \right)^*
    (x^*_1,\ldots,x^*_{n+1})
    =
    \begin{cases}
      S_{f,n}^*
    (x^*_1,\ldots,x^*_{n})
      \quad
      &\text{if}\ 
      x_{n+1}^*=0\,,
      \\
      \infty
      \quad
      &\text{else,}
    \end{cases}
  \end{gather}

  Next, note that for $n=1$ we arrive at the result. Thus, for some $n\in \mathbb{N}$ it holds
  $
  \left( 
    S_{f,n}
  \right)
  ^*
  =
    S_{f,n}^*
  $.
  In order to apply the conjugate sum rule to 
  $
    S_{f,n}
  $
  and
  $
    f_{n+1}
  $
  we note that
  \begin{align*}
    \mathrm{dom}\, f_i
    &
    \ =\ 
    \left\{ 
      (x_1,\ldots,x_{n+1})
      \in \R^{n+1}
      :
      x_i\in \mathrm{dom}\,f
    \right\}
    \ 
    \neq 
    \ 
    \emptyset
    \qquad
    \text{for all}
    \ 
    i=1,\ldots,n+1
    \,,
    \\
    \bigcap_{i=1}^{n+1}
    \mathrm{dom}\, f_i
    &
    \ =\ 
    \left\{ 
      (x_1,\ldots,x_{n+1})
      \in \R^{n+1}
      :
      x_i\in \mathrm{dom}\,f
      \ 
    \text{for all}
    \ 
    i=1,\ldots,n+1
    \right\}
    \ 
    \neq 
    \ 
    \emptyset
    \,,
  \end{align*}
  and
\begin{align*}
  &
  \mathrm{ri}\left( 
    \mathrm{dom}
    \left( 
    S_{f,n}
    \circ
    p_{\left\{ 1,\ldots,n \right\}}
    \right)
  \right)
  \ 
  \cap
  \ 
  \mathrm{ri}\left( 
    \mathrm{dom}\,f_{n+1}
  \right)
  \\
  &
  \hspace{25mm}
  =
  \ 
  \mathrm{ri}\left( 
    \mathrm{dom}
    \left( 
    S_{f,n}
    \circ
    p_{\left\{ 1,\ldots,n \right\}}
    \right)
  \ 
  \cap
  \ 
    \mathrm{dom}\,f_{n+1}
  \right)
  \ 
  =
  \ 
  \mathrm{ri}
  \left( 
    \bigcap_{i=1}^{n+1}
    \mathrm{dom}\, f_i
  \right)
  \ 
  \neq
  \ 
  \emptyset
  \,.
\end{align*}
By the conjugate sum rule it follows
\begin{align*}
  (
  S_{f,n+1}
  )^*
  =
  (
    S_{f,n}
    \circ
    p_{\left\{ 1,\ldots,n \right\}}
  +
  f_{n+1}
  )^*
  =
  (
    S_{f,n}
    \circ
    p_{\left\{ 1,\ldots,n \right\}}
  )
  ^*
  \square
  f_{n+1}^*
  \\
  =
    S_{f,n}^*
    \circ
    p_{\left\{ 1,\ldots,n \right\}}
    +
  f_{n+1}^*
  =
  S^*_{f,n+1}
  \,.
\end{align*}
\end{example}





