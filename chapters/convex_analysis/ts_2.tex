
\begin{ftheorem}
  \label{cv:ts:th}
  Consider the optimization problem
\begin{align}
  \label{cv:ts:primal}
  %%%% objective %%%%
    &\underset{w \in \R^n}
    {\mathrm{minimize}}
    &&\qquad\qquad
    f(w)
    &&&
    \\
    %%%% Ax >= b %%%%
    \nonumber
    &\mathrm{subject}\ \mathrm{to} 
    &&\qquad\qquad
    \mathbf{U}w
    \ 
    \ge
    \ 
    d
    \\
    \nonumber
    &
    &&\qquad\qquad
    \mathbf{A}w
    \ 
    =
    \ 
    a
    \,,
\end{align}
and its dual problem
  \begin{alignat}{2}
    \label{cv:ts:dual}
  %%%% objective %%%%
    &\underset{
    \lambda_d \in \R^r
,
    \lambda_a \in \R^s
  }
    {\mathrm{maximize}}
    &&\qquad\qquad
    \inner
    {\lambda_d}
    {d}
    \ 
    +
    \ 
    \inner
    {\lambda_a}
    {a}
    \ 
    -
    \ 
    f^*
    \left( 
      \mathbf{U}^\top  \lambda_d
      +
      \mathbf{A}^\top  \lambda_a
    \right)
    \\
    %%%% Ax >= b %%%%
    \nonumber
    &\mathrm{subject}\ \mathrm{to} 
    &&\qquad\qquad
    \lambda_d
    \ 
    \ge
    \ 
    0
    \,.
\end{alignat}
  Let 
$
(\lambda_d^*,\lambda_a^*)
$
be an optimal solution to \eqref{cv:ts:dual}.
If the objective function $f$ of 
\eqref{cv:ts:primal} is strictly convex and its
convex conjugate $f^*$ is continuously differentiable,
then the unique optimal solution to 
\eqref{cv:ts:primal}
is given by
\begin{gather}
  w^*
  =
  \nabla
    f^*
    \left( 
      \mathbf{U}^\top  \lambda_d^*
      +
      \mathbf{A}^\top  \lambda_a^*
    \right)
    \,.
\end{gather}
\end{ftheorem}
%%%%%%%%%%%%%%%%%%%%%%%
%%%% PLAN OF PROOF %%%%
%%%%%%%%%%%%%%%%%%%%%%%
We need strict convexity of $f$ to obtain stationarity. The other
conditions follow from 
continuous differentiability of $f^*$.
Also we will use
%%%%%%%%%%%%%%%
%%%% PROOF %%%%
%%%%%%%%%%%%%%%
\begin{proof}
  Let 
$
(\lambda_d^*,\lambda_a^*)
$
be an optimal solution to
\eqref{cv:ts:dual}.
  \subsubsection*{Stationarity}
  First we show
\begin{gather}
  \label{cv:ts:st:1}
      \mathbf{U}^\top  \lambda_d^*
      +
      \mathbf{A}^\top  \lambda_a^*
      \in
\partial f (w^*)
\,,
\end{gather}
where
\begin{gather*}
  w^*
  :=
  \nabla
    f^*
    \left( 
      \mathbf{U}^\top  \lambda_d^*
      +
      \mathbf{A}^\top  \lambda_a^*
    \right)
    \,.
\end{gather*}
To this end we need
\begin{proposition}
  \label{cv:ts:prop}
  \emph{
\cite[Theorem~23.5(a)-(b)]{Rockafellar1970}.
  }
   For any proper convex function $f$ and any vector $x$, 
   it holds $t\in \partial f(x)$ 
   if and only if 
   $
   \inner
   {\cdot}{t}
   -
   f(\cdot)
   $
   achieves its supremum at $x$.
\end{proposition}
By Proposition~\ref{cv:ts:prop}
it suffices to show
that
$
$
\begin{gather}
  w
  \mapsto
\inner
{w}
{
      \mathbf{U}^\top  \lambda_d^*
      +
      \mathbf{A}^\top  \lambda_a^*
}
-
f(w)
\end{gather}
achieves its supremum at
$w^*$.
Since $f$ is strictly convex 
there exists a unique vector $x^*$
where
the above expression achieves its maximum.
Since
$f^*$ is differentiable it holds
\begin{gather*}
  w^*
  =
  \nabla
    f^*
    \left( 
      \mathbf{U}^\top  \lambda_d^*
      +
      \mathbf{A}^\top  \lambda_a^*
    \right)
    =
    \nabla
    \left( 
      \lambda
      \mapsto
\inner
{x^*}
{
  \lambda
}
-
f(x^*)
    \right)
    \left( 
      \mathbf{U}^\top  \lambda_d^*
      +
      \mathbf{A}^\top  \lambda_a^*
    \right)
    =
x^*
\,.
\end{gather*}
It follows 
\eqref{cv:ts:st:1}
Next we show
\begin{gather}
  -
  \left( 
      \mathbf{U}^\top  
      +
      \mathbf{A}^\top  
  \right)
  \in
  \left[ 
    \partial
    \left( 
      w
      \mapsto
      d
      -
      \mathbf{U}w
    \right)
    (w^*)
    +
    \partial
    \left( 
      w
      \mapsto
      a
      -
      \mathbf{A}w
    \right)
    (w^*)
  \right]
  \,.
\end{gather}

To this end, note that
\begin{gather}
  \inner
  {
  -
\mathbf{U}^\top
\cdot e_i
}
  {w-w^*}
  =
  \left( 
    d-
\mathbf{U}^\top
\cdot w
  \right)_i
  -
  \left( 
    d-
\mathbf{U}^\top\cdot w^*
  \right)_i
  \,.
\end{gather}
Thus
$
-
\mathbf{U}^\top
\in
    \partial
    \left( 
      w
      \mapsto
      d
      -
      \mathbf{U}w
    \right)
    (w^*)
$.
In the same way it follows
$
-
\mathbf{A}^\top
\in
    \partial
    \left( 
      w
      \mapsto
      d
      -
      \mathbf{A}w
    \right)
    (w^*)
$.
We conclude that
\begin{gather}
  \mathrm{0}
  \in
  [
  \partial
  f(w^*)
  +
    \partial
    \left( 
      w
      \mapsto
      d
      -
      \mathbf{U}w
    \right)
    (w^*)
    \cdot
    \lambda_d^*
    +
    \partial
    \left( 
      w
      \mapsto
      a
      -
      \mathbf{A}w
    \right)
    (w^*)
    \cdot
    \lambda_a^*
  ]
  \,.
\end{gather}
\subsubsection*{Complementary Slackness}
  We fix 
  $
  \lambda_a^*
  $
  and
  work with the objective function $G$ of the dual problem, that is,
  \begin{gather}
    G
(\lambda_d)
:
=
    \inner
    {\lambda_d}
    {d}
    \ 
    +
    \ 
    \inner
    {\lambda_a}
    {a}
    \ 
    -
    \ 
    f^*
    \left( 
      \mathbf{U}^\top  \lambda_d
      +
      \mathbf{A}^\top  \lambda_a
    \right)
    \,.
  \end{gather}
  Since $f^*$ is continuously differentiable, so is $G$. 
  Since $f^*$ is convex, $G$ is concave.
  For concave differentiable functions it holds
  \begin{gather}
    G(x)-G(y)
    \ge
    \nabla
    G(x)^\top
    (x-y)
    \,.
  \end{gather}
Let
$\lambda_{d,i}^*$ be the $i$-th coordinate of $\lambda_d^*$ and for
fixed
$\lambda_a^*$ let
$
\nabla
G_i
(\lambda_d^*)
$
be the $i$-th coordinate of 
$
\nabla
G
(\lambda_d^*)
$.
We show that for all 
$
  i\in \left\{ 1,\ldots, s \right\}
$
it holds
\begin{alignat*}{2}
  \text{either}
  &
  &&
  \qquad
  \lambda_{d,i}^*
  = 0
  \quad
  \text{and}
  \quad
  \nabla
  G
  _i(
  \lambda_{d}^*
  ) \le 0
  \\
  \text{or}
  &
  &&
  \qquad
  \lambda_{d,i}^*
  > 0
  \quad
  \text{and}
  \quad
  \nabla
  G
  _i(
  \lambda_{d}^*
  ) = 0
  \,.
\end{alignat*}
Assume towards a contradiction that 
$
\nabla G_i(\lambda_d^*)>0
$
for some 
$
  i\in \left\{ 1,\ldots, s \right\}
$.
By the continuity of $\nabla G$ there exists $\varepsilon>0$ such that 
$
\nabla G_i(
\lambda_d^*
+
e_i\cdot \varepsilon
)
>
0
$.
Thus
\begin{gather}
  G
  (
\lambda_d^*
+
e_i\cdot \varepsilon
  )
  -
  G
  (
\lambda_d^*
  )
  \ge
\nabla G_i(
\lambda_d^*
+
e_i\cdot \varepsilon
)
\cdot
\varepsilon
>0
\,,
\end{gather}
which contradicts the optimality of 
$
\lambda_d^*
$.
Now assume that
$ \lambda_{d,i}^*>0 $ and 
$
  \nabla G_i(\lambda_d^*)< 0
$
for some
$
  i\in \left\{ 1,\ldots, s \right\}
$.
Again, by the 
continuity of $\nabla G$ there exists $\varepsilon>0$ such that
$
  \nabla G_i(\lambda_d^*-e_i\cdot \varepsilon)< 0
$
and
$
\varepsilon
-
\lambda_{d,i}^*
<0
$.
Thus
\begin{gather}
  G
  (
\lambda_d^*
-
e_i\cdot \varepsilon
  )
  -
  G
  (
\lambda_d^*
  )
  \ge
\nabla G_i(
\lambda_d^*
-
e_i\cdot \varepsilon
)
\cdot
\left( 
\varepsilon
-
\lambda_{d,i}^*
\right)
>0
\,,
\end{gather}
which contradicts the optimality of 
$
\lambda_d^*
$.
We have shown the complementary slackness.
\subsubsection*{Primal Feasibility}
In the previous subsection we showed
$
  \nabla G(\lambda_d^*)\le 0
$.
But it holds
\begin{gather}
  \nabla G(\lambda_d^*)
  \ 
  =
  \ 
  d
  -
  \mathbf{U}
  \cdot
  \nabla
    f^*
    \left( 
      \mathbf{U}^\top  \lambda_d^*
      +
      \mathbf{A}^\top  \lambda_a^*
    \right)
    \ 
  =
  \ 
  d
  -
  \mathbf{U}w^*
  \,.
\end{gather}
Thus
$w^*$ satisfies the inequality constraints. 

We view $G$ from a different angel. Let for fixed
$\lambda^*_d$
\begin{gather}
  G(\lambda_a)
  :=
  \inner
  {\lambda_a}{a}
  -
  \left( 
    f^*
    \left( 
      \mathbf{U}^\top  \lambda_d^*
      +
      \mathbf{A}^\top  \lambda_a^*
    \right)
    -
  \inner
  {\lambda_d^*}{d}
  \right)
  =:
  \inner
  {\lambda_a}{a}
  -
  g(\lambda_a)
  .
\end{gather}
The function $g$ inherits convexity and differentiability from 
$f^*$.
From the optimality of $\lambda_a^*$ we know that
$G$ takes its maximum there. But then by Proposition~\ref{cv:ts:prop}
and the differentiability of $g$ it holds
\begin{gather}
  a
  \in
  \partial
  g(\lambda_a^*)
  =
  \left\{ 
    \mathbf{A}
    \nabla
    f^*
    \left( 
      \mathbf{U}^\top  \lambda_d^*
      +
      \mathbf{A}^\top  \lambda_a^*
    \right)
  \right\}
  =
  \left\{ 
    \mathbf{A}
    w^*
  \right\}
  \,.
\end{gather}
Thus $a=
    \mathbf{A}
    w^*
$. But then $w^*$ satisfies also the equality constraints.
\subsubsection*{Conclusion}
Since the dual constraints imply dual feasibility, we verified 
all Karush-Kuhn-Tucker conditions. The result follows from applying 
\cite[Theorem~28.3]{Rockafellar1970}.
\end{proof}
