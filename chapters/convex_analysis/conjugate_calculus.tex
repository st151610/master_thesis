The goal of this section is to establish the tools to calculate convex conjugates. 
We cite the conjugate sum and chain rule without proof.
After some examples, we cite the Fenchel-Rockafellar Theorem.
\begin{definition}
  \label{ def_convex_conjugate }
  \emph{(Convex conjugate)}
  Given a function
  $
    f:
    \R^n \to \overline{\R}
  $
  ,
  the 
  \textbf{convex conjugate}
  $
    f^*:
    \R^n \to \overline{\R}
  $
  of $f$ is defined as
  \begin{gather}
    f^*(x^*)
    :=
    \sup_{ x \in \R^n }
    (x^*)^T x - f(x)
  \end{gather}
\end{definition}
\todo[color=orange!40,inline]{Add comment on nomenclature. What is Legendre transformation in this context?}

Note that $f$ in Definition~\ref{ def_convex_conjugate }
does not have to be convex. On the other hand, the convex conjugate is always convex:

\begin{proposition}
  Let  
  $
    f:
    \R^n \to ( - \infty, \infty ]
  $
  be a proper function. 
  Then its convex conjugate
  $
    f^*:
    \R^n \to ( - \infty, \infty ]
  $
  is convex.
\end{proposition}
\begin{proof}
  \cite[Proposition~4.2]{Mordukhovich2022}
\end{proof}


% conjugate chain rule %
 %%%%%%%%%%%%%%%%%%%%%%
\begin{theorem}
  \emph{(Conjugate Chain Rule)}
  \label{cvxa_conjugate_chain_rule}
  Let 
  $
    A:
      \R^m \to \R^n
  $
  be a linear map (matrix)
  and
  $
    g:
      \R^n \to (-\infty, \infty]
  $
  a proper convex function. If
  $
    \text{Im}(A) \cap \text{ri}(\text{dom}(g))
    \neq
    \emptyset
  $
  it follows
  \begin{gather}
    ( g \circ A )^* ( x^* )
    =
    \inf_
          { y^* \in ( A^* )^{ -1 } ( x^* )}
                                          g^*( y^* )
                                          .
  \end{gather}
  Furthermore, 
    for any 
      $
        x^* \in \text{dom}( g \circ A)^*
      $
        there exists
          $
            y^* \in ( A^* )^{ -1 } ( x^* )
          $
            such that
              $
                ( g \circ A)^* ( x^* )
                =
                g^*( y^* )
              $.
\end{theorem}
\begin{proof}
  \cite[Proposition~4.28]{Mordukhovich2022}
\end{proof}

% conjugate sum rule %
 %%%%%%%%%%%%%%%%%%%%



\begin{theorem}
  Let
  $
    f,g:
    \R^n \to (-\infty, \infty]
  $
  be proper convex functions 
  and
  $
  \text{ri}\left( \text{dom}(f) \right)
  \cap
  \text{ri}\left( \text{dom}(g) \right)
  \neq 
  \emptyset
  .
  $
  Then we have the conjugate sum rule
  \begin{gather}
    ( f + g )^*(x^*)
    =
    ( f^* \square g^*)(x^*)
  \end{gather}
  for all $x^* \in \R^n$.
  Moreover, the infimum in 
  $
    ( f^* \square g^*)(x^*)
  $
  is attained, i.e., for any
  $
    x^* \in \text{dom}(f+g)^*
  $
  there exists vectors $x_1^*, x_2^*$
  for which
  \begin{gather}
    (f+g)^*(x^*)
    =
    f^*(x_1^*)
    +
    g^*(x_2^*),
    \quad
    x^* = x_1^* + x_2^*.
  \end{gather}
\end{theorem}

\todo[color=orange!40,inline]{Include lemma on convex conjugates of indicator functions. This should be straightforward.}

\todo[color=red!40,inline]{
Write example on convex conjugates of
$
F(w)
=
\sum_{i=1}^{n} 
f(w_i)
$
.
See notes.
}


\begin{example*}
  Let 
  $
    f:\R\to \overline{\R}
  $
  be a proper convex function, that is, 
  $
    \mathrm{dom}\,f
    \neq
    \emptyset
  $
  and $f$ is convex.
  In steps we apply the conjugate chain and sum rule, together with mathematical induction,
  to prove the conjugate relationship 
  \begin{align*}
    &S_f:\R^n \to \overline{\R},
    \qquad
    (x_1,\ldots,x_n)
    \mapsto
    \sum_{i=1}^{n} 
    f(x_i)
    ,
    \\
    &S_f^*:\R^n \to \overline{\R},
    \qquad
    (x^*_1,\ldots,x^*_n)
    \mapsto
    \sum_{i=1}^{n} 
    f^*(x^*_i)
    \,.
  \end{align*}
  This relationship is very natural and the ensuing calculations serve to confirm our intuition.

  First, we work in the projections on the coordinates. 
  For the $i$-th coordinate, where $i=1,\ldots,n$, this is 
  \begin{gather}
    p_i:\R^n\to \R
    ,
    \quad
    (x_1,\ldots,x_n)
    \mapsto
    x_i\,.
  \end{gather}
  All projections 
  $p_i$
  are linear function with matrix representation
  $
    e_i^\top
  $,
  where $e_i$ is $i$-the coordinate vector.
  The adjoint of $p_i$ is therefore
  \begin{gather}
    p^*_i:\R\to \R^n
    ,
    \quad
    x
    \mapsto
    e_i\cdot x
    \,.
  \end{gather}
  For the inverse image of the adjoint of $p_i$ it holds
  \begin{gather}
    (p_i^*)^{-1}
    \left\{ 
    (x_1^*,\ldots,x_n^*)
    \right\}
    \ 
    =
    \ 
    \begin{cases}
      \left\{ x_i^* \right\},
      \quad
      &\text{if}\ 
      x_j^*=0\ \text{for all}\ j\neq i\,,
      \\
      \ \ \emptyset
      \quad
      &\text{else.}
    \end{cases}
  \end{gather}
  Throughout this example we use the asterisk character $^*$ somewhat inconsistently. 
  Note that $f^*$ is the convex conjugate 
  of the function $f$ and $p_i^*$ is the adjoint linear function of the projection on the $i$-th coordinate. Likewise, we denote dual variables, that is, the arguments of convex conjugates, as $x^*$.

  Next, we employ the conjugate chain rule to establish the conjugate relationship 
  \begin{align*}
    f_i&:\R^n\to \overline{\R}
    ,
    \quad
    (x_1,\ldots,x_n)\mapsto x_i \mapsto f(x_i)
    \,,
    \\
    f^*_i&:\R^n\to \overline{\R}
    ,
    \quad
    (x^*_1,\ldots,x^*_n)\mapsto 
    \begin{cases}
      f^*(x_i^*),
      \quad
      &\text{if}\ 
      x_j^*=0\ \text{for all}\ j\neq i\,,
      \\
      \infty
      \quad
      &\text{else.}
    \end{cases}
  \end{align*}
  Note, that 
  $
    f_i
    =
    (f\circ p_i)
  $
  and
  $
    f^*_i
    =
    (f\circ p_i)^*
  $.
  Since 
  $
    \mathrm{Im}\,p_i=\R
  $
  and 
  $
    \mathrm{dom}\, f
    \neq
    \emptyset
  $,
  it holds
  $
    \mathrm{Im}\, p_i
    \cap
    \mathrm{ri}(
    \mathrm{dom}\, f
    )
    \neq
    \emptyset
  $.
  Then $f$ and $p_i$ conform with the demands of the conjugate chain rule.
  It follows
  \begin{align*}
    &f_i^*
    (x^*_1,\ldots,x^*_n) 
    \ 
    =
    \ 
    (f\circ p_i)^*
    (x^*_1,\ldots,x^*_n) 
    \ =
    \ 
    \inf
    \left\{ 
    f^*(y)
    \ 
    |
    \ 
    y\in 
    (p_i^*)^{-1}
    \left\{ 
    (x_1^*,\ldots,x_n^*)
    \right\}
    \right\}
    \\
    &\quad=
    \ 
    \begin{cases}
      f^*(x_i^*),
      \quad
      &\text{if}\ 
      x_j^*=0\ \text{for all}\ j\neq i\,,
      \\
      \infty
      \quad
      &\text{else,}
    \end{cases}
  \end{align*}
  where we keep to the convention $\inf\emptyset=\infty$.
\end{example*}






\todo[color=green!40,inline]{Find right moment to introduce nomenclature for optimization problem. See also end of Tseng Bertsekas chapter.}
Given 
proper convex functions $f, g : \R^n \to \overline{\R}$ 
and
a matrix $A \in \R^{n \times n}$,
we define 
the primal minimization problem as follows:
\begin{problem}
  \emph{(Primal)}
  Given proper convex functions 
  $
  f:\R^n \to \overline{\R} 
  $,
  $
  g:\R^m \to \overline{\R} 
  $
  and a matrix 
  $
    A\in \R^{m \times n}
  $
  we define the \textbf{primal optimization problem} to be
  \label{cvxa_primal_problem}
  \begin{gather*}
    \underset{x \in \R^n}{\mathrm{minimize}}
    \qquad
    f(x) + g(Ax)
  \end{gather*}
\end{problem}
\begin{remark}
  Problem \autoref{cvxa_primal_problem}
  appears in the unconstrained form. We can impose constraints by controling for the domains of $f$ and $g$.
  To incorporate linear constraints $Ax \le 0$
  or more general constraints $x\in\Omega$, where $\Omega$ is a convex set,
  we can choose
  \begin{gather}
    g(x)=\delta_\Omega(x):=
  \end{gather}
  where $x\notin\Omega$
  leads to 
  $f(x) + g(x)=\infty$
  and the optimization problem (if feasible) will exclude $x$ from the solutions.
\end{remark}
\begin{problem}
  \emph{(Dual)}
  \label{cvxa_dual_problem}
  Consider the same setting as in Problem~\autoref{cvxa_primal_problem}.
  Using the convex conjugates of $f,g$ and the transpose of $A$
  we define the \textbf{dual problem} of Problem~\autoref{cvxa_primal_problem} to be
  \begin{gather*}
    \underset{y^* \in \R^m}{\mathrm{maximize}}
    \qquad
   - f^*(A^\top y^*) - g^*(y^*).
  \end{gather*}
\end{problem}
\begin{proposition}
  Consider the optimization problem~\autoref{cvxa_primal_problem} and its dual~\autoref{cvxa_dual_problem}, where the functions $f$ and $g$ are not assumed to be convex. Define the \textbf{optimal values} of these problems by
  \begin{gather*}
    \widehat{p}:= \inf_{x\in\R^n}f(x)+g(Ax)
    \quad
    \text{and}
    \quad
    \widehat{d}:= \sup_{y\in\R^m} - f^*(A^\top y) - g^*(y).
  \end{gather*}
  Then we have the relationship
  $\widehat{d}\le \widehat{p}$.
\end{proposition}
\begin{proof}
  \cite[Proposition 4.62]{Mordukhovich2022}
  \end{proof}
\begin{theorem}
  \label{cvxa_fenchel_theorem}
  Let 
  $f, g : \R^n \to \overline{\R}$ 
  be proper convex functions
  and
  $0 \in \text{ri}(\text{dom}(g) - A (\text{dom}(f)) )$
  .
  Then the optimal values of \eqref{cvxa_primal_problem} and \eqref{cvxa_dual_problem} are equal, 
  i.e.
  \begin{gather}
    \inf_{x \in \R^n} 
    \left\{ f(x) + g(Ax) \right\}
    =
    \sup_{y \in \R^n} \left\{   -f^* \left( A^T y \right) - g^*(-y) \right\}
    .
  \end{gather}
\end{theorem}
\begin{proof}
  \cite[Theorem~4.63]{Mordukhovich2022}
\end{proof}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\todo[color=green!40,inline]{Insert lemma in chapter 1.}
\begin{lemma}
  \label{syu_1_result}
  Let 
  $f : \R^n \to (-\infty, \infty]$ 
  be convex.
  Then 
  for all $y \in \R^n$ and $C>0$ 
    \begin{gather}
      \label{7060_0}
      \inf_{\norm{\Delta}=C} f(y+\Delta) - f(y) \ge 0 \quad
      \Longrightarrow
      \quad
    \exists y^* \in \R^n
    \colon
    y^* \,\text{is global minimum of $f$ and}\,
      \norm{y^* - y} \le C.
    \end{gather}
\end{lemma}
\begin{proof}
  Since 
  $\mathcal{C}:=\left\{ \norm{\Delta}\le C \right\}$
  is convex
  $f$ has a local minimum in 
  $
    y + \mathcal{C}
    :=
    \left\{ 
      y + \Delta \,
      \mid \,
      \norm{\Delta}\le C
    \right\}
    .
  $
  Suppose towards a contradiction that
  $
    y^* \in 
            y + \mathcal{C}
  $
  is a local minimum, but not a global minimum 
  and
  the left-hand side of 
  \eqref{7060_0} is true.
  Then it holds
  \begin{gather}
    \label{7060_3}
    f(x) < f(y^*)
    \quad
    \text{for some}\ 
    x 
    \in 
    \R^n 
    \setminus 
      y + \mathcal{C}
    .
  \end{gather}
  Furthermore since $y + \mathcal{C}$ is compact and contains $y^*$,
  the line segment $\mathcal{L}[y^*,x]$ contains a point on the boundary of 
  $y + \mathcal{C}$, i.e.
  \begin{gather}
    \label{7060_4}
    \theta x + (1 - \theta) y^* = y + \Delta_x
    \quad
    \text{for some}\ 
    \theta \in (0,1)\ 
    \text{and}\ 
    \Delta_x \ 
    \text{with}\ 
    \norm{\Delta_x}=C
    .
  \end{gather}
    It follows
    \begin{align}
      \label{7060_5}
      \begin{split}
      f(y^*)
      \le
      f(y)
      \le
      f(y + \Delta_x)
      &=
      f(
        \theta x + (1 - \theta) y^*
      )
      \\
      &\le
      \theta f(x)
      + 
      (1 - \theta)
      f(y^*)
      <
      f(y^*)
      ,
      \end{split}
    \end{align}
    which is a contradiction.
    Thus every local minimum of $f$ in $y + \mathcal{C}$ is also a global minimum.
    The first inequality is due to
    $y^*$ being a local minimum of $f$ in
    $
      y + \mathcal{C},
    $
    the second inequality is due to the left-hand side of 
    \eqref{7060_0} being true,
    the equality is due to \eqref{7060_4},
    the third inequality is due to the convexity of $f$
    and the strict inequality is due to \eqref{7060_3}.
    %It follows the right-hand side of \eqref{7060_0}.
\end{proof}


\begin{takeaways}
  Conjugate sum and chain rule are direct consequences of the support function intersection rule. They are powerful tools, that allow us to compute convex conjugates of difficult expressions as well as proving the Fenchel-Rockafellar Duality theorem.
\end{takeaways}
