The goal of this section is to establish the tools to calculate convex conjugates. 
We prove the conjugate sum and chain rule.
After some examples we will derive the Fenchel-Rockafellar Theorem.
\begin{definition}
  \label{ def_convex_conjugate }
  \emph{(Convex conjugate)}
  Given a function
  $
    f:
    \R^n \to \overline{\R}
  $
  ,
  the 
  \textbf{convex conjugate}
  $
    f^*:
    \R^n \to \overline{\R}
  $
  of $f$ is defined as
  \begin{gather}
    f^*(x^*)
    :=
    \sup_{ x \in \R^n }
    (x^*)^T x - f(x)
  \end{gather}
\end{definition}

Note that $f$ in Definition~\ref{ def_convex_conjugate }
does not have to be convex. On the other hand, the convex conjugate is always convex:

\begin{proposition}
  Let  
  $
    f:
    \R^n \to ( - \infty, \infty ]
  $
  be a proper function. 
  Then its convex conjugate
  $
    f^*:
    \R^n \to ( - \infty, \infty ]
  $
  is convex.
\end{proposition}


\begin{lemma}
  For any proper function
  $
    f:\R^n\to\overline{\R}
  $
  we have
  \begin{gather}
    f^*(x^*) 
    =
    \sigma_{\mathrm{epi}(f)}
    (x^*,-1)
    \qquad
    \text{for}
    \ 
    x^* \in \R^n.
  \end{gather}
\end{lemma}
\begin{proof}
  Let $x^*\in\R^n$
  and
  $
    (x,\lambda)\in \mathrm{epi}(f).
  $
  Then
  $
    x \in \mathrm{dom}(f)
  $
  and
  $
    f(x)\le \lambda.
  $
  Thus
  \begin{gather}
    \inner{x^*}{x} - f(x)
    \ge
    \inner{x^*}{x} - \lambda
    \qquad
    \text{for all}\ 
    (x,\lambda)\in \mathrm{epi}(f).
  \end{gather}
  On the other hand 
  $
    (x,f(x))\in \mathrm{epi}(f)
  $
  for all
  $
    x \in \mathrm{dom}(f).
  $
  It follows
  \begin{gather}
    \inner{x^*}{x} - f(x)
    \le
    \sup_{(x,\lambda)\in\mathrm{epi}(f)}
    \inner{x^*}{x} - \lambda
    \qquad
    \text{for all}\ 
    x \in \mathrm{dom}(f).
  \end{gather}
  Taking the supremum in the last two displays yields
  \begin{align}
    f^*(x^*)
    =
    \sup_{x\in\mathrm{dom}(f)}
    \inner{x^*}{x} - f(x)
    &=
    \sup_{(x,\lambda)\in\mathrm{epi}(f)}
    \inner{x^*}{x} - \lambda
    \\
    &=
    \sup_{(x,\lambda)\in\mathrm{epi}(f)}
    \inner{(x^*,-1)}{(x,\lambda)} 
    =
    \sigma_{\mathrm{epi}(f)}
    (x^*,-1).
  \end{align}
\end{proof}
% conjugate chain rule %
 %%%%%%%%%%%%%%%%%%%%%%
\begin{theorem}
  \emph{(Conjugate Chain Rule)}
  \label{cvxa_conjugate_chain_rule}
  Let 
  $
    A:
      \R^m \to \R^n
  $
  be a linear map (matrix)
  and
  $
    g:
      \R^n \to (-\infty, \infty]
  $
  a proper convex function. If
  $
    \text{Im}(A) \cap \text{ri}(\text{dom}(g))
    \neq
    \emptyset
  $
  it follows
  \begin{gather}
    ( g \circ A )^* ( x^* )
    =
    \inf_
          { y^* \in ( A^* )^{ -1 } ( x^* )}
                                          g^*( y^* )
                                          .
  \end{gather}
  Furthermore, 
    for any 
      $
        x^* \in \text{dom}( g \circ A)^*
      $
        there exists
          $
            y^* \in ( A^* )^{ -1 } ( x^* )
          $
            such that
              $
                ( g \circ A)^* ( x^* )
                =
                g^*( y^* )
              $.
\end{theorem}

% conjugate sum rule %
 %%%%%%%%%%%%%%%%%%%%



\begin{theorem}
  Let
  $
    f,g:
    \R^n \to (-\infty, \infty]
  $
  be proper convex functions 
  and
  $
  \text{ri}\left( \text{dom}(f) \right)
  \cap
  \text{ri}\left( \text{dom}(g) \right)
  \neq 
  \emptyset
  .
  $
  Then we have the conjugate sum rule
  \begin{gather}
    ( f + g )^*(x^*)
    =
    ( f^* \square g^*)(x^*)
  \end{gather}
  for all $x^* \in \R^n$.
  Moreover, the infimum in 
  $
    ( f^* \square g^*)(x^*)
  $
  is attained, i.e., for any
  $
    x^* \in \text{dom}(f+g)^*
  $
  there exists vectors $x_1^*, x_2^*$
  for which
  \begin{gather}
    (f+g)^*(x^*)
    =
    f^*(x_1^*)
    +
    g^*(x_2^*),
    \quad
    x^* = x_1^* + x_2^*.
  \end{gather}
\end{theorem}
\begin{proof}
  Let $x^*\in\R^n$ and fix $x_1^*,x_2^*\in\R^n$ such that
  $x^*=x^*_1+x^*_2$.
  We get
  \begin{align*}
    f^*(x^*_1)+g^*(x^*_2)
    &=
    \sup_{x\in\R^n}
    \inner{x^*_1}{x}-f(x)
    +
    \sup_{x\in\R^n}
    \inner{x^*_2}{x}-g(x)
    \\
    &\ge
    \sup_{x\in\R^n}
    \inner{x^*_1}{x}-f(x)
    +
    \inner{x^*_2}{x}-g(x)
    =
    \sup_{x\in\R^n}
    \inner{x^*_1+x^*_2}{x}-(f(x)+g(x))
    \\
    &=
    \sup_{x\in\R^n}
    \inner{x^*}{x}-(f+g)(x)
    =(f+g)^*(x^*)
  \end{align*}
  Taking the infimum over $x_1^*,x_2^*\in\R^n$ in the above display gives 
  $
  (f^*\square g^*)(x^*)
  \ge
  (f+g)^*(x^*).
  $
  Let us prove now $\le$ under the condition
  $
  \text{ri}\left( \text{dom}(f) \right)
  \cap
  \text{ri}\left( \text{dom}(g) \right)
  \neq 
  \emptyset
  .
  $
  The only case we need to consider is
  $
    (f+g)^*(x^*)<\infty.
  $
  Define two convex sets by
  \begin{align}
    \Omega_1
    &:=
    \left\{ 
      (x,\alpha,\beta)\in\R^{n+2}
      \colon
      \alpha\ge f(x)
    \right\}
    =
    \mathrm{epi}(f)\times \R,
    \\
    \Omega_2
    &:=
    \left\{ 
      (x,\alpha,\beta)\in\R^{n+2}
      \colon
      \beta\ge g(x)
    \right\}.
  \end{align}
  Similar to Lemma we get the representation
  \begin{gather}
    (f+g)^*(x^*)
    =
    \sigma_{\Omega_1\cap\Omega_2}
    (x^*,-1,-1).
  \end{gather}
  Indeed, the only thing we need to verify is
  $
    \mathrm{dom}(f)\cap\mathrm{dom}(g)
    =
    \mathrm{dom}(f+g).
  $
  The inclusion $\subseteq$ is clear.
  Assume towards a contradiction that
  $
    (f+g)(x)<\infty
  $
  and
  $
    f(x)=\infty.
  $
  Since $g(x)>-\infty$ it holds
  \begin{gather}
    \infty
    =
    \infty+g(x)
    =f(x)+g(x)
    =(f+g)(x)
    <
    \infty.
  \end{gather}
  This is a contradiction. The same holds for $f$ and $g$ reversed. It follows the inclusion $\supseteq$ and equality.
  By the support function intersection rule there exist triples
  \begin{gather}
    (x^*_1,-\alpha_1,-\beta_1),
    (x^*_2,-\alpha_2,-\beta_2)
    \in \R^{n+2}
    \quad
    \text{such that}
    \quad
    (x^*,-1,-1)
    =
    (x^*_1+x^*_2,-(\alpha_1+\alpha_2),-(\beta_1+\beta_2))
  \end{gather}
  and
  \begin{gather}
    (f+g)^*(x^*)
    =
    \sigma_{\Omega_1\cap\Omega_2}
    (x^*,-1,-1)
    =
    \sigma_{\Omega_1}
    (x^*_1,-\alpha_1,-\beta_1)
    +
    \sigma_{\Omega_2}
    (x^*_2,-\alpha_2,-\beta_2).
  \end{gather}
  Next we show
  $\beta_1=\alpha_2=0.$
  Suppose towards a contradiction that 
  $\beta_1\neq 0.$ 
  We fix 
  $(\overline{x},\overline{\alpha})\in\mathrm{epi}(f).$
  Then
  \begin{gather}
    \sigma_{\Omega_1}
    (x^*_1,-\alpha_1,-\beta_1)
    =
    \sup_{(x,\alpha,\beta)\in \mathrm{epi}(f)\times \R}
    \inner{x^*}{x}-\alpha \alpha_1 -\beta \beta_1
    \ge
    \sup_{\beta\in \R}
    \inner{x^*}{\overline{x}}-\overline{\alpha} \alpha_1 -\beta \beta_1
    =\infty.
  \end{gather}
  This contradicts
  $
    (f+g)^*(x^*)<\infty.
  $
  In a similar fashion we can derive a contradiction for $\alpha_2\neq0.$
  Employing Lemma and taking into account the structures of the sets 
  $\Omega_1$ and $\Omega_2$ this implies
  \begin{align}
    (f+g)^*(x^*)
    &=
    \sigma_{\Omega_1\cap\Omega_2}
    (x^*,-1,-1)
    =
    \sigma_{\Omega_1}
    (x^*_1,-1,0)
    +
    \sigma_{\Omega_2}
    (x^*_2,0,-1)
    \\
    &=
    \sigma_{\mathrm{epi}(f)}(x^*_1,-1)
    +
    \sigma_{\mathrm{epi}(g)}(x^*_2,-1)
    =
    f^*(x^*_1)
    +
    g^*(x^*_2)
    \ge
    (f^*\square g^*)(x^*).
  \end{align}
  This finishes the proof.
\end{proof}




Given 
proper convex functions $f, g : \R^n \to \overline{\R}$ 
and
a matrix $A \in \R^{n \times n}$,
we define 
the primal minimization problem as follows:
\begin{problem}
  \emph{(Primal)}
  Given proper convex functions 
  $
  f:\R^n \to \overline{\R} 
  $,
  $
  g:\R^m \to \overline{\R} 
  $
  and a matrix 
  $
    A\in \R^{m \times n}
  $
  we define the \textbf{primal optimization problem} to be
  \label{cvxa_primal_problem}
  \begin{gather*}
    \underset{x \in \R^n}{\mathrm{minimize}}
    \qquad
    f(x) + g(Ax)
  \end{gather*}
\end{problem}
\begin{remark}
  Problem \autoref{cvxa_primal_problem}
  appears in the unconstrained form. We can impose constraints by controling for the domains of $f$ and $g$.
  To incorporate linear constraints $Ax \le 0$
  or more general constraints $x\in\Omega$, where $\Omega$ is a convex set,
  we can choose
  \begin{gather}
    g(x)=\delta_\Omega(x):=
  \end{gather}
  where $x\notin\Omega$
  leads to 
  $f(x) + g(x)=\infty$
  and the optimization problem (if feasible) will exclude $x$ from the solutions.
\end{remark}
\begin{problem}
  \emph{(Dual)}
  \label{cvxa_dual_problem}
  Consider the same setting as in Problem~\autoref{cvxa_primal_problem}.
  Using the convex conjugates of $f,g$ and the transpose of $A$
  we define the \textbf{dual problem} of Problem~\autoref{cvxa_primal_problem} to be
  \begin{gather*}
    \underset{y^* \in \R^m}{\mathrm{maximize}}
    \qquad
   - f^*(A^\top y^*) - g^*(y^*).
  \end{gather*}
\end{problem}
\begin{proposition}
  Consider the optimization problem~\autoref{cvxa_primal_problem} and its dual~\autoref{cvxa_dual_problem}, where the functions $f$ and $g$ are not assumed to be convex. Define the \textbf{optimal values} of these problems by
  \begin{gather*}
    \widehat{p}:= \inf_{x\in\R^n}f(x)+g(Ax)
    \quad
    \text{and}
    \quad
    \widehat{d}:= \sup_{y\in\R^m} - f^*(A^\top y) - g^*(y).
  \end{gather*}
  Then we have the relationship
  $\widehat{d}\le \widehat{p}$.
\end{proposition}
\begin{proof}
  It holds
  \begin{align*}
    - f^*(A^\top y^*) - g^*(y^*) 
    &=
      -\sup_{x\in \R^n}\inner{A^\top y^*}{x}-f(x)
      -\sup_{y\in \R^m}\inner{-y^*}{y}-g(y)
      \\
    &=
    \inf_{x\in \R^n}f(x)-\inner{y^*}{Ax}
      +\inf_{y\in \R^m}g(y)+\inner{y^*}{y}
      \\
    &\le
      \inf_{x\in \R^n}f(x)-\inner{y^*}{Ax}
      +\inf_{x\in \R^n}g(Ax)+\inner{y^*}{Ax}
      \\
    &\le
      \inf_{x\in \R^n}f(x)-\inner{y^*}{Ax} + g(Ax) + \inner{y^*}{Ax}
      \\
    &=  
      \inf_{x\in \R^n}f(x)+g(Ax)
    =
      \widehat{p}
  \end{align*}
  The first equality is due to the definition of convex conjugates, the second equality due to $\inner{A^\top y}{x}=\inner{y}{Ax}$ and $\inf \left\{ -B \right\}=-\sup \left\{ B \right\}$ for all $B \subseteq \overline{\R}$ and the first inequality due to $\mathrm{Im}(A)\subseteq \R^m$.
  Taking the supremum with respect to all 
  $y^*\in\R^m$
  yields the result.
\end{proof}
\begin{theorem}
  \label{cvxa_fenchel_theorem}
  Let 
  $f, g : \R^n \to \overline{\R}$ 
  be proper convex functions
  and
  $0 \in \text{ri}(\text{dom}(g) - A (\text{dom}(f)) )$
  .
  Then the optimal values of \eqref{cvxa_primal_problem} and \eqref{cvxa_dual_problem} are equal, 
  i.e.
  \begin{gather}
    \inf_{x \in \R^n} 
    \left\{ f(x) + g(Ax) \right\}
    =
    \sup_{y \in \R^n} \left\{   -f^* \left( A^T y \right) - g^*(-y) \right\}
    .
  \end{gather}
\end{theorem}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{lemma}
  \label{syu_1_result}
  Let 
  $f : \R^n \to (-\infty, \infty]$ 
  be convex.
  Then 
  for all $y \in \R^n$ and $C>0$ 
    \begin{gather}
      \label{7060_0}
      \inf_{\norm{\Delta}=C} f(y+\Delta) - f(y) \ge 0 \quad
      \Longrightarrow
      \quad
    \exists y^* \in \R^n
    \colon
    y^* \,\text{is global minimum of $f$ and}\,
      \norm{y^* - y} \le C.
    \end{gather}
\end{lemma}
\begin{proof}
  Since 
  $\mathcal{C}:=\left\{ \norm{\Delta}\le C \right\}$
  is convex
  $f$ has a local minimum in 
  $
    y + \mathcal{C}
    :=
    \left\{ 
      y + \Delta \,
      \mid \,
      \norm{\Delta}\le C
    \right\}
    .
  $
  Suppose towards a contradiction that
  $
    y^* \in 
            y + \mathcal{C}
  $
  is a local minimum, but not a global minimum 
  and
  the left-hand side of 
  \eqref{7060_0} is true.
  Then it holds
  \begin{gather}
    \label{7060_3}
    f(x) < f(y^*)
    \quad
    \text{for some}\ 
    x 
    \in 
    \R^n 
    \setminus 
      y + \mathcal{C}
    .
  \end{gather}
  Furthermore since $y + \mathcal{C}$ is compact and contains $y^*$,
  the line segment $\mathcal{L}[y^*,x]$ contains a point on the boundary of 
  $y + \mathcal{C}$, i.e.
  \begin{gather}
    \label{7060_4}
    \theta x + (1 - \theta) y^* = y + \Delta_x
    \quad
    \text{for some}\ 
    \theta \in (0,1)\ 
    \text{and}\ 
    \Delta_x \ 
    \text{with}\ 
    \norm{\Delta_x}=C
    .
  \end{gather}
    It follows
    \begin{align}
      \label{7060_5}
      \begin{split}
      f(y^*)
      \le
      f(y)
      \le
      f(y + \Delta_x)
      &=
      f(
        \theta x + (1 - \theta) y^*
      )
      \\
      &\le
      \theta f(x)
      + 
      (1 - \theta)
      f(y^*)
      <
      f(y^*)
      ,
      \end{split}
    \end{align}
    which is a contradiction.
    Thus every local minimum of $f$ in $y + \mathcal{C}$ is also a global minimum.
    The first inequality is due to
    $y^*$ being a local minimum of $f$ in
    $
      y + \mathcal{C},
    $
    the second inequality is due to the left-hand side of 
    \eqref{7060_0} being true,
    the equality is due to \eqref{7060_4},
    the third inequality is due to the convexity of $f$
    and the strict inequality is due to \eqref{7060_3}.
    %It follows the right-hand side of \eqref{7060_0}.
\end{proof}


\begin{takeaways}
  Almost there 
  \lipsum[1]
\end{takeaways}
