The goal of this section is to establish the tools to calculate convex conjugates. 
We cite the conjugate sum and chain rule without proof.
After some examples, we cite the Fenchel-Rockafellar Theorem.
\begin{definition}
  \label{ def_convex_conjugate }
  \emph{(Convex conjugate)}
  Given a function
  $
    f:
    \R^n \to \overline{\R}
  $
  ,
  the 
  \textbf{convex conjugate}
  $
    f^*:
    \R^n \to \overline{\R}
  $
  of $f$ is defined as
  \begin{gather}
    f^*(x^*)
    :=
    \sup_{ x \in \R^n }
    (x^*)^T x - f(x)
  \end{gather}
\end{definition}

Note that $f$ in Definition~\ref{ def_convex_conjugate }
does not have to be convex. On the other hand, the convex conjugate is always convex:

\begin{proposition}
  Let  
  $
    f:
    \R^n \to ( - \infty, \infty ]
  $
  be a proper function. 
  Then its convex conjugate
  $
    f^*:
    \R^n \to ( - \infty, \infty ]
  $
  is convex.
\end{proposition}
\begin{proof}
  \cite[Proposition~4.2]{Mordukhovich2022}
\end{proof}
% conjugate sum rule %
 %%%%%%%%%%%%%%%%%%%%



\begin{theorem}
  Let
  $
    f,g:
    \R^n \to (-\infty, \infty]
  $
  be proper convex functions 
  and
  $
  \text{ri}\left( \text{dom}(f) \right)
  \cap
  \text{ri}\left( \text{dom}(g) \right)
  \neq 
  \emptyset
  .
  $
  Then we have the 
  \textbf{
  conjugate sum rule
  }
  \begin{gather}
    ( f + g )^*(x^*)
    =
    ( f^* \square g^*)(x^*)
  \end{gather}
  for all $x^* \in \R^n$.
  Moreover, the infimum in 
  $
    ( f^* \square g^*)(x^*)
  $
  is attained, i.e., for any
  $
    x^* \in \text{dom}(f+g)^*
  $
  there exists vectors $x_1^*, x_2^*$
  for which
  \begin{gather}
    (f+g)^*(x^*)
    =
    f^*(x_1^*)
    +
    g^*(x_2^*),
    \quad
    x^* = x_1^* + x_2^*.
  \end{gather}
\end{theorem}
\begin{proof}
  \cite[Theorem~4.27(c)]{Mordukhovich2022}
\end{proof}



% conjugate chain rule %
 %%%%%%%%%%%%%%%%%%%%%%
\begin{theorem}
  \label{cvxa_conjugate_chain_rule}
  Let 
  $
    A:
      \R^m \to \R^n
  $
  be a linear map (matrix)
  and
  $
    g:
      \R^n \to (-\infty, \infty]
  $
  a proper convex function. If
  $
    \text{Im}(A) \cap \text{ri}(\text{dom}(g))
    \neq
    \emptyset
  $
  it follows
  the 
  \textbf{conjugate chain rule}
  \begin{gather}
    ( g \circ A )^* ( x^* )
    =
    \inf_
          { y^* \in ( A^* )^{ -1 } ( x^* )}
                                          g^*( y^* )
                                          .
  \end{gather}
  Furthermore, 
    for any 
      $
        x^* \in \text{dom}( g \circ A)^*
      $
        there exists
          $
            y^* \in ( A^* )^{ -1 } ( x^* )
          $
            such that
              $
                ( g \circ A)^* ( x^* )
                =
                g^*( y^* )
              $.
\end{theorem}
\begin{proof}
  \cite[Theorem~4.28(c)]{Mordukhovich2022}
\end{proof}
%%%%%%%%%%%%%%%%%
%%%% EXAMPLE %%%%
%%%%%%%%%%%%%%%%%
\begin{example}
  \label{cv:cc:ex}
  Let 
  $
    f:\R\to \overline{\R}
  $
  be a proper convex function, that is, 
  $
    \mathrm{dom}\,f
    \neq
    \emptyset
  $
  and $f$ is convex.
  In steps we apply the conjugate chain and sum rule, together with mathematical induction,
  to prove the conjugate relationship 
  \begin{align*}
    &S_{f,n}:\R^n \to \overline{\R},
    \qquad
    (x_1,\ldots,x_n)
    \mapsto
    \sum_{i=1}^{n} 
    f(x_i)
    ,
    \\
    &S_{f,n}^*:\R^n \to \overline{\R},
    \qquad
    (x^*_1,\ldots,x^*_n)
    \mapsto
    \sum_{i=1}^{n} 
    f^*(x^*_i)
    \,.
  \end{align*}
  This relationship is very natural and the ensuing calculations serve to confirm our intuition.

  First, we work in the projections on the coordinates. 
  For the $i$-th coordinate, where $i=1,\ldots,n$, this is 
  \begin{gather}
    p_i:\R^n\to \R
    ,
    \quad
    (x_1,\ldots,x_n)
    \mapsto
    x_i\,.
  \end{gather}
  All projections 
  $p_i$
  are linear function with matrix representation
  $
    e_i^\top
  $,
  where $e_i$ is $i$-the coordinate vector.
  The adjoint of $p_i$ is therefore
  \begin{gather}
    p^*_i:\R\to \R^n
    ,
    \quad
    x
    \mapsto
    e_i\cdot x
    \,.
  \end{gather}
  For the inverse image of the adjoint of $p_i$ it holds
  \begin{gather}
    (p_i^*)^{-1}
    \left\{ 
    (x_1^*,\ldots,x_n^*)
    \right\}
    \ 
    =
    \ 
    \begin{cases}
      \left\{ x_i^* \right\},
      \quad
      &\text{if}\ 
      x_j^*=0\ \text{for all}\ j\neq i\,,
      \\
      \ \ \emptyset
      \quad
      &\text{else.}
    \end{cases}
  \end{gather}
  Throughout this example we use the asterisk character $^*$ somewhat inconsistently. 
  Note that $f^*$ is the convex conjugate 
  of the function $f$ and $p_i^*$ is the adjoint linear function of the projection on the $i$-th coordinate. Likewise, we denote dual variables, that is, the arguments of convex conjugates, as $x^*$.

  Next, we employ the conjugate chain rule to establish the conjugate relationship 
  \begin{align*}
    f_i&:\R^n\to \overline{\R}
    ,
    \quad
    (x_1,\ldots,x_n)
    \mapsto x_i \mapsto f(x_i)
    \,,
    \\
    f^*_i&:\R^n\to \overline{\R}
    ,
    \quad
    (x^*_1,\ldots,x^*_n)\mapsto 
    \begin{cases}
      f^*(x_i^*),
      \quad
      &\text{if}\ 
      x_j^*=0\ \text{for all}\ j\neq i\,,
      \\
      \infty
      \quad
      &\text{else.}
    \end{cases}
  \end{align*}
  Note, that 
  $
    f_i
    =
    (f\circ p_i)
  $
  and
  $
    f^*_i
    =
    (f\circ p_i)^*
  $.
  Since 
  $
    \mathrm{Im}\,p_i=\R
  $
  and 
  $
    \mathrm{dom}\, f
    \neq
    \emptyset
  $,
  it holds
  $
    \mathrm{Im}\, p_i
    \cap
    \mathrm{ri}(
    \mathrm{dom}\, f
    )
    \neq
    \emptyset
  $.
  Then $f$ and $p_i$ conform with the demands of the conjugate chain rule.
  It follows
  \begin{align*}
    &f_i^*
    (x^*_1,\ldots,x^*_n) 
    \ 
    =
    \ 
    (f\circ p_i)^*
    (x^*_1,\ldots,x^*_n) 
    \ =
    \ 
    \inf
    \left\{ 
    f^*(y)
    \ 
    |
    \ 
    y\in 
    (p_i^*)^{-1}
    \left\{ 
    (x_1^*,\ldots,x_n^*)
    \right\}
    \right\}
    \\
    &\quad=
    \ 
    \begin{cases}
      f^*(x_i^*),
      \quad
      &\text{if}\ 
      x_j^*=0\ \text{for all}\ j\neq i\,,
      \\
      \infty
      \quad
      &\text{else,}
    \end{cases}
  \end{align*}
  where we keep to the convention $\inf\emptyset=\infty$.
  In the same way it follows
  \begin{gather}
    \left( 
      S_{f,n}
      \circ
      p_{\left\{ 1,\ldots,n \right\}}
    \right)^*
    (x^*_1,\ldots,x^*_{n+1})
    =
    \begin{cases}
      S_{f,n}^*
    (x^*_1,\ldots,x^*_{n})
      \quad
      &\text{if}\ 
      x_{n+1}^*=0\,,
      \\
      \infty
      \quad
      &\text{else,}
    \end{cases}
  \end{gather}

  Next, note that for $n=1$ we arrive at the result. Thus, for some $n\in \mathbb{N}$ it holds
  $
  \left( 
    S_{f,n}
  \right)
  ^*
  =
    S_{f,n}^*
  $.
  In order to apply the conjugate sum rule to 
  $
    S_{f,n}
  $
  and
  $
    f_{n+1}
  $
  we note that
  \begin{align*}
    \mathrm{dom}\, f_i
    &
    \ =\ 
    \left\{ 
      (x_1,\ldots,x_{n+1})
      \in \R^{n+1}
      :
      x_i\in \mathrm{dom}\,f
    \right\}
    \ 
    \neq 
    \ 
    \emptyset
    \qquad
    \text{for all}
    \ 
    i=1,\ldots,n+1
    \,,
    \\
    \bigcap_{i=1}^{n+1}
    \mathrm{dom}\, f_i
    &
    \ =\ 
    \left\{ 
      (x_1,\ldots,x_{n+1})
      \in \R^{n+1}
      :
      x_i\in \mathrm{dom}\,f
      \ 
    \text{for all}
    \ 
    i=1,\ldots,n+1
    \right\}
    \ 
    \neq 
    \ 
    \emptyset
    \,,
  \end{align*}
  and
\begin{align*}
  &
  \mathrm{ri}\left( 
    \mathrm{dom}
    \left( 
    S_{f,n}
    \circ
    p_{\left\{ 1,\ldots,n \right\}}
    \right)
  \right)
  \ 
  \cap
  \ 
  \mathrm{ri}\left( 
    \mathrm{dom}\,f_{n+1}
  \right)
  \\
  &
  \hspace{25mm}
  =
  \ 
  \mathrm{ri}\left( 
    \mathrm{dom}
    \left( 
    S_{f,n}
    \circ
    p_{\left\{ 1,\ldots,n \right\}}
    \right)
  \ 
  \cap
  \ 
    \mathrm{dom}\,f_{n+1}
  \right)
  \ 
  =
  \ 
  \mathrm{ri}
  \left( 
    \bigcap_{i=1}^{n+1}
    \mathrm{dom}\, f_i
  \right)
  \ 
  \neq
  \ 
  \emptyset
  \,.
\end{align*}
By the conjugate sum rule it follows
\begin{align*}
  (
  S_{f,n+1}
  )^*
  =
  (
    S_{f,n}
    \circ
    p_{\left\{ 1,\ldots,n \right\}}
  +
  f_{n+1}
  )^*
  =
  (
    S_{f,n}
    \circ
    p_{\left\{ 1,\ldots,n \right\}}
  )
  ^*
  \square
  f_{n+1}^*
  \\
  =
    S_{f,n}^*
    \circ
    p_{\left\{ 1,\ldots,n \right\}}
    +
  f_{n+1}^*
  =
  S^*_{f,n+1}
  \,.
\end{align*}
\end{example}






%\todo[color=green!40,inline]{Find right moment to introduce nomenclature for optimization problem. See also end of Tseng Bertsekas chapter.}
%Given 
%proper convex functions $f, g : \R^n \to \overline{\R}$ 
%and
%a matrix $A \in \R^{n \times n}$,
%we define 
%the primal minimization problem as follows:
%\begin{problem}
%  \emph{(Primal)}
%  Given proper convex functions 
%  $
%  f:\R^n \to \overline{\R} 
%  $,
%  $
%  g:\R^m \to \overline{\R} 
%  $
%  and a matrix 
%  $
%    A\in \R^{m \times n}
%  $
%  we define the \textbf{primal optimization problem} to be
%  \label{cvxa_primal_problem}
%  \begin{gather*}
%    \underset{x \in \R^n}{\mathrm{minimize}}
%    \qquad
%    f(x) + g(Ax)
%  \end{gather*}
%\end{problem}
%\begin{remark}
%  Problem \autoref{cvxa_primal_problem}
%  appears in the unconstrained form. We can impose constraints by controling for the domains of $f$ and $g$.
%  To incorporate linear constraints $Ax \le 0$
%  or more general constraints $x\in\Omega$, where $\Omega$ is a convex set,
%  we can choose
%  \begin{gather}
%    g(x)=\delta_\Omega(x):=
%  \end{gather}
%  where $x\notin\Omega$
%  leads to 
%  $f(x) + g(x)=\infty$
%  and the optimization problem (if feasible) will exclude $x$ from the solutions.
%\end{remark}
%\begin{problem}
%  \emph{(Dual)}
%  \label{cvxa_dual_problem}
%  Consider the same setting as in Problem~\autoref{cvxa_primal_problem}.
%  Using the convex conjugates of $f,g$ and the transpose of $A$
%  we define the \textbf{dual problem} of Problem~\autoref{cvxa_primal_problem} to be
%  \begin{gather*}
%    \underset{y^* \in \R^m}{\mathrm{maximize}}
%    \qquad
%   - f^*(A^\top y^*) - g^*(y^*).
%  \end{gather*}
%\end{problem}
%%\begin{proposition}
%%  Consider the optimization problem~\autoref{cvxa_primal_problem} and its dual~\autoref{cvxa_dual_problem}, where the functions $f$ and $g$ are not assumed to be convex. Define the \textbf{optimal values} of these problems by
%%  \begin{gather*}
%%    \widehat{p}:= \inf_{x\in\R^n}f(x)+g(Ax)
%%    \quad
%%    \text{and}
%%    \quad
%%    \widehat{d}:= \sup_{y\in\R^m} - f^*(A^\top y) - g^*(y).
%%  \end{gather*}
%%  Then we have the relationship
%%  $\widehat{d}\le \widehat{p}$.
%%\end{proposition}
%%\begin{proof}
%%  \cite[Proposition 4.62]{Mordukhovich2022}
%%  \end{proof}
%\begin{theorem}
%  \label{cvxa_fenchel_theorem}
%  Let 
%  $f$ 
%  and
%  $g\, :\, \R^n \to \ \overline{\R}$ 
%  be proper convex functions
%  and
%  \begin{gather*}
%  \mathrm{ri}(\mathrm{dom}\,g)
%  \,
%  \cap
%  \,
%  \mathrm{ri}(A \, \mathrm{dom}\,f)
%  \ 
%  \neq
%  \ 
%  \emptyset
%  \,.
%  \end{gather*}
%  Then the optimal values of \eqref{cvxa_primal_problem} and \eqref{cvxa_dual_problem} are equal, 
%  that is,
%  \begin{gather*}
%    \inf_{x \in \R^n} 
%    \left\{ f(x) 
%      \ 
%    +
%    \ 
%  g(Ax) \right\}
%    \ 
%    =
%    \ 
%    \sup_{y \in \R^n}
%    \left\{   -f^* \left( A^T y \right) 
%      \ 
%    - 
%      \ 
%  g^*(-y) \right\}
%  \,.
%  \end{gather*}
%\end{theorem}
%\begin{proof}
%  \cite[Theorem~4.63]{Mordukhovich2022}
%\end{proof}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\todo[color=green!40,inline]{Insert lemma in chapter 1.}
\begin{lemma}
  \label{syu_1_result}
  Let 
  $f \,:\, \R^n \to \overline{\R}$ 
  be convex.
  Then 
  for all $y \in \R^n$ and $\varepsilon>0$ 
    \begin{gather}
      \label{7060_0}
      \inf_{\norm{\Delta}=\varepsilon} f(y+\Delta) - f(y) \ge 0 \quad
    \end{gather}
    implies
    the existence of  
    a global minimum
    $
    y^* \in \,\R^n
    $
    of $f$
    satisfying
    $
      \norm{y^* - y} \le \varepsilon
    $.
\end{lemma}
\begin{proof}
  Since 
  $
  y
  \,
  +
  \,
  \varepsilon
  B
  $
  is convex, it contains a 
  local minimum  
  of $f$.
  Suppose towards a contradiction that
  $
    y^* 
    \ 
    \in 
    \ 
  y
  \,
  +
  \,
  \varepsilon
  B
  $
  is a local minimum, but not a global one, and
  \eqref{7060_0} is true.
  Then it holds
  \begin{gather}
    \label{7060_3}
    f(x) < f(y^*)
    \quad
    \text{for some}\ 
    x 
    \in 
    \R^n 
    \setminus 
    \left( 
  y
  \,
  +
  \,
  \varepsilon
  B
    \right)
  \,.
  \end{gather}
  Furthermore, since 
  $
  y
  \,
  +
  \,
  \varepsilon
  B
  $ is compact and contains $y^*$,
  the line segment connecting 
  $y^*$ and $x$
  intersects the boundary of 
  $y + \mathcal{C}$, that is,
  there exist
  $
    \theta \in (0,1)
  $
  and 
  $
    \Delta_x
  $
  with 
  $
    \norm{\Delta_x}=\varepsilon
  $
  such that
  \begin{gather}
    \label{7060_4}
    \theta x + (1 - \theta) y^* = y + \Delta_x
    \,.
  \end{gather}
    It follows
    \begin{align}
      \label{7060_5}
      \begin{split}
      f(y^*)
      \le
      f(y)
      \le
      f(y + \Delta_x)
      &=
      f(
        \theta x + (1 - \theta) y^*
      )
      \\
      &\le
      \theta f(x)
      + 
      (1 - \theta)
      f(y^*)
      <
      f(y^*)
      ,
      \end{split}
    \end{align}
    which is a contradiction.
    The first inequality is due to
    $y^*$ being a local minimum of $f$ in
    $
  y
  \,
  +
  \,
  \varepsilon
  B
    $,
    the second inequality is due to  
    \eqref{7060_0} being true,
    the equality is due to \eqref{7060_4},
    the third inequality is due to the convexity of $f$
    and the strict inequality is due to \eqref{7060_3}.
    Thus every local minimum of $f$ in
    $
  y
  \,
  +
  \,
  \varepsilon
  B
    $
    is also a global minimum.
    %It follows the right-hand side of \eqref{7060_0}.
\end{proof}


\begin{takeaways}
  Conjugate sum and chain rule are direct consequences of the support function intersection rule. They are powerful tools, that allow us to compute convex conjugates of difficult expressions as well as proving the Fenchel-Rockafellar Duality theorem.
\end{takeaways}
