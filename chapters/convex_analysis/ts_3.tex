We consider a general convex optimization problem 
with matrix equality and inequality constraints.
For this problem there exists a related problem,
which we call its dual.
With ideas from \cite{Tseng1991} we establish 
a functional relationship
between the optimal solution of the original problem 
and
optimal solutions of the dual.
The main assumption is that in the original problem we have a strictly convex objective function 
with continuously differentiable 
convex conjugate(cf. Definition~\ref{cv:cc:d:cc}). 
\begin{ftheorem}
  \label{cv:ts:th}
  Consider the optimization problem
\begin{align}
  \label{cv:ts:primal}
  %%%% objective %%%%
    &\underset{w \in \R^n}
    {\mathrm{minimize}}
    &&\qquad\qquad
    f(w)
    &&&
    \\
    %%%% Ax >= b %%%%
    \nonumber
    &\mathrm{subject}\ \mathrm{to} 
    &&\qquad\qquad
    \mathbf{U}w
    \ 
    \ge
    \ 
    d
    \,.
    \\
    \nonumber
    &
    &&\qquad\qquad
    \mathbf{A}w
    \ 
    =
    \ 
    a
    \,,
\end{align}
and its dual problem
  \begin{alignat}{2}
    \label{cv:ts:dual}
  %%%% objective %%%%
    &\underset{
    \lambda_d \in \R^r
,
    \lambda_a \in \R^s
  }
    {\mathrm{maximize}}
    &&\qquad\qquad
    \inner
    {\lambda_d}
    {d}
    \ 
    +
    \ 
    \inner
    {\lambda_a}
    {a}
    \ 
    -
    \ 
    f^*
    \!
    \left( 
      \mathbf{U}^\top \! \lambda_d
      +
      \mathbf{A}^\top \! \lambda_a
    \right)
    \\
    %%%% Ax >= b %%%%
    \nonumber
    &\mathrm{subject}\ \mathrm{to} 
    &&\qquad\qquad
    \lambda_d
    \ 
    \ge
    \ 
    0
    \,.
\end{alignat}
  Let 
$
(\lambda_d^\dagger,\lambda_a^\dagger)
$
be an optimal solution to \eqref{cv:ts:dual}.
If the objective function $f$ of 
\eqref{cv:ts:primal} is strictly convex and its
convex conjugate $f^*$ is continuously differentiable,
then the unique optimal solution to 
\eqref{cv:ts:primal}
is given by
\begin{gather}
  w^\dagger
  =
  \nabla
    f^*
    \!
    \left( 
      \mathbf{U}^\top  \lambda_d^\dagger
      +
      \mathbf{A}^\top  \lambda_a^\dagger
    \right)
    \,.
\end{gather}
\end{ftheorem}

\subsubsection*{Plan of Proof}
We show that 
$w^\dagger$ and 
$
(\lambda_d^\dagger,\lambda_a^\dagger)
$
meet the 
Karush-Kuhn-Tucker conditions for \ref{cv:ts:primal},
that is,
\textbf{complementary slackness}
\begin{gather}
  \label{cv:ts:comps}
\inner
{\lambda_d^\dagger\,}{d-\mathbf{U} w^\dagger}
\ 
=
\ 
0
\,,
\end{gather}
\textbf{primal} and \textbf{dual feasibility}
\begin{align}
  \label{primal_feas}
    \mathbf{U}w^\dagger
    &
    \ 
    \ge
    \ 
    d
    \,,
    \\
    \nonumber
    \mathbf{A}w^\dagger
    &
    \ 
    =
    \ 
    a
    \,,
  \\
  \label{dual_feas}
  \lambda_d^\dagger
    &
    \ 
  \ge
  \ 
  0
    \,,
\end{align}
and 
\textbf{stationarity}
\begin{gather}
  \mathrm{0}_n
  \ 
  \in
  \ 
  [
  \partial
  f(w^\dagger)
  \ 
  +
  \ 
    \partial
    \left( 
      w
      \mapsto
      d
      -
      \mathbf{U}w
    \right)
    (w^\dagger)
    \cdot
    \lambda_d^\dagger
    \ 
    +
    \ 
    \partial
    \left( 
      w
      \mapsto
      a
      -
      \mathbf{A}w
    \right)
    (w^\dagger)
    \cdot
    \lambda_a^\dagger
    \,
  ]
  \,.
\end{gather}
Applying the well know result\cite[Theorem~28.3]{Rockafellar1970}
finishes the proof.
Apart from elementary calculations, our main tools are the 
strict convexity of $f$, the smoothness of $f^*$ and 
\begin{proposition}
  \emph{
\cite[Theorem~23.5(a)-(b)]{Rockafellar1970}.
  }
  \label{cv:ts:prop}
   For any proper convex function $g$ and any vector $w$, 
   it holds $t\in \partial f(w)$ 
   if and only if 
   $
   x
   \mapsto
   \inner
   {x}{t}
   -
   f(x)
   $
   achieves its supremum at $w$.
\end{proposition}

\begin{proof}
  Let 
$
(\lambda_d^\dagger,\lambda_a^\dagger)
$
be an optimal solution to \eqref{cv:ts:dual}. 

\subsubsection*{Complementary Slackness}
  We fix 
  $
  \lambda_a^\dagger
  $
  and
  work with the objective function $G$ of the dual problem, that is,
  \begin{gather*}
    G
(\lambda_d)
\
:
=
\
    \inner
    {\lambda_d}
    {d}
    \ 
    +
    \ 
    \inner
    {\lambda_a^\dagger}
    {a}
    \ 
    -
    \ 
    f^*
    \left( 
      \mathbf{U}^\top \!\lambda_d
      +
      \mathbf{A}^\top \! \lambda_a^\dagger
    \right)
    \,.
  \end{gather*}
  Since $f^*$ is continuously differentiable, so is $G$.
  Thus
  \begin{gather*}
    \nabla
    G
(\lambda_d^\dagger)
\
:
=
\
d
\ 
    -
    \ 
    \mathbf{U}
    \cdot
    \nabla
    f^*
    \!
    \left( 
      \mathbf{U}^\top \lambda_d^\dagger
      +
      \mathbf{A}^\top  \lambda_a^\dagger
    \right)
    \ 
    =
    \ 
d
\ 
    -
    \ 
    \mathbf{U}
    w^\dagger
    \,.
  \end{gather*}
Let
$\lambda_{d,i}^\dagger$ be the $i$-th coordinate of $\lambda_d^\dagger$ 
and
$
\nabla
G_i
(\lambda_d^\dagger)
$
be the $i$-th coordinate of 
$
\nabla
G
(\lambda_d^\dagger)
$.
  To establish \eqref{cv:ts:comps} we will show
  for all coordinates 
\begin{alignat*}{2}
  \text{either}
  &
  &&
  \qquad
  \lambda_{d,i}^\dagger
  = 0
  \quad
  \text{and}
  \quad
  \nabla
  G
  _i(
  \lambda_{d}^\dagger
  ) \le 0
  \\
  \text{or}
  &
  &&
  \qquad
  \lambda_{d,i}^\dagger
  > 0
  \quad
  \text{and}
  \quad
  \nabla
  G
  _i(
  \lambda_{d}^\dagger
  ) = 0
  \,.
\end{alignat*}
It is well know that a concave functions $g$ satisfies
  \begin{gather}
    \label{cv:ts:concD}
    g(x)-g(y)
    \ge
    \nabla
    g(x)^\top
    (x-y)
    \qquad 
    \text{for all}\ 
    x,y\,.
  \end{gather}
  But $G$ is concave 
  by the convexity of $f^*$(cf. Proposition~\ref{cv:cc:pr:ccc}).

First, we show 
\begin{gather}
  \label{cv:ts:comps:d}
\nabla G_i(\lambda_d^\dagger)
\ 
\le
\ 
0
\qquad
\text{for all}\ 
  i\in \left\{ 1,\ldots, s \right\}
  \,.
\end{gather}
Assume towards a contradiction that 
$
\nabla G_i(\lambda_d^\dagger)>0
$
for some 
$
  i\in \left\{ 1,\ldots, s \right\}
$.
By the continuity of $\nabla G$ there exists $\varepsilon>0$ such that 
$
\nabla G_i(
\lambda_d^\dagger
+
e_i\cdot \varepsilon
)
>
0
$.
It follows from \eqref{cv:ts:concD}
\begin{gather*}
  G
  (
\lambda_d^\dagger
+
e_i\cdot \varepsilon
  )
  -
  G
  (
\lambda_d^\dagger
  )
  \ge
\nabla G_i(
\lambda_d^\dagger
+
e_i\cdot \varepsilon
)
\cdot
\varepsilon
>0
\,,
\end{gather*}
which contradicts the optimality of 
$
\lambda_d^\dagger
$
for\eqref{cv:ts:dual}.
It follows\eqref{cv:ts:comps:d}.
\end{proof}
