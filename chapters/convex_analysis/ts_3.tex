We consider a general convex optimization problem 
with matrix equality and inequality constraints.
For this problem there exists a related problem,
which we call its dual.
With ideas from \cite{Tseng1991} we establish 
a functional relationship
between the optimal solution of the original problem 
and
optimal solutions of the dual.
The main assumption is that in the original problem we have a strictly convex objective function 
with continuously differentiable 
convex conjugate(cf. Definition~\ref{cv:cc:d:cc}). 
\begin{ftheorem}
  \label{cv:ts:th}
  Consider the optimization problem
\begin{align}
  \label{cv:ts:primal}
  %%%% objective %%%%
    &\underset{w \in \R^n}
    {\mathrm{minimize}}
    &&\qquad\qquad
    f(w)
    &&&
    \\
    %%%% Ax >= b %%%%
    \nonumber
    &\mathrm{subject}\ \mathrm{to} 
    &&\qquad\qquad
    \mathbf{U}w
    \ 
    \ge
    \ 
    d
    \,.
    \\
    \nonumber
    &
    &&\qquad\qquad
    \mathbf{A}w
    \ 
    =
    \ 
    a
    \,,
\end{align}
and its dual problem
  \begin{alignat}{2}
    \label{cv:ts:dual}
  %%%% objective %%%%
    &\underset{
    \lambda_d \in \R^r
,
    \lambda_a \in \R^s
  }
    {\mathrm{maximize}}
    &&\qquad\qquad
    \inner
    {\lambda_d}
    {d}
    \ 
    +
    \ 
    \inner
    {\lambda_a}
    {a}
    \ 
    -
    \ 
    f^*
    \!
    \left( 
      \mathbf{U}^\top \! \lambda_d
      +
      \mathbf{A}^\top \! \lambda_a
    \right)
    \\
    %%%% Ax >= b %%%%
    \nonumber
    &\mathrm{subject}\ \mathrm{to} 
    &&\qquad\qquad
    \lambda_d
    \ 
    \ge
    \ 
    0
    \,.
\end{alignat}
  Let 
$
(\lambda_d^\dagger,\lambda_a^\dagger)
$
be an optimal solution to \eqref{cv:ts:dual}.
If the objective function $f$ of 
\eqref{cv:ts:primal} is strictly convex and its
convex conjugate $f^*$ is continuously differentiable,
then the unique optimal solution to 
\eqref{cv:ts:primal}
is given by
\begin{gather}
  w^\dagger
  =
  \nabla
    f^*
    \!
    \left( 
      \mathbf{U}^\top  \lambda_d^\dagger
      +
      \mathbf{A}^\top  \lambda_a^\dagger
    \right)
    \,.
\end{gather}
\end{ftheorem}

\subsubsection*{Plan of Proof}
We show that 
$w^\dagger$ and 
$
(\lambda_d^\dagger,\lambda_a^\dagger)
$
meet the 
Karush-Kuhn-Tucker conditions for \ref{cv:ts:primal},
that is,
\textbf{complementary slackness}
\begin{gather}
  \label{cv:ts:comps}
\inner
{\lambda_d^\dagger\,}{d-\mathbf{U} w^\dagger}
\ 
=
\ 
0
\,,
\end{gather}
\textbf{primal} and \textbf{dual feasibility}
\begin{align}
  \label{cv:ts:pfea}
    \mathbf{U}w^\dagger
    &
    \ 
    \ge
    \ 
    d
    \,,
    \\
    \nonumber
    \mathbf{A}w^\dagger
    &
    \ 
    =
    \ 
    a
    \,,
  \\
  \label{cv:ts:dfea}
  \lambda_d^\dagger
    &
    \ 
  \ge
  \ 
  0
    \,,
\end{align}
and 
\textbf{stationarity}
\begin{gather}
  \label{cv:ts:st}
  \mathrm{0}_n
  \ 
  \in
  \ 
  [
  \partial
  f(w^\dagger)
  \ 
  +
  \ 
    \partial
    \left( 
      w
      \mapsto
      d
      -
      \mathbf{U}w
    \right)
    (w^\dagger)
    \cdot
    \lambda_d^\dagger
    \ 
    +
    \ 
    \partial
    \left( 
      w
      \mapsto
      a
      -
      \mathbf{A}w
    \right)
    (w^\dagger)
    \cdot
    \lambda_a^\dagger
    \,
  ]
  \,.
\end{gather}
Applying the well know result\cite[Theorem~28.3]{Rockafellar1970}
finishes the proof.
Apart from elementary calculations, our main tools are the 
strict convexity of $f$, the smoothness of $f^*$ and 
\begin{proposition}
  \emph{
\cite[Theorem~23.5(a)-(b)]{Rockafellar1970}.
  }
  \label{cv:ts:prop}
   For any proper convex function $g$ and any vector $w$, 
   it holds $t\in \partial f(w)$ 
   if and only if 
   $
   x
   \mapsto
   \inner
   {x}{t}
   -
   f(x)
   $
   achieves its supremum at $w$.
\end{proposition}

\begin{proof}
  Let 
$
(\lambda_d^\dagger,\lambda_a^\dagger)
$
be an optimal solution to \eqref{cv:ts:dual}. 

\subsubsection*{Complementary Slackness}
  We fix 
  $
  \lambda_a^\dagger
  $
  and
  work with the objective function $G$ of the dual problem, that is,
  \begin{gather*}
    G
(\lambda_d)
\
:
=
\
    \inner
    {\lambda_d}
    {d}
    \ 
    +
    \ 
    \inner
    {\lambda_a^\dagger}
    {a}
    \ 
    -
    \ 
    f^*
    \left( 
      \mathbf{U}^\top \!\lambda_d
      +
      \mathbf{A}^\top \! \lambda_a^\dagger
    \right)
    \,.
  \end{gather*}
  Since $f^*$ is continuously differentiable, so is $G$.
  Thus
  \begin{gather*}
    \nabla
    G
(\lambda_d^\dagger)
\
:
=
\
d
\ 
    -
    \ 
    \mathbf{U}
    \cdot
    \nabla
    f^*
    \!
    \left( 
      \mathbf{U}^\top \lambda_d^\dagger
      +
      \mathbf{A}^\top  \lambda_a^\dagger
    \right)
    \ 
    =
    \ 
d
\ 
    -
    \ 
    \mathbf{U}
    w^\dagger
    \,.
  \end{gather*}
Let
$\lambda_{d,i}^\dagger$ be the $i$-th coordinate of $\lambda_d^\dagger$ 
and
$
\nabla
G_i
(\lambda_d^\dagger)
$
be the $i$-th coordinate of 
$
\nabla
G
(\lambda_d^\dagger)
$.
  To establish \eqref{cv:ts:comps} we will show
  for all coordinates 
\begin{alignat*}{2}
  \text{either}
  &
  &&
  \qquad
  \lambda_{d,i}^\dagger
  = 0
  \quad
  \text{and}
  \quad
  \nabla
  G
  _i(
  \lambda_{d}^\dagger
  ) \le 0
  \\
  \text{or}
  &
  &&
  \qquad
  \lambda_{d,i}^\dagger
  > 0
  \quad
  \text{and}
  \quad
  \nabla
  G
  _i(
  \lambda_{d}^\dagger
  ) = 0
  \,.
\end{alignat*}
It is well know that a concave functions $g$ satisfies
  \begin{gather}
    \label{cv:ts:concD}
    g(x)-g(y)
    \ge
    \nabla
    g(x)^\top
    (x-y)
    \qquad 
    \text{for all}\ 
    x,y\,.
  \end{gather}
  But $G$ is concave 
  by the convexity of $f^*$(cf. Proposition~\ref{cv:cc:pr:ccc}).

First, we show 
\begin{gather}
  \label{cv:ts:comps:d}
\nabla G_i(\lambda_d^\dagger)
\ 
\le
\ 
0
\qquad
\text{for all}\ 
  i\in \left\{ 1,\ldots, s \right\}
  \,.
\end{gather}
Assume towards a contradiction that 
$
\nabla G_i(\lambda_d^\dagger)>0
$
for some 
$
  i\in \left\{ 1,\ldots, s \right\}
$.
By the continuity of $\nabla G$ there exists $\varepsilon>0$ such that 
$
\nabla G_i(
\lambda_d^\dagger
+
e_i\cdot \varepsilon
)
>
0
$.
It follows from \eqref{cv:ts:concD}
\begin{gather*}
  G
  (
\lambda_d^\dagger
+
e_i\cdot \varepsilon
  )
  \ 
  -
  \ 
  G
  (
\lambda_d^\dagger
  )
  \ 
  \ge
  \ 
\nabla G_i(
\lambda_d^\dagger
+
e_i\cdot \varepsilon
)
\cdot
\varepsilon
\ 
>
\ 
0
\,,
\end{gather*}
which contradicts the optimality of 
$
\lambda_d^\dagger
$
for \eqref{cv:ts:dual}.
It follows \eqref{cv:ts:comps:d}.

Next, 
we assume that
$ \lambda_{d,i}^\dagger>0 $ 
and 
$
  \nabla G_i(\lambda_d^\dagger)< 0
$
for some
$
  i\in \left\{ 1,\ldots, s \right\}
$.
Again, by the 
continuity of $\nabla G$ there exists $\varepsilon>0$ such that
$
  \nabla G_i(\lambda_d^\dagger-e_i\cdot \varepsilon)< 0
$
and
$
\varepsilon
-
\lambda_{d,i}^\dagger
<0
$.
Thus
\begin{gather*}
  G
  (
\lambda_d^\dagger
-
e_i\cdot \varepsilon
  )
  -
  G
  (
\lambda_d^\dagger
  )
  \ge
\nabla G_i(
\lambda_d^\dagger
-
e_i\cdot \varepsilon
)
\cdot
\left( 
  -
\varepsilon
\right)
>0
\,,
\end{gather*}
which contradicts the optimality of 
$
\lambda_d^\dagger
$.
It follows \eqref{cv:ts:comps}, that is,
we proved complementary slackness.
\subsubsection*{Primal Feasibility}
Since $f^*$ is continuously differentiable it holds
\begin{gather*}
  \nabla G(\lambda_d^\dagger)
  \ 
  =
  \ 
  d
  \ 
  -
  \ 
  \mathbf{U}
  \cdot
  \nabla
    f^*
    \left( 
      \mathbf{U}^\top  \lambda_d^\dagger
      +
      \mathbf{A}^\top  \lambda_a^\dagger
    \right)
    \ 
  =
  \ 
  d
  -
  \mathbf{U}w^\dagger
  \,.
\end{gather*}
Thus, by \eqref{cv:ts:comps:d},
$w^\dagger$ satisfies the inequality constraints in \eqref{cv:ts:primal}. 
To prove this for the equality constraints,
we view $G$ from a different angel. Let for fixed
$\lambda^\dagger_d$
\begin{gather*}
  G(\lambda_a)
  \ 
  :=
  \ 
  \inner
  {\lambda_a}{a}
  \ 
  -
  \ 
  \left( 
    f^*
    \left( 
      \mathbf{U}^\top  \lambda_d^\dagger
      +
      \mathbf{A}^\top  \lambda_a
    \right)
    -
  \inner
  {\lambda_d^\dagger}{d}
  \right)
  \ 
  =:
  \ 
  \inner
  {\lambda_a}{a}
  \ 
  -
  \ 
  g(\lambda_a)
  \,
  .
\end{gather*}
The function $g$ inherits convexity and differentiability from 
$f^*$.
From the optimality of $\lambda_a^\dagger$ we know that
$G$ takes its maximum there. But then by Proposition~\ref{cv:ts:prop}
and the differentiability of $g$ it holds
\begin{gather}
  a
  \ 
  \in
  \ 
  \partial
  g(\lambda_a^\dagger)
  \ 
  =
  \ 
  \left\{ 
    \mathbf{A}
    \cdot
    \nabla
    f^*
    \left( 
      \mathbf{U}^\top  \lambda_d^\dagger
      +
      \mathbf{A}^\top  \lambda_a^\dagger
    \right)
  \right\}
  \ 
  =
  \ 
  \left\{ 
    \mathbf{A}
    w^\dagger
  \right\}
  \,.
\end{gather}
Thus $a=
    \mathbf{A}
    w^\dagger
$. But then $w^\dagger$ satisfies also the equality constraints.
We proved \eqref{cv:ts:pfea}.
\subsubsection*{Stationarity}
First we show
\begin{gather}
  \label{cv:ts:st:1}
      \mathbf{U}^\top  \lambda_d^\dagger
      \ 
      +
      \ 
      \mathbf{A}^\top  \lambda_a^\dagger
      \ 
      \in
      \ 
\partial f (w^\dagger)
\,.
\end{gather}
By Proposition~\ref{cv:ts:prop}
it suffices to show
that
\begin{gather*}
  w
  \ 
  \mapsto
  \ 
\inner
{w}
{
      \mathbf{U}^\top  \lambda_d^\dagger
      +
      \mathbf{A}^\top  \lambda_a^\dagger
}
  \ 
-
  \ 
f(w)
\end{gather*}
achieves its supremum at
$w^\dagger$.
Since $f$ is strictly convex 
there exists a unique vector $x^\dagger$
where
the above expression achieves its maximum.
Since
$f^*$ is differentiable it holds
\begin{gather*}
  w^\dagger
  \ 
  =
  \ 
  \nabla
    f^*
    \left( 
      \mathbf{U}^\top  \lambda_d^\dagger
      +
      \mathbf{A}^\top  \lambda_a^\dagger
    \right)
  \ 
    =
  \ 
    \nabla
    \left( 
      \lambda
      \mapsto
\inner
{x^\dagger}
{
  \lambda
}
  \ 
-
  \ 
f(x^\dagger)
    \right)
    \left( 
      \mathbf{U}^\top  \lambda_d^\dagger
      +
      \mathbf{A}^\top  \lambda_a^\dagger
    \right)
  \ 
    =
  \ 
x^\dagger
\,.
\end{gather*}
It follows 
\eqref{cv:ts:st:1}.
Next we show
\begin{gather}
  \label{cv:ts:st:2}
-
\mathbf{U}^\top
\in
\ 
    \partial
    \left( 
      w
      \mapsto
      d
      -
      \mathbf{U}w
    \right)
    (w^\dagger)
    \qquad
    \text{and}
\qquad
-
\mathbf{A}^\top
\in
\ 
    \partial
    \left( 
      w
      \mapsto
      d
      -
      \mathbf{A}w
    \right)
    (w^\dagger)
    \,.
\end{gather}
To this end, note that
\begin{gather*}
  \inner
  {
  -
\mathbf{U}^\top
\!
 e_i
}
  {w-w^\dagger}
  \ 
  =
  \ 
  \left( 
    d-
\mathbf{U}
w
  \right)_i
  \ 
  -
  \ 
  (
    d-
\mathbf{U} w^\dagger
  )
  _i
  \qquad
  \text{for all}\ 
  i\in\left\{ 1,\ldots,r \right\}
  \,.
\end{gather*}
Thus
$
-
\mathbf{U}^\top
\in
    \partial
    \left( 
      w
      \mapsto
      d
      -
      \mathbf{U}w
    \right)
    (w^\dagger)
$.
In the same way it follows
$
-
\mathbf{A}^\top
\in
    \partial
    \left( 
      w
      \mapsto
      d
      -
      \mathbf{A}w
    \right)
    (w^\dagger)
$.
From \eqref{cv:ts:st:1} and \eqref{cv:ts:st:2} we conclude
\begin{align*}
  \mathrm{0}_n
  &
      \ 
  =
      \ 
      \left( 
      \mathbf{U}^\top  \lambda_d^\dagger
      \ 
      +
      \ 
      \mathbf{A}^\top  \lambda_a^\dagger
      \right)
      -
      \mathbf{U}^\top  \lambda_d^\dagger
      \ 
      -
      \ 
      \mathbf{A}^\top  \lambda_a^\dagger
      \\
      &
      \ 
  \in
      \ 
  [
  \partial
  f(w^\dagger)
  \ 
  +
  \ 
    \partial
    \left( 
      w
      \mapsto
      d
      -
      \mathbf{U}w
    \right)
    (w^\dagger)
    \cdot
    \lambda_d^\dagger
  \ 
    +
  \ 
    \partial
    \left( 
      w
      \mapsto
      a
      -
      \mathbf{A}w
    \right)
    (w^\dagger)
    \cdot
    \lambda_a^\dagger
    \, 
  ]
  \,.
\end{align}
We have proved \eqref{cv:ts:st}, that is, stationarity.
\subsubsection*{Dual Feasibility and Conclusion}
Dual feasibility \eqref{cv:ts:dfea} follows immediately from the optimality of $\lambda_d^\dagger$ for \eqref{cv:ts:dual}.
Thus, $(\lambda_d^\dagger,\lambda_a^\dagger)$ and $w^\dagger$ satisfy the Karush-Kuhn-Tucker conditions for \eqref{cv:ts:primal}.
Applying 
\cite[Theorem~28.3]{Rockafellar1970} finishes the proof.
\end{proof}
\begin{takeaways}
  For strictly convexity objective functions with continuously
  differentiable convex conjugate we get a functional relationship
  of primal and dual solutions via the Karush-Kuhn-Tucker conditions.
\end{takeaways}
