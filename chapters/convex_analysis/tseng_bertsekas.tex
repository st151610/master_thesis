We present the relevant parts of the paper \cite{Bertsekas2003}.

Consider the following optimization problem

  \begin{gather*}
    \underset{x\in \R^m}{\mathrm{minimize}}
    \qquad
    f(x)
  \end{gather*}
subject to the constraints
\begin{gather}
  \mathbf{A}x \ge a\,,
  \\
  \mathbf{B}x = b\,.
\end{gather}
Where 
$
  f:
  \R^m
  \to 
  \overline{\R}
  $, 
  $
  \mathbf{A} 
  \in 
  \R^{n_a\times m}
$, 
$
  \mathbf{B} 
  \in 
  \R^{n_b\times m}
$, 
$
a\in \R^{n_a}
$
and 
$
b\in \R^{n_b}
$.
\todo[color=green!40, inline]{
  Adjust notation with matrix chapter.
}
\begin{assumption}
  \begin{enumerate}[label={(\roman*)}]
    Assume that the map 
    $
      f: \R^m \to \overline{\R}
    $
    has the following properties.
    \item
      $
        f 
        \ 
        \text{is strictly convex.}
      $
    \item
      $
        f
        \ 
        \text{is lower-semicontinuous and continuous}
        \ 
        \mathrm{dom}(f)
        .
      $
    \item
      $
        \text{The convex conjugate}
        \ 
        f^*
        \ 
        \text{of}\ 
        f
        \ 
        \text{is finite}
        .
      $
  \end{enumerate}
\end{assumption}

The dual optimization problem $(D)$ associated with $(P)$
is 


  \begin{gather*}
    \underset{
      (y_a,y_b)\in \R^{n_a+n_b}
    }{\mathrm{maximize}}
    \qquad
    q(y_{a},y_{b})
  \end{gather*}
subject to the constraints
\begin{gather}
  y_a \ge 0
  ,
\end{gather}
where 
$
q: \R^{n_a}\!\!\times\R^{n_b}\, \to\  \overline{\R}
$
is the concave function given by
\begin{gather}
  q(y_a,y_b)
  \ 
  :=
  \ 
  \inner{y_a}{a}
  +
  \inner{y_b}{b}
  \ 
  -
  \ 
  f^*(
    \mathbf{A}^{\!\!\top}y_a
    +
    \mathbf{B}^{\!\top} y_b 
  )
  \,
  .
\end{gather}
The dual problem $(D)$ is a concave program
with nonnegativity constraints on the dual variable $y_a$
of the inequality constraints in $(P)$.
Furthermore, strong duality holds for $(P)$ and $(D)$,
that is, they have the same optimal value.

Since $f^*$ is real-valued and $f$ is strictly convex, 
$f^*$ and $q$ are continuously differentiable~(cf.\cite[Theorem~26.3]{Rockafellar1970}).
\begin{theorem*}
  \emph{}
  A closed proper convex function is strictly convex
  if and only if its conjugate is continuously differentiable.
\end{theorem*}
\todo[color=green!40, inline]{What does closed mean and does $f$ meet this condition?}

We will denote the gradient of $q$ at $p$ by $d(p)$
and its $i$th coordinate by $d_i(p).$
Since $q$ is continuously differentiable, $d_i(p)$ is continuous, 
and since $q$ is concave, 
$d_i(p)$ as nonincreasing in $p_i.$

By differentiating and by using the chain rule, 
we obtain the dual cost gradient
\begin{gather}
  d(p)=b-\mathbf{A}x,
  \quad
  \text{where}
  \quad
  x:=
  \nabla
  f^*
  (
    \mathbf{A}^\top
    p
  )
  =
  \mathrm{argsup}
  _{\xi \in \R^m}
  \inner{p}{\mathbf{A}\xi}
  - f(\xi)
  .
\end{gather}
\todo[color=green!40, inline]{Read first paper of tseng Bertsekas for equality constraints.}
The last equality follows from Danskin's Theorem and \cite[Theorem~23.5]{Rockafellar1970}


\todo[color=red!40,inline]{Read and understand proof (p.80) }
\begin{proposition}
  \emph{(Danskin's Theorem \cite[page 649]{Bertsekas2003})}
  Let 
  $
    Z \subseteq \R^m
  $
  be a non-empty set, 
  and let 
  $
    \phi :
    \R^n \times Z \to \R
  $
  be a continuous function such that
  $
    \phi(\cdot,z)
    :
    \R^n \to \R,
  $
  viewed as a function of its first argument, 
  is convex for each 
  $
    z \in Z.
  $
  Then the function 
  \begin{gather}
    f:\R^n \to \R
    ,
    \qquad
    x\mapsto
    \sup_{z\in Z}
    \phi(x,z)
  \end{gather}
  is convex and has directional derivative given by
  \begin{gather}
    f^{'}(x;y)
    =
    \sup_{z \in Z(x)}
    \phi^{'}(x,z; y)
    ,
  \end{gather}
  where 
  $
    \phi^{'}(x,z; y)
  $
  is the directional derivative of the function 
  $
    \phi(\cdot,z)
  $
  at
  $x$ in the direction $y,$
  and
  \begin{gather}
    Z(x)
    :=
    \left\{ 
      \overline{z} \in \R^m
      \colon
      \phi(x, \overline{z})
      =
    \sup_{z\in Z}
    \phi(x,z)
  \right\}
  .
  \end{gather}
  In particular, if $Z(x)$ consists of a unique point 
  $\overline{z}$
  and
  $
    \phi(\cdot, \overline{z})
  $
  is differentiable at $x,$
  and 
  $
    \nabla f(x)
    =
    \nabla_x
    \phi(x,\overline{z})
    ,
  $
  where 
  $
    \nabla_x
    \phi(x,\overline{z})
  $
  is the vector with coordinates
  $
    (\partial \phi / \partial x_i)
    (x,\overline{z})
  $
\end{proposition}

Note that $x$ is the unique vector satisfying 
\begin{gather}
  \mathbf{A}p
  \in 
  \partial
  f(x)
  .
\end{gather}
From the optimality conditions for $(D)$ it follows that 
a dual vector is an optimal solution of $(D)$
if and only if 
\begin{gather}
  p
  =
  [p + d(p)]^+
  ,
\end{gather}
where $[\ \cdot \ ]^+$ is the projection onto the positive orthant, i.e.,
$
  [y]^+
  =
  [
    0 \lor y_1,
    \ldots
    0 \lor y_n,
  ]
  ^\top
  .
$
\todo[color=orange!40,inline]{Provide details. See notes.}

Given an optimal dual solution $p$,
we may obtain an optimal primal solution from the equation 
$
  x=
  \nabla
  f^*
  (
    \mathbf{A}^\top
    p
  )
.
$
To see this, 
note that 
 \begin{gather}
   \mathbf{A}x \ge b
   \quad
   \text{and}
   \quad
   p_i 
   =
   0
   \quad 
   \text{for all}
   \ 
   i
   \ 
   \text{such that}
   \quad 
   \sum_{j=1}^{m} 
   a_{ij}
   x_j
   >
   b_i
   .
 \end{gather}

 We can show that $p$ and $x$ satisfy the KKT conditions and thus $x$ is an optimal solution to $(P)$.

n 

\begin{definition}
  \emph{\cite[ยง28]{Rockafellar1970}}
  By an \textbf{ordinary convex program}
  $(P)$
  we mean an optimization problem of the following form
  \begin{gather*}
    \underset{x\in C}{\mathrm{minimize}}
    \qquad
    f_0(x)
  \end{gather*}
subject to the constraints
\begin{gather}
  f_1(x)\le 0,
  \ldots,
  f_r(x)\le 0,
  \qquad
  f_{r+1}(x)= 0,
  \ldots,
  f_m(x)= 0,
\end{gather}
where $C\subseteq \R^n$
is a non-empty convex set,
$f_i$ is a finite convex function on $C$ for $i\in \left\{ 1,\ldots,r \right\}$
 and 
 $f_i$ is an affine function on $C$ for $i\in \left\{ r+1, \ldots, m \right\}.$
\end{definition}

\begin{definition}
  We define 
  $
    [\lambda_1, \ldots, \lambda_m]\in \R^m
  $
  to be a \textbf{Karush-Kuhn-Tucker (KKT) vector}
  for $(P)$, if
  \begin{enumerate}[label={(\roman*)}]
    \item
      $
        \lambda_i \ge 0
        \ 
        \text{for all}
        \ 
        i\in \left\{ 1,\ldots,r \right\}.
      $
    \item
      The infimum of the proper convex function 
      $
        f_0
        +
        \sum_{i=1}^{m}
        \lambda_1 f_i
      $
      is finite and equal to the optimal value in $(P).$
  \end{enumerate}
\end{definition}

\begin{ftheorem}
  \emph{(Karush-Kuhn-Tucker conditions)}
  Let $(P)$
  be an ordinary convex program,
  $
  \overline{\alpha}
  \in \R^m
  $,
   and 
   $
   \overline{z}
   \in \R^n.
   $
   Then 
   $
  \overline{\alpha}
   $
   is a KKT vector for $(P)$
   and 
   $
   \overline{z}
   $
   is an optimal solution to $(P)$
   if and only if 
   $
   \overline{z}
   $
   and 
   the components $\alpha_i$ of
   $
  \overline{\alpha}
   $
   satisfy 
   the following conditions.

  \begin{enumerate}[label={(\roman*)}]
    \item
      $
        \alpha_i \ge 0,
        \ 
        f_i(
   \overline{z}
        )
        \le 0,
        \ 
        \text{and}
        \ 
        \alpha_i 
        f_i(
   \overline{z}
        )
        =0
        \ 
        \text{for all}
        \ 
        i\in \left\{ 1, \ldots, r \right\}
        .
      $
      \item
        $
        f_i(
   \overline{z}
        )
        =0
        \ 
        \text{for}
        \ 
        i\in \left\{ r+1, \ldots, m \right\}
        .
        $
      \item
        $
         0
         _n
         \in 
         [
          \partial
        f_0(
   \overline{z}
        )
        +
        \sum_{\alpha_i\neq 0}
        \alpha_i 
        \partial
        f_i(
   \overline{z}
        )
         ]
.
        $
  \end{enumerate}
\end{ftheorem}
\begin{proof}
  \cite[Theorem~28.3]{Rockafellar1970}
\end{proof}

\begin{takeaways}
  Employing the Karush-Kuhn-Tucker conditions, we
  derive a dual relationship between optimal solutions
  for strictly convex functions.
\end{takeaways}
