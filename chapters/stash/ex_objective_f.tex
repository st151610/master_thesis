\begin{example}
  For two discrete distributions
  \begin{gather*}
    p:=[p_1,\ldots,p_N]
    \qquad
    \text{and}
    \qquad
    q:=[q_1,\ldots,q_N]
  \end{gather*}
  we consider the following distance measure
  \begin{gather*}
   D(p|q)
   \ 
   :=
   \ 
   \sum_{i=1}^{N} 
   p_i
   \cdot
   \log
   \left( 
     \frac{p_i}{q_i}
   \right)
   \,.
  \end{gather*}
  This is known as the Kullback-Leibler-Entropy.
  In \cite[ยง3.1]{Hainmueller2012} the author connects this concept to a convex optimization problem similar to Problem~\ref{bw:1:primal}.
  The idea is, to optimize the Kullback-Leibler-Entropy of the distribution induced by the weights and some base weights.
  For example, if we choose
  \begin{gather*}
    w:=
    \frac{1}{N}[w_1\cdot T_1,\ldots,w_N\cdot T_N]
    \qquad
    \text{and}
    \qquad
    q:=\frac{1}{N}[1,\ldots,1]
  \end{gather*}
  we get
  \begin{align}
    \label{8926}
   D(w|q)
   \ 
   =
   \ 
   \frac{1}{N}
   \sum_{i=1}^{N} 
w_i\cdot T_i
   \cdot
   \log
   \left( 
     \frac{w_i\cdot T_i}{N}\cdot N
   \right)
   \ 
   =
   \ 
   \frac{1}{N}
   \sum_{i=1}^{n} 
w_i
   \cdot
   \log
   \left( 
     w_i
   \right)
   \,,
  \end{align}
  where we set $"0\cdot \log(0)"=0$. Thus, the optimization problem
  \begin{gather*}
    \underset{w_1, \ldots, w_n \in \R}
    {\text{minimize}}
    \qquad\qquad
    \sum_{i = 1}^{n} 
w_i
   \cdot
   \log
   \left( 
     w_i
   \right)
  \end{gather*}
  produces the same optimal solutions as minimizing the Kullback-Leibler-Entropy \eqref{8926} with respect to $w$.
Thus, we consider 
\begin{gather*}
  \varphi
  \ 
  \colon
  \R
  \ 
  \to
  \ 
  \overline{\R}
  \,,
  \qquad
  x
  \ 
  \mapsto
  \ 
  \begin{cases}
    x\cdot \log x\ &\text{if}\ x>0\,, \\
    0 \ &\text{if}\, x=0\,,\\
    \infty\ &\text{if}\ x<0\,.
  \end{cases}
\end{gather*}
We show, that this choices satisfies Assumption~\ref{asu:objective_f}.
By definition it holds \textit{(i)}.
For the continuity in $0$ note, that $\lim_{x\to 0} x\log x=0$. 
Since the second derivative on $(0,\infty)$ is $x\mapsto 1/x$, by the second derivative test, $\varphi$ is strictly convex.
Clearly it is also continuously differentiable on $(0,\infty)$.
For \textit{(iii)} note, that $x\mapsto \log x$ is continuous and strictly non-decreasing on $\R$. 
Since 
\begin{align*}
  \lim_{x\to 0}\log x\ =\ -\infty
  \qquad
  \text{and}
  \qquad
  \lim_{x\to \infty}\log x\ =\ +\infty
  \,,
\end{align*}
and $\varphi^{'}=(x\mapsto \log x +1)$ on $(0,\infty)$, it follows \textit{(iii)}. 
Finally, it holds 
\begin{align*}
(\varphi^{'})^{-1}
\ 
=
\ 
(x\ \mapsto \ \exp(x-1))
\qquad
\text{on}\ \R
\,.
\end{align*}
Thus, it follows \textit{(iv)}.
\end{example}
