Is there a better estimator of the distribution function than the empirical distribution function?
Yes, a weighted empirical distribution function.
Is there a worse estimator of the distribution function than the empirical distribution function?
Yes, a weighted empirical distribution function.
It depends on the weights.

We made an effort to show that the weights of this method are consistent estimators of the best possible weights - the ones created from the inverse propensity score.
Now it's time to analyse the weighted distribution function - and hope for the best.
Loosely speaking, if 
\begin{align*}
  &
  \frac{1}{N}
  \sum_{i=1}^{n} 
  \frac{T_i}{\pi(X_i)}f(Y_i(T_i))
  \ 
  \overset{\P}
  {\to}
  \ 
  \E
  \left[ 
    f(Y(1))
  \right]
  \intertext{or}
  &
  \frac{1}{\sqrt{N}}
  \sum_{i=1}^{n} 
  \frac{T_i}{\pi(X_i)}f(Y_i(T_i))
  \qquad
  \text{converges in distribution}
\end{align*}
we expect, by the consistency of the weights for the inverse propensity score, the same behaviour of the weighted mean.
To be more precise, we could try the following error decomposition
\begin{align*}
  &
  \frac{1}{N}
  \sum_{i=1}^{n} 
  T_i\cdot w^\dagger_0(X_i)\cdot f(Y_i(T_i))
  \ 
  - 
  \ 
  \E
  \left[ 
    f(Y(1))
  \right]
  \\
  &
  \ 
  =
  \ 
  \frac{1}{N}
  \sum_{i=1}^{n} 
  T_i
  \left( 
  w^\dagger_0(X_i)
  -
  \frac{T_i}{\pi(X_i)}
  \right)f(Y_i(T_i))
  \ 
  + 
  \ 
  \left( 
  \frac{1}{N}
  \sum_{i=1}^{n} 
  \frac{T_i}{\pi(X_i)}f(Y_i(T_i))
  \ 
  -
  \ 
  \E
  \left[ 
    f(Y(1))
  \right]
  \right)
  \,.
\end{align*}
Clearly, the second term goes to 0 in probability.
Since the difference in the first term goes to 0, by the consistency of the weights, we would expect the first term also to be well behaved.
It turns out that something similar is the case (only the argument is much more involved).
But the high-level idea remains that the best possible weights, the propensity score weights, are well behaved and the weights  of Problem~\ref{bw:1:primal} approximate them (reasonably) well.

Throughout this section we use the following notation.
Let $F_{Y(1)}$ denote the distribution function of $Y(1)$, that is,
\index{$F_{Y(1)}$, distribution function of the potential outcome under treatment}
\begin{gather*}
  F_{Y(1)}
  \ 
  \colon
  \ 
  \R
  \ 
  \to
  \ 
  [0,1]
  \, 
  , 
  \qquad
  z
  \ 
  \mapsto
  \ 
  \P
  [
  Y(1)
  \le
  z
  ]
  \,.
\end{gather*}
Let $F_{Y(1)}(\cdot|x)$ denote the distribution function of $Y(1)$ conditional on $X=x\in\mathcal{X}$, that is,
\index{$F_{Y(1)}(\cdot \lvert x)$, conditional distribution function of the potential outcome under treatment}
\begin{gather}
  \label{3228}
  F_{Y(1)}
  (z|x)
\ 
  =
\ 
  \P
  [
  Y(1)
  \le
  z
  \,
  |
  \,
  X=x
  ]
  \qquad
  \text{for all}\ 
  (z,x)\in\R\times\mathcal{X}
  \,.
\end{gather}

We illustrate the flexibility of 
the weighted mean estimator by 
extending the method of \cite{Wang2019} to
estimates of 
the distribution function of $Y(1)$, that is, $F_{Y(1)}$.
For the asymptotic analysis of estimating the mean $\E[Y(1)]$ see \cite[Proof of Theorem~3]{Wang2019}.
To make this extension, the central observation is, that we can adapt the error decomposition in \cite[page 27]{Wang2019} 
to estimates of the distribution function $F_{Y(1)}$ of $Y(1)$.
We do this in Lemma~\ref{aa:mean:lemma_decomp}.
With this modification, we aim at proving
the convergence of
\begin{gather}
    \sqrt{N}
    \left( 
  \frac{1}{N}
    \sum_{i=1}^{n} 
    w_0^\dagger(X_i)
    \mathbf{1}\left\{ Y_i(T_i)\,\le\, z \right\}
    \ 
    -
    \ 
    F_{Y(1)}(z)
    \right)
    _{z\in\R}
    \,
  \end{gather}
  in
  $l^\infty(\R)$
  to a Gaussian process with mean 0 and covariance specified in Theorem~\ref{th:main}.
\section{Tools}
  \input{chapters/conv_mean/tools.tex}
\section{Main Result}
  \input{chapters/conv_mean/main_theo.tex}
\section{Error Decomposition}
  \input{chapters/conv_mean/error_decomp.tex}
\section{Analysis of the Error Terms}
  \subsection{Analysis of $R_1$}
    \input{chapters/conv_mean/r_1.tex}
  \subsection{Analysis of $R_2$}
    \input{chapters/conv_mean/r_2.tex}
  \subsection{Analysis of $R_3$}
    \input{chapters/conv_mean/r_3.tex}
  \subsection{Analysis of $R_4$}
    \input{chapters/conv_mean/r_4.tex}
  \subsection{Proof of Theorem~\ref{th:main}}
    \input{chapters/conv_mean/proof_final_theo.tex}
