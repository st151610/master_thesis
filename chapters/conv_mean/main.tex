Is there a better estimator of the distribution function than the empirical distribution function?
Yes, a weighted empirical distribution function.
Is there a worse estimator of the distribution function than the empirical distribution function?
Yes, a weighted empirical distribution function.
It depends on the weights.

In Chapter~4 we show that the optimal weights of Problem~\ref{bw:1:primal} are consistent estimators of the best possible weights - the inverse propensity score weights.
Now we want to use this to obtain good asymptotic properties of a weighted mean estimator.
To this end, let $Y_1,\ldots,Y_N$ be independent and identically distributed observed outcomes. To do this, we have to drop the order $T_i=1$ for $i\le n$ introduced in Chapter~2. 
Note that $Y_i=Y_i(T_i)$, where $(Y_i(0),Y_i(1))$ are the potential outcomes of unit $i$. 

We mentioned in the introduction to Chapter~2 that the weights work for different outcomes.
If Assumption~\ref{asu:treatment_asign_str_ing} holds, by the law of large numbers and by the central limit theorem it holds
\begin{align*}
  &
  \frac{1}{N}
  \sum_{i=1}^{N} 
  \frac{T_i}{\pi(X_i)}f(Y_i(T_i))
  \ 
  \overset{\P}
  {\to}
  \ 
  \E
  \left[ 
    f(Y(1))
  \right]
  \intertext{and}
  &
  \frac{1}{\sqrt{N}}
  \sum_{i=1}^{N} 
  \frac{T_i}{\pi(X_i)}f(Y_i(T_i))
  \qquad
  \text{converges in distribution}
  \,.
\end{align*}
By the consistency of the weights, we hope to recover the good asymptotic behaviour of the propensity score weights.
To prove this,
we could try the following error decomposition.
\begin{align*}
  &
  \frac{1}{N}
  \sum_{i=1}^{N} 
  T_i\cdot w^\dagger_0(X_i)\cdot f(Y_i(T_i))
  \ 
  - 
  \ 
  \E
  \left[ 
    f(Y(1))
  \right]
  \\
  &
  \ 
  =
  \ 
  \frac{1}{N}
  \sum_{i=1}^{N} 
  T_i
  \left( 
  w^\dagger_0(X_i)
  -
  \frac{T_i}{\pi(X_i)}
  \right)f(Y_i(T_i))
  \ 
  + 
  \ 
  \left( 
  \frac{1}{N}
  \sum_{i=1}^{N} 
  \frac{T_i}{\pi(X_i)}f(Y_i(T_i))
  \ 
  -
  \ 
  \E
  \left[ 
    f(Y(1))
  \right]
  \right)
  \,.
\end{align*}
Clearly, the second term goes to 0 in probability.
Since the difference in the first term goes to 0, by the consistency of the weights, we would expect the first term also to be well behaved.
It turns out that something similar is the case for an estimate of the distribution function of $Y(1)$ (only the argument is much more involved).
The high-level idea remains that the best possible weights, the propensity score weights, are well behaved and the weights of Problem~\ref{bw:1:primal} approximate them (reasonably) well.

Throughout this section we use the following notation.
Let $F_{Y(1)}$ denote the distribution function of $Y(1)$, that is,
\index{$F_{Y(1)}$, distribution function of the potential outcome under treatment}
\begin{gather*}
  F_{Y(1)}
  \ 
  \colon
  \ 
  \R
  \ 
  \to
  \ 
  [0,1]
  \, 
  , 
  \qquad
  z
  \ 
  \mapsto
  \ 
  \P
  [
  Y(1)
  \le
  z
  ]
  \,.
\end{gather*}
Let $F_{Y(1)}(\cdot|x)$ denote the distribution function of $Y(1)$ conditional on $X=x\in\mathcal{X}$, that is,
\index{$F_{Y(1)}(\cdot \lvert x)$, conditional distribution function of the potential outcome under treatment}
\begin{gather}
  \label{3228}
  F_{Y(1)}
  (z|x)
\ 
  =
\ 
  \P
  [
  Y(1)
  \le
  z
  \,
  |
  \,
  X=x
  ]
  \qquad
  \text{for all}\ 
  (z,x)\in\R\times\mathcal{X}
  \,.
\end{gather}

We illustrate the flexibility of 
the weighted mean estimator by 
extending the method of \cite{Wang2019} to
estimates of 
the distribution function of $Y(1)$, that is, $F_{Y(1)}$.
For the asymptotic analysis of estimating the mean $\E[Y(1)]$ see \cite[Proof of Theorem~3]{Wang2019}.
To make this extension, the central observation is, that we can adapt the error decomposition in \cite[page 27]{Wang2019} 
to estimates of the distribution function $F_{Y(1)}$ of $Y(1)$.
We do this in Lemma~\ref{aa:mean:lemma_decomp}.
With this modification, we aim at proving
the convergence of
\begin{gather*}
    \sqrt{N}
    \left( 
  \frac{1}{N}
    \sum_{i=1}^{n} 
    w_0^\dagger(X_i)
    \mathbf{1}\left\{ Y_i(T_i)\,\le\, z \right\}
    \ 
    -
    \ 
    F_{Y(1)}(z)
    \right)
    _{z\in\R}
    \,
  \end{gather*}
  in
  $l^\infty(\R)$
  to a Gaussian process with mean 0 and covariance specified in Theorem~\ref{th:main}.
\section{Tools}
  \input{chapters/conv_mean/tools.tex}
Now we are ready to state the main result.
\newpage
\section{Main Result}
  \input{chapters/conv_mean/main_theo.tex}
In the introduction to this section we talked about proof strategies. The next section gives an error decomposition that is central to proof.
It consists of four terms that we shall bound consecutively. 
\section{Error Decomposition}
  \input{chapters/conv_mean/error_decomp.tex}
\section{Analysis of the Error Terms}
  \subsection{Analysis of $R_1$}
    \input{chapters/conv_mean/r_1.tex}
  \subsection{Analysis of $R_2$}
    \input{chapters/conv_mean/r_2.tex}
  \subsection{Analysis of $R_3$}
    \input{chapters/conv_mean/r_3.tex}
  \subsection{Analysis of $R_4$}
    \input{chapters/conv_mean/r_4.tex}
  \subsection{Proof of Theorem~\ref{th:main}}
    \input{chapters/conv_mean/proof_final_theo.tex}
