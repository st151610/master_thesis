%%%%%%%%%%%%%%%%%%%ASSUMPTION 1%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


\begin{assumption}
  Assume, the following conditions hold:
  %
  \begin{subassumption}
    \label{assumption_1_i} 
    The minimizer 
    $
    \lambda_0 
    =
    \arg \min_{\lambda \in \Theta}
    \E
    \left[ 
      -T n 
      \rho 
      \left( 
      B(X)^T \lambda
      \right)
      +
      B(X)^T \lambda
    \right]
    $
    is unique,
    where 
    $\Theta \subseteq \R^n$ is the parameter space for $\lambda$.
  \end{subassumption}
  %
  \begin{subassumption}
    \label{assumption_1_ii} 
    The parameter space 
    $\Theta \subseteq \R^n$
    is compact.
  \end{subassumption}
  %
  \begin{subassumption}
    \label{assumption_1_iii}
    $\lambda_0 \in \text{int}(\Theta)$,
    where
    $\text{int}(\cdot)$
    stands for the interior of a set.
  \end{subassumption}
  %
  \begin{subassumption}
    \label{assumption_1_iv}
    There exists
    $\lambda^*_1 \in \Theta$
    such that
    $
      \norm{
        m^*(\cdot)
        -
        B(\cdot)^T \lambda^*_1
      }_\infty
      \le 
      \varphi_{m^*}
    $,
    where
    $
      m^*(\cdot)
      :=
      \left( \rho^{'} \right)^{-1}
      \left( \frac{1}{n \pi(\cdot)} \right).
    $
  \end{subassumption}
  %
  \begin{subassumption}
    \label{assumption_1_v}
    There exists a constant 
    $
      \varphi_{\pi} 
      \in 
      \left(0, \frac{1}{2} \right)
    $
    such that
    $
      \pi(x)
      \in 
      (
      \varphi_{ \pi },
      1 - \varphi_{ \pi }
      )
    $
    for all
    $
      x
      \in
      \mathcal{X}
    $
  \end{subassumption}
  %
  \begin{subassumption}
    \label{assumption_1_vi}
    There exists 
    $ \varphi_{\rho^{''}} > 0 $
    such that
    $ -\rho^{''} \ge \varphi_{\rho^{''}} > 0 $
  \end{subassumption}
  %
  \begin{subassumption}
    \label{assumption_1_vii}
    There exists 
    $ \varphi_{B(x) B(x)^T} > 0 $ 
    such that
    $
      B(x) B(x)^T 
      \succcurlyeq 
      \varphi_{B(x) B(x)^T} I 
    $
  \end{subassumption}
  %
  \begin{subassumption}
    \label{assumption_1_viii}
    There exists
    $ \varphi_{\norm{B}} > 0 $
    such that
    $
      \sup_{x \in \mathcal{X}} \norm{B(x)}_2
      \le 
      \varphi_{\norm{B}}
    $.
  \end{subassumption}
  %
  \begin{subassumption}
    \label{assumption_1_ix}
    The number of basis functions
    satisfies
    $
      K = o(n)
    $.
  \end{subassumption}
\end{assumption}



We study the following problem:
%%%%%%%%%%%%%%%%%%
% PROBLEM PRIMAL %
%%%%%%%%%%%%%%%%%%
\begin{problem}
  \begin{gather*}
    \underset{w \in \R^n}{\text{minimize}}
    \qquad
    \sum_{i = 1}^{n} T_i f(w_i)
  \end{gather*}
subject to
\begin{align}
  \label{primal_weighting_binary}
  \begin{split}
    w_i &\ge 0
    \\
    \sum_{i = 1}^{n} 
      w_i
    &=
    1
    \\
    \left| 
      \sum_{i = 1}^{n} w_i T_i B_k(X_i)
      - 
      \frac{1}{n} \sum_{i = 1}^{n} B_k(X_i)
    \right|
    &\le 
    \delta_k,\,
    k = 1, \ldots, K
  \end{split}
\end{align}
\end{problem}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

We aim to prove that the solution to 
Problem \eqref{primal_weighting_binary}
is asymptotical consistent with the propensity score, i.e.

\begin{theorem}
  \label{ch_theorem_1}
  Under some (non-optimal) Assumptions,
  there exist constants
  $
    c_{1}
    , 
    c_{2}
    >
    0
  $
  and decreasing sequences
  $
    (\varepsilon_n^1)
    ,
    (\varepsilon_n^2)
    \subset
    (0,1]
  $
  that converge to 0
  such that
  for all
  $
    \tau
    \in
    (0,1]
  $
  there exists a constant
  $
    c_{\tau}
    \in
    [0,\infty)
  $
  only depending on 
  $ \tau $
  such that for all 
  $ n \ge 1 $
  and
  $
    \tau
    \in
    (0,1]
  $
  it holds
  \begin{align}
    \label{ch_theorem_1_alternative}
    \begin{split}
   & \P
    \left( 
  \norm{
    w^*_i
    -
    \frac{1}{n \pi(X_i)}
  }_\infty
  \le 
  c_{1}
  c_{\tau}
  \varepsilon_{n}^1 
   \right)
   \ge
  1 - \tau
  ,
\\
   &\quad \
   \norm{
    w^*_i
    -
    \frac{1}{n \pi(X_i)}
  }_{\P, 2}
  \le 
  c_{2}
  \varepsilon_n^{2},
      \end{split}
  \end{align}
  where 
  $w^*$
  is the solution to 
  Problem \eqref{primal_weighting_binary}.
\end{theorem}

\section*{Plan of Proof}

It is easier to study the dual of 
Problem \eqref{primal_weighting_binary}.
Thus we employ results from convex analysis
\cite{Mordukhovich2022}
to establish
\begin{proposition}
  \label{ch_1_dual}
  The dual of Problem \eqref{primal_weighting_binary}
  is equivalent to the unconstrained optimization problem
  \begin{gather}
    \label{dual_weighting_binary}
    \underset{\lambda \in \R^K}{\text{minimize}}
    \quad
    \frac{1}{n}
    \sum_{j = 1}^{n} 
      \left[ 
        -T_j n 
        \rho 
        \left( 
          B(X_j)^T \lambda
        \right)
        +
        B(X_j)^T \lambda
      \right]
    +
    |\lambda|^T \delta,
  \end{gather}
  where 
  $
    B(X_j)=(B_k(X_j))_{1 \le k \le K}
  $
  denotes the $K$ basis functions of the covariates,
  $
    \rho(t) 
    :=
    \frac{t}{n}
    -
    t
    (h^{'})^{-1}(t)
    +
    h((h^{'})^{-1}(t))
  $
  with
  $
    h(x)
    :=
    f%
    \left( 
      \frac{1}{n}
      -
      x
    \right)
  $
  and
  $
    | \lambda | 
    :=
    \left(
    | \lambda_k | 
    \right)_{1 \le k \le K}
  $.
  Moreover,
  the primal solution
  $
    w^*_j
  $
  satisfies
  \begin{gather}
    \label{ch_1_w*}
    w^*_j
    =
    \rho^{'}
    \left(
      B(X_j)^T
      \lambda^\dagger
    \right)
  \end{gather}
  for 
  $
    j
    =
    1, \ldots , n
  $,
  where 
  $
    \lambda^\dagger
  $
  is the solution to the dual optimization problem.
\end{proposition}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

The core of the subsequent analysis is based on
Assumption~\ref{assumption_1_iv}, 
i.e.
the existence of an oracle parameter
$\lambda_1^{*}$
in a sieve estimate of the true propensity score (or a transformation). 
It is then natural to enquire about the convergence of the dual solution
$
  \lambda^\dagger
$
to 
$
  \lambda^*_1
$.
Making certain assumptions and employing
matrix concentration inequalitys \cite{Tropp2015}
we can establish
\begin{proposition}
  \label{ch_1_near_oracle}
  Under some (non-optimal) Assumptions,
  there exists a constant
  $
    c_{3}
    >
    0
  $
  and a decreasing sequence
  $
    (\varepsilon_n^3)
    \subset
    (0,1]
  $
  that converges to 0
  such that
  for all
  $
    \tau
    \in
    (0,1]
  $
  there exists a constant
  $
    \tilde{
    c_{\tau}
    }
    \in
    [0,\infty)
  $
  only depending on 
  $ \tau $
  such that for all 
  $ n \ge 1 $
  and
  $
    \tau
    \in
    (0,1]
  $
  it holds
  \begin{gather}
    \P
    \left( 
      \norm{
        \lambda^\dagger
        -
        \lambda^*_1
      }_2
      \le
      c^3
      \tilde{
        c_{\tau}
      }
      (
        \varepsilon_n^3
      )
    \right)
    \ge 
    1 - \tau.
  \end{gather}
\end{proposition}%
It is then straightforward to prove a more general result
than
Theorem~\ref{ch_theorem_1}%
.
%
\begin{theorem}
  Under some (non-optimal) Assumptions,
  there exist constants
  $
    c_{1}
    , 
    c_{2}
    >
    0
  $
  and decreasing sequences
  $
    (\varepsilon_n^1)
    ,
    (\varepsilon_n^2)
    \subset
    (0,1]
  $
  that converge to 0
  such that
  for all
  $
    \tau
    \in
    (0,1]
  $
  there exists a constant
  $
    c_{\tau}
    \in
    [0,\infty)
  $
  only depending on 
  $ \tau $
  such that for all 
  $ n \ge 1 $
  and
  $
    \tau
    \in
    (0,1]
  $
  it holds
  \begin{align*}
   & \P
    \left( 
  \norm{
    w^*(\cdot)
    -
    \frac{1}{n \pi(\cdot)}
  }_\infty
  \le 
  c_{1}
  c_{\tau}
  \varepsilon_{n}^1 
   \right)
   \ge
  1 - \tau
  ,
\\
   &\quad \
   \norm{
    w^*(X)
    -
    \frac{1}{n \pi(X)}
  }_{\P, 2}
  \le 
  c_{2}
  \varepsilon_n^{2},
  \end{align*}
  where 
  $w^*(X)$
  is as in \eqref{ch_1_w*}
  without the index.
\end{theorem}

\section*{Proof of theorem 2.2}


\begin{proof}
  Motivated by Proposition~\ref{syu_1_result}
  we consider
 \begin{gather}
   \label{ch_1_G}
   G(\lambda)
   :=
      \frac{1}{n}
      \sum_{j = 1}^{n} 
      \left[ 
        -T_j n 
        \rho 
        \left( 
          B(X_j)^T \lambda
        \right)
      +
      B(X_j)^T \lambda
      \right]
      +
      |\lambda|^T \delta.
 \end{gather} 
 Since 
 $\rho \in C^2(\R)$
 we can employ 
 \eqref{ch_1_G},
 Corollary~\ref{syu_taylor_corollary}
 and
 Proposition~\ref{syu_triangle}
 to get
 \begin{align}
   \label{ch_1_proof1_1}
  \begin{split}
  & 
  G(\lambda^*_1 + \Delta) 
  -
  G(\lambda^*_1)
  \\
  &
  \quad
  \ge
      \frac{1}{n}
      \sum_{j = 1}^{n} 
      \left[ 
        -T_j n 
        \rho^{'} 
        \left( 
          B(X_j)^T \lambda^*_1
        \right)
      +
      1
      \right]
      \Delta^T B(X_j)
      \\
  & 
  \qquad
  +
      \frac{1}{2}
      \sum_{j = 1}^{n} 
        -T_j  
        \rho^{''} 
        \left( 
          B(X_j)^T (\lambda^*_1 + \xi \Delta)
        \right)
        \Delta^T
        \left( 
          B(X_j)
          B(X_j)^T
        \right)
        \Delta
        \\
  &
  \qquad
  -
      |\Delta|^T \delta
  \\
  &  
  \quad
  \ge
    - \norm{\Delta}_2
    \left( 
    \norm{
      \frac{1}{n}
      \sum_{j = 1}^{n} 
      \left[ 
        -T_j n 
        \rho^{'} 
        \left( 
          B(X_j)^T \lambda^*_1
        \right)
      +
      1
      \right]
      B(X_j)
    }_2
    +
    \norm{\delta}_2
    \right)
    \\
  & 
  \qquad
  +
  n
  \norm{\Delta}^2_2
   \varphi_{\rho^{''}}
  \underline{\varphi_{BB^T}}
  \\
  &
  \quad
  :=
  -\norm{\Delta}_2
  (I_1 + \norm{\delta}_2)
  +
  \norm{\Delta}^2_2
  I_2
  .
  \end{split}
\end{align}
The second inequality is due to 
the Cauchy-Schwarz-Inequality 
and
Assumptions~\ref{assumption_1_vi} and \ref{assumption_1_vii}
.
We want to establish probabilistic upper bounds of the factor associated with 
$
  -\norm{\Delta}_2
$. 
This will be done with appropriate assumptions on
$
  \norm{\delta}_2
$
and a thorough analysis of $I_1$.
If we then restrict lower bounds of
$
  I_2
$
to approprietely slow convergence to 0,
e.g. by assumptions on
$
\varphi_{\rho^{''}}
$
and
$
\underline{
  \varphi_{BB^T}
}
$,
we can choose
$
  \norm{\Delta}_2
$
large enough,
such that 
\eqref{ch_1_proof1_1}
yields
$
  G(\lambda^*_1 + \Delta) 
  -
  G(\lambda^*_1)
  >
  0
$
with arbitrarily large probabitity for $n$ large enough.
With Proposition~\ref{syu_1_result} it follows then immediately
Proposition~\ref{ch_1_near_oracle}
.
\subsection*{Analysis of $I_1$}
We want to use Assumption~\ref{assumption_1_iii}.
Thus we perform the following split:
\begin{align}
  \label{ch_1_i1}
  \begin{split}
  I_1 
  &\le
    \norm{
      \sum_{j = 1}^{n} 
        T_j  
      \left[ 
        \rho^{'} 
        \left( 
          B(X_j)^T \lambda^*_1
        \right)
      -
      \frac{1}{n \pi(X_j)}
      \right]
      B(X_j)
    }_2
  \\
  &+
    \norm{
      \frac{1}{n}
      \sum_{j = 1}^{n} 
      \left[ 
        \frac{T_j}{\pi(X_j)}
      -
      1
      \right]
      B(X_j)
    }_2
    \\
  &=:
  J_1 + J_2
  \end{split}
\end{align}

\subsubsection*{Analysis of $J_1$}

By the Lipschitz-continuity of 
$\rho^{'}$,
Assumption~\ref{assumption_1_viii}
and
Assumption~\ref{assumption_1_iv},
$T \in \{0, 1\}$
and 
the triangle inequality 
we have
\begin{gather}
  \label{ch_1_j1_result}
  J_1 
  \le
  n L_{\rho^{'}}\varphi_{\norm{B(x)}} \varphi_{m^*}
\end{gather}

\subsubsection*{Analysis of $J_2$}
We want to employ 
Theorem~\ref{rmineq_bernstein}%
. %
To this end we define
the independent random matrices
\begin{align}
  \label{ch_1_j2_1}
  \begin{split}
  A_j
  &:=
      \frac{1}{n}
      \left[ 
        \frac{T_j}{\pi(X_j)}
      -
      1
      \right]
      B(X_j)
      \,,
      \quad 
      j=1,\ldots,n,
\\
  S
  &:=
  \sum_{j = 1}^{n} 
  A_j
  \end{split}
\end{align}
and check conditions 
\eqref{rmineq_bernstein_cond_1} 
and
\eqref{rmineq_bernstein_cond_2}.
Note that 
$
  \norm{S}_2
  =
  J_2
$
.
By the properties of conditional expectation 
it holds
\begin{gather}
  \label{ch_1_j2_2}
  \E
  \left[  
    \frac{T_j}{\pi(X_j)}
    B(X_j)
  \right]
  =
  \E
  \left[  
    \E
    \left[  
      T_j
      \, | \,
      X_j
    \right]
    \frac{1}{\pi(X_j)}
    B(X_j)
  \right]
  =
  \E[B(X_j)]. 
\end{gather}
Taking the expectation in 
\eqref{ch_1_j2_1} 
and using
\eqref{ch_1_j2_2}
we get
$\E[A_j] = 0$
for all 
$
  j=1,\ldots,n
$
.
Since
\begin{gather}
  \label{ch_1_j2_3}
  \left| 
      \frac{T_j}{\pi(X_j)}
      -
      1
      \right|
  \le
  1 + \frac{1 - \varphi_{\pi}}{\varphi_{\pi}}
  =
  \frac{1}{\varphi_{\pi}}
\end{gather}
by Assumption~\ref{assumption_1_v},
we can employ Assumption~\ref{assumption_1_viii}
together with 
\eqref{ch_1_j2_3}
and
\eqref{ch_1_j2_1}
to get
\begin{gather}
  \label{ch_1_j2_4}
  \norm{A_j}_2
  \le
  \frac{\varphi_{\norm{B}}}{n \varphi_{\pi}}
  =:
  L.
\end{gather}
Thus,
condition \eqref{rmineq_bernstein_cond_1}
is satisfied.
Next we turn to the matrix variance statistic
$ v(S) $
\eqref{rmineq_bernstein_cond_2}.
By 
\eqref{ch_1_j2_1}
and
\eqref{ch_1_j2_3}
we have
\begin{gather}
  \label{ch_1_j2_5}
  \E
  \left[ 
    A_j A_j^T
  \right]
  \le
  \left( 
    \frac{1}{n \varphi_{\pi}}
  \right)^2
  \E
  \left[ 
    B(X) B(X)^T
  \right]
\end{gather}
and by \eqref{ch_1_j2_4} 
\begin{gather}
  \label{ch_1_j2_6}
  \E
  \left[ 
    A_j^T A_j
  \right]
  \le
  L^2.
\end{gather}
Since 
$
  \max\{a,b\}
  \le
  | a | + | b |
$
we can use 
\eqref{ch_1_j2_5}
and
\eqref{ch_1_j2_6}
to get
\begin{gather}
  \label{ch_1_j2_7}
  v(S)
  \le
  \frac{1}{n}
  \frac{\lambda_{\max}}{\varphi_{\pi}^2}
  +
  n L^2,
\end{gather}
where 
$\lambda_{\max}$ 
is the maximal eigenvalue of
the symmetric (non-random) matrix
$
  \E
  \left[ 
    B(X) B(X)^T
  \right]
$.
Having dealt with 
\eqref{rmineq_bernstein_cond_1} 
and
\eqref{rmineq_bernstein_cond_2}
we can establish 
the
expectation bound
\eqref{rmineq_bernstein_expectation_bound}
of Theorem~\ref{rmineq_bernstein}.
Together with 
\eqref{ch_1_j2_4}
and
\eqref{ch_1_j2_7}
we get
\begin{align}
  \label{ch_1_j2_8}
  \begin{split}
    &\E[J_2]
\\
  &\le
  \sqrt{
    \frac{
    2 \log (K + 1)
    \left( 
      \lambda_{\max}  + \varphi_{\norm{B}}^2
    \right)
    }
    {
      n \varphi_{\pi}^2
    }
  }
  +
  \frac{
    \log (K + 1)
    \varphi_{\norm{B}}
  }
  {
    3 n \varphi_{\pi}
  }
\\
  &\le
  \frac{1}{\varphi_{\pi}}
  \sqrt{
    \frac{
      \log(K + 1)
    }
    {
      n
    }
  }
  \left[
    \varphi_{\norm{B}}
    \left(
      \sqrt{2}
      +
      \frac{1}{3}
      \sqrt{
        \frac{\log (K + 1)}{n}
      }
    \right)
    +
    \sqrt{
      2
      \lambda_{\max}
    }
  \right].
\end{split}
\end{align}
Since
$
  K = o(n)
$
by Assumption~\ref{assumption_1_ix}
we can discuss the other influences on the quality of the bound
\eqref{ch_1_j2_8}.
On a high-level it is readily clear
that appropriate bounds on
$
\varphi_{\pi},
\varphi_{\norm{B}}
$
and
$
  \lambda_{\max}
$
will shrink
$
  \E[J_2]
$
to 0 
and will assist in establishing learning rates.

We could also have invoked the probabitity bound 
\eqref{rmineq_bernstein_probability_bound} 
of Theorem~\ref{rmineq_bernstein}.
But for the sake of simplicity we prefer the combination 
of the expectation bound 
\eqref{ch_1_j2_8}
and the Markov inequality.
With the latter we get
\begin{align}
  \label{ch_1_j2_result}
  J_2
  \le
  \frac{1}{\tau}
  \frac{1}{\varphi_{\pi}}
  \sqrt{
    \frac{
      \log(K + 1)
    }
    {
      n
    }
  }
  \left[
    \varphi_{\norm{B}}
    \left(
      \sqrt{2}
      +
      \frac{1}{3}
      \sqrt{
        \frac{\log (K + 1)}{n}
      }
    \right)
    +
    \sqrt{
      2
      \lambda_{\max}
    }
  \right]
\end{align}
with probabitity 
$
  \ge
  1 - \tau
$.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


If we choose
$
\norm{\Delta}_2
$
to be
\begin{align}
  \label{ch_1_choice_norm_Delta}
  \begin{split} 
    &\left(
     \sqrt{2}
    \frac{1}{\tau}
    \frac{1}{\varphi_{\pi}}
    \sqrt{
      \frac{
        \log(K + 1)
      }
      {
        n^3
      }
    }
    \left[
      \varphi_{\norm{B}}
      \left(
        1
        +
        \sqrt{
          \frac{\log (K + 1)}{n}
        }
      \right)
      +
      \sqrt{
        \lambda_{\max}
      }
  \right]
  \right.
\\
  &
  \quad
  +
     L_{\rho^{'}}
     \varphi_{\norm{B}}
     \varphi_{m^*}
  +
  \left.
  \frac{
    \norm{\delta}_2
  }
  {n}
  \right)
  \frac{1}
  {
    \varphi_{\rho^{''}}
    \underline{
      \varphi_{BB^T}
    }
  }
\end{split}
\end{align}
we get by
\eqref{ch_1_proof1_1},
\eqref{ch_1_i1},
\eqref{ch_1_j1_result},
\eqref{ch_1_j2_result}
and 
Proposition~\ref{syu_1_result}
\begin{align}
  \label{ch_1_near_oracle_probabilistic_bound}
  \begin{split}
  \P
  \left( 
      \norm{
        \lambda^\dagger
        -
        \lambda^*_1
      }_2
      \le
      C 
  \right)
  &=
  \P
  \left( 
    \inf_{\norm{\Delta}_2 = C}
    G(\lambda^*_1 + \Delta) 
    -
    G(\lambda^*_1)
    >
    0
  \right)
  \\
  &\ge
  1 - \tau,
  \end{split}
\end{align}
where 
$
  C
$
is as in 
\eqref{ch_1_choice_norm_Delta}.
With appropriate Assumptions (as discussed before) we can then establish
Proposition~\ref{ch_1_near_oracle}.

We can invoke 
\eqref{ch_1_near_oracle_probabilistic_bound}
to derive bounds as in 
Theorem~\ref{ch_theorem_1_alternative}:
\begin{align*}
  \norm{
    w^*(X)
    -
    \frac{1}{n \pi(X)}
  }_{\P, 2}
  &\le
  L_{\rho^{'}}
  \left[ 
    \norm{
      B(X)^T 
      \left(
        \lambda^\dagger
        -
        \lambda^*_1
      \right)
    }_{\P,2}
    \right.
    \\
    &+
    \left. 
    \norm{
      m^*(X)
      -
      B(X)^T 
      \lambda^*_1
    }_{\P,2}
  \right]
  \\
  &\le
  L_{\rho^{'}}
  \left(
    \varphi_{\norm{B}}
    \sqrt{
      C^2(1 - \tau)
      +
      \text{diam}(\Theta)^2
      \tau
    } 
    +
    \varphi_{m^*}
  \right)
\end{align*}
%%%%%%%%%%%%%%%%%%%
\begin{align*}
  \norm{
    w^*(\cdot)
    -
    \frac{1}{n \pi(\cdot)}
  }_{\infty}
  &\le
  L_{\rho^{'}}
  \left[ 
    \norm{
      B(\cdot)^T 
      \left(
        \lambda^\dagger
        -
        \lambda^*_1
      \right)
    }_{\infty}
    \right.
    \\
    &+
    \left. 
    \norm{
      m^*(\cdot)
      -
      B(\cdot)^T 
      \lambda^*_1
    }_{\infty}
  \right]
  \\
  &\le
  L_{\rho^{'}}
  \left(
    \varphi_{\norm{B}}
    C
    +
    \varphi_{m^*}
  \right)
\end{align*}
with probabitity greater than 
$1 - \tau$.
\end{proof}
\begin{remark}
  By Corollary~\ref{rmineq_intrinsic_bernstein_vector_corollary}
  we can get rid of the $\log(K)$ term in
\eqref{ch_1_choice_norm_Delta}.
\end{remark}
\begin{remark}
  By the matrix Rosenthal-Pinelis Inequality
  \cite{Chen2012}[Thm.A.1]
  we can weaken Assumption~\ref{assumption_1_v} to a lower bound on the expectition of 
  $\pi(X)$
\end{remark}
The next step consists of strenghtening the Assumptions to get concrete learning rates.
This can be done in a series of examples.
