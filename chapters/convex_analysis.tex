%%%%%%%%%%%%%%%%
% INTRODUCTION %
%%%%%%%%%%%%%%%%
In the following we do not expect the reader to be familiar with convex analysis. However, some very well known results will be stated without proof. The interested reader can study \cite{Mordukhovich2022} for the bedrock analysis.


We begin by defining convex sets
%

\begin{definition}
  \emph{(Convex Set)}
  A subset $\Omega\subseteq \R^n$ is called \textbf{convex} if we have 
  \begin{gather}
    \lambda x + (1-\lambda) y\in \Omega 
    \quad
    \text{for all}\ 
    x,y\in \Omega 
    \
    \text{and}\ 
    \lambda\in (0,1). 
  \end{gather}
\end{definition}

Clearly, the line segment 
$[a,b]:=\left\{ \lambda a+(1-\lambda)b\,\mid \, \lambda\in [0,1] \right\}$ is contained in $\Omega$ for all $a,b\in \Omega$ if and only if $\Omega$ is a convex set.
%

Next we define convex functions. 
%

The concept of convex functions is closely related to convex sets.
%  
 
The line segment between two points on the graph of a convex function lies on or above and does not intersect the graph.
%

In other words: The area above the graph of a convex function $f$ is a convex set, i.e. the \textit{epigraph}
$\text{epi}(f):=\left\{ (x,\alpha)\in \R^n\times\R\,\mid\, f(x)\le \alpha\right\}$ is a convex set in $\R^{n+1}$.
%

Often an equivalent characterisation of convex functions is more useful.
%

\begin{theorem}
  The convexity of a function $f:\R^n\to \overline{\R}$ on $\R^n$ is equivalent to the following statement:

  For all $x,y\in \R^n$ and $\lambda\in(0,1)$ we have 
    \begin{align}
      f(\lambda x + (1-\lambda)y)\le \lambda f(x)+(1-\lambda)f(y).
    \end{align}
\end{theorem}


\begin{definition}
  \emph{proper convex function}
\end{definition}


%%%%%%%%%%%%%%%%%%%%%%
% CONJUGATE CALCULUS %
%%%%%%%%%%%%%%%%%%%%%%
\section{Conjugate Calculus}
When studying different primal problems such as \eqref{primal_weighting_binary} we often turn to the dual instead.
Therefore we need some reliable tools.
Begin able to compute specific convex conjugates is one tool required.


\begin{definition}
  \label{ def_convex_conjugate }
  \emph{(Convex conjugate)}
  Given a function
  $
    f:
    \R^n \to \overline{\R}
  $
  ,
  the 
  \textbf{convex conjugate}
  $
    f^*:
    \R^n \to \overline{\R}
  $
  of $f$ is defined as
  \begin{gather}
    f^*(x^*)
    :=
    \sup_{ x \in \R^n }
    (x^*)^T x - f(x)
  \end{gather}
\end{definition}

Note that $f$ in Definition~\ref{ def_convex_conjugate }
does not have to be convex. On the other hand, the convex conjugate is always convex:

\begin{proposition}
  Let  
  $
    f:
    \R^n \to ( - \infty, \infty ]
  $
  be a proper function. 
  Then its convex conjugate
  $
    f^*:
    \R^n \to ( - \infty, \infty ]
  $
  is convex.
\end{proposition}

\begin{definition}
  Given a nonempty subset 
  $\Omega \subseteq \R^n$
  the \textbf{support function} 
  $
  \sigma_\Omega : \R^n \to \overline{\R}
  $
  of $\Omega$
  is defined by
  \begin{gather}
    \sigma_\Omega
    (x^*)
    :=
    \sup_{x \in \Omega}
    \ 
    \inner{x^*}{x}
    \qquad
    \text{for}\ 
    x^* \in \R^n
    .
  \end{gather}
\end{definition}

\begin{lemma}
  For any proper function
  $
    f:\R^n\to\overline{\R}
  $
  we have
  \begin{gather}
    f^*(x^*) 
    =
    \sigma_{\mathrm{epi}(f)}
    (x^*,-1)
    \qquad
    \text{for}
    \ 
    x^* \in \R^n.
  \end{gather}
\end{lemma}
\begin{proof}
  Let $x^*\in\R^n$
  and
  $
    (x,\lambda)\in \mathrm{epi}(f).
  $
  Then
  $
    x \in \mathrm{dom}(f)
  $
  and
  $
    f(x)\le \lambda.
  $
  Thus
  \begin{gather}
    \inner{x^*}{x} - f(x)
    \ge
    \inner{x^*}{x} - \lambda
    \qquad
    \text{for all}\ 
    (x,\lambda)\in \mathrm{epi}(f).
  \end{gather}
  On the other hand 
  $
    (x,f(x))\in \mathrm{epi}(f)
  $
  for all
  $
    x \in \mathrm{dom}(f).
  $
  It follows
  \begin{gather}
    \inner{x^*}{x} - f(x)
    \le
    \sup_{(x,\lambda)\in\mathrm{epi}(f)}
    \inner{x^*}{x} - \lambda
    \qquad
    \text{for all}\ 
    x \in \mathrm{dom}(f).
  \end{gather}
  Taking the supremum in the last two displays yields
  \begin{align}
    f^*(x^*)
    =
    \sup_{x\in\mathrm{dom}(f)}
    \inner{x^*}{x} - f(x)
    &=
    \sup_{(x,\lambda)\in\mathrm{epi}(f)}
    \inner{x^*}{x} - \lambda
    \\
    &=
    \sup_{(x,\lambda)\in\mathrm{epi}(f)}
    \inner{(x^*,-1)}{(x,\lambda)} 
    =
    \sigma_{\mathrm{epi}(f)}
    (x^*,-1).
  \end{align}
\end{proof}
% conjugate chain rule %
 %%%%%%%%%%%%%%%%%%%%%%
\begin{proposition}

\end{proposition}
\begin{theorem}
  \emph{(Conjugate Chain Rule)}
  \label{cvxa_conjugate_chain_rule}
  Let 
  $
    A:
      \R^m \to \R^n
  $
  be a linear map (matrix)
  and
  $
    g:
      \R^n \to (-\infty, \infty]
  $
  a proper convex function. If
  $
    \text{Im}(A) \cap \text{ri}(\text{dom}(g))
    \neq
    \emptyset
  $
  it follows
  \begin{gather}
    ( g \circ A )^* ( x^* )
    =
    \inf_
          { y^* \in ( A^* )^{ -1 } ( x^* )}
                                          g^*( y^* )
                                          .
  \end{gather}
  Furthermore, 
    for any 
      $
        x^* \in \text{dom}( g \circ A)^*
      $
        there exists
          $
            y^* \in ( A^* )^{ -1 } ( x^* )
          $
            such that
              $
                ( g \circ A)^* ( x^* )
                =
                g^*( y^* )
              $.
\end{theorem}

% conjugate sum rule %
 %%%%%%%%%%%%%%%%%%%%

\begin{definition}
  \emph{(Infimal convolution)}
  Given functions
  $
    f_i:
    \R^n \to (-\infty, \infty]
  $
  for $ i = 1, \ldots, n $
  the \textbf{infimal convolution} of these functions as defined as
  \begin{gather}
    (f_1 \square \ldots \square f_m)(x)
    :=
    \inf_{
    \begin{smallmatrix}
      x_i \in \R^n \\
      \sum_{i = 1}^{m} 
        x_i
      =
      x
    \end{smallmatrix}
    }
    \sum_{i = 1}^{m}
      f_i(x_i)
  \end{gather}
\end{definition}


\begin{theorem}
  Let
  $
    f,g:
    \R^n \to (-\infty, \infty]
  $
  be proper convex functions 
  and
  $
  \text{ri}\left( \text{dom}(f) \right)
  \cap
  \text{ri}\left( \text{dom}(g) \right)
  \neq 
  \emptyset
  .
  $
  Then we have the conjugate sum rule
  \begin{gather}
    ( f + g )^*(x^*)
    =
    ( f^* \square g^*)(x^*)
  \end{gather}
  for all $x^* \in \R^n$.
  Moreover, the infimum in 
  $
    ( f^* \square g^*)(x^*)
  $
  is attained, i.e., for any
  $
    x^* \in \text{dom}(f+g)^*
  $
  there exists vectors $x_1^*, x_2^*$
  for which
  \begin{gather}
    (f+g)^*(x^*)
    =
    f^*(x_1^*)
    +
    g^*(x_2^*),
    \quad
    x^* = x_1^* + x_2^*.
  \end{gather}
\end{theorem}


%%%%%%%%%%%%%%%%%%%
% FENCHEL DUALITY %
%%%%%%%%%%%%%%%%%%%
\section{Fenchel Duality}


Given 
proper convex functions $f, g : \R^n \to \overline{\R}$ 
and
a matrix $A \in \R^{n \times n}$,
we define 
the primal minimization problem as follows:
\begin{problem}
  \emph{(Primal)}
  Given proper convex functions 
  $
  f:\R^n \to \overline{\R} 
  $,
  $
  g:\R^m \to \overline{\R} 
  $
  and a matrix 
  $
    A\in \R^{m \times n}
  $
  we define the \textbf{primal optimization problem} to be
  \label{cvxa_primal_problem}
  \begin{gather*}
    \underset{x \in \R^n}{\mathrm{minimize}}
    \qquad
    f(x) + g(Ax)
  \end{gather*}
\end{problem}
\begin{remark}
  Problem \autoref{cvxa_primal_problem}
  appears in the unconstrained form. We can impose constraints by controling for the domains of $f$ and $g$.
  To incorporate linear constraints $Ax \le 0$
  or more general constraints $x\in\Omega$, where $\Omega$ is a convex set,
  we can choose
  \begin{gather}
    g(x)=\delta_\Omega(x):=
  \end{gather}
  where $x\notin\Omega$
  leads to 
  $f(x) + g(x)=\infty$
  and the optimization problem (if feasible) will exclude $x$ from the solutions.
\end{remark}
\begin{problem}
  \emph{(Dual)}
  \label{cvxa_dual_problem}
  Consider the same setting as in Problem~\autoref{cvxa_primal_problem}.
  Using the convex conjugates of $f,g$ and the transpose of $A$
  we define the \textbf{dual problem} of Problem~\autoref{cvxa_primal_problem} to be
  \begin{gather*}
    \underset{y^* \in \R^m}{\mathrm{maximize}}
    \qquad
   - f^*(A^\top y^*) - g^*(y^*).
  \end{gather*}
\end{problem}
\begin{proposition}
  Consider the optimization problem~\autoref{cvxa_primal_problem} and its dual~\autoref{cvxa_dual_problem}, where the functions $f$ and $g$ are not assumed to be convex. Define the \textbf{optimal values} of these problems by
  \begin{gather*}
    \widehat{p}:= \inf_{x\in\R^n}f(x)+g(Ax)
    \quad
    \text{and}
    \quad
    \widehat{d}:= \sup_{y\in\R^m} - f^*(A^\top y) - g^*(y).
  \end{gather*}
  Then we have the relationship
  $\widehat{d}\le \widehat{p}$.
\end{proposition}
\begin{proof}
  It holds
  \begin{align*}
    - f^*(A^\top y^*) - g^*(y^*) 
    &=
      -\sup_{x\in \R^n}\inner{A^\top y^*}{x}-f(x)
      -\sup_{y\in \R^m}\inner{-y^*}{y}-g(y)
      \\
    &=
    \inf_{x\in \R^n}f(x)-\inner{y^*}{Ax}
      +\inf_{y\in \R^m}g(y)+\inner{y^*}{y}
      \\
    &\le
      \inf_{x\in \R^n}f(x)-\inner{y^*}{Ax}
      +\inf_{x\in \R^n}g(Ax)+\inner{y^*}{Ax}
      \\
    &\le
      \inf_{x\in \R^n}f(x)-\inner{y^*}{Ax} + g(Ax) + \inner{y^*}{Ax}
      \\
    &=  
      \inf_{x\in \R^n}f(x)+g(Ax)
    =
      \widehat{p}
  \end{align*}
  The first equality is due to the definition of convex conjugates, the second equality due to $\inner{A^\top y}{x}=\inner{y}{Ax}$ and $\inf \left\{ -B \right\}=-\sup \left\{ B \right\}$ for all $B \subseteq \overline{\R}$ and the first inequality due to $\mathrm{Im}(A)\subseteq \R^m$.
  Taking the supremum with respect to all 
  $y^*\in\R^m$
  yields the result.
\end{proof}
\begin{theorem}
  \label{cvxa_fenchel_theorem}
  Let 
  $f, g : \R^n \to \overline{\R}$ 
  be proper convex functions
  and
  $0 \in \text{ri}(\text{dom}(g) - A (\text{dom}(f)) )$
  .
  Then the optimal values of \eqref{cvxa_primal_problem} and \eqref{cvxa_dual_problem} are equal, 
  i.e.
  \begin{gather}
    \inf_{x \in \R^n} 
    \left\{ f(x) + g(Ax) \right\}
    =
    \sup_{y \in \R^n} \left\{   -f^* \left( A^T y \right) - g^*(-y) \right\}
    .
  \end{gather}
\end{theorem}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{lemma}
  \label{syu_1_result}
  Let 
  $f : \R^n \to (-\infty, \infty]$ 
  be convex.
  Then 
  for all $y \in \R^n$ and $C>0$ 
    \begin{gather}
      \label{7060_0}
      \inf_{\norm{\Delta}=C} f(y+\Delta) - f(y) \ge 0 \quad
      \Longrightarrow
      \quad
    \exists y^* \in \R^n
    \colon
    y^* \,\text{is global minimum of $f$ and}\,
      \norm{y^* - y} \le C.
    \end{gather}
\end{lemma}
\begin{proof}
  Since 
  $\mathcal{C}:=\left\{ \norm{\Delta}\le C \right\}$
  is convex
  $f$ has a local minimum in 
  $
    y + \mathcal{C}
    :=
    \left\{ 
      y + \Delta \,
      \mid \,
      \norm{\Delta}\le C
    \right\}
    .
  $
  Suppose towards a contradiction that
  $
    y^* \in 
            y + \mathcal{C}
  $
  is a local minimum, but not a global minimum 
  and
  the left-hand side of 
  \eqref{7060_0} is true.
  Then it holds
  \begin{gather}
    \label{7060_3}
    f(x) < f(y^*)
    \quad
    \text{for some}\ 
    x 
    \in 
    \R^n 
    \setminus 
      y + \mathcal{C}
    .
  \end{gather}
  Furthermore since $y + \mathcal{C}$ is compact and contains $y^*$,
  the line segment $\mathcal{L}[y^*,x]$ contains a point on the boundary of 
  $y + \mathcal{C}$, i.e.
  \begin{gather}
    \label{7060_4}
    \theta x + (1 - \theta) y^* = y + \Delta_x
    \quad
    \text{for some}\ 
    \theta \in (0,1)\ 
    \text{and}\ 
    \Delta_x \ 
    \text{with}\ 
    \norm{\Delta_x}=C
    .
  \end{gather}
    It follows
    \begin{align}
      \label{7060_5}
      \begin{split}
      f(y^*)
      \le
      f(y)
      \le
      f(y + \Delta_x)
      &=
      f(
        \theta x + (1 - \theta) y^*
      )
      \\
      &\le
      \theta f(x)
      + 
      (1 - \theta)
      f(y^*)
      <
      f(y^*)
      ,
      \end{split}
    \end{align}
    which is a contradiction.
    Thus every local minimum of $f$ in $y + \mathcal{C}$ is also a global minimum.
    The first inequality is due to
    $y^*$ being a local minimum of $f$ in
    $
      y + \mathcal{C},
    $
    the second inequality is due to the left-hand side of 
    \eqref{7060_0} being true,
    the equality is due to \eqref{7060_4},
    the third inequality is due to the convexity of $f$
    and the strict inequality is due to \eqref{7060_3}.
    %It follows the right-hand side of \eqref{7060_0}.
\end{proof}


