%%%%%%%%%%%%%%%%
% INTRODUCTION %
%%%%%%%%%%%%%%%%
In the following we do not expect the reader to be familiar with convex analysis. However, some very well known results will be stated without proof. The interested reader can study \cite{Mordukhovich2022} for the bedrock analysis.


We begin by defining convex sets
%

\begin{definition}
  \emph{(Convex Set)}
  A subset $\Omega\subseteq \R^n$ is called \textbf{convex} if we have 
  \begin{gather}
    \lambda x + (1-\lambda) y\in \Omega 
    \quad
    \text{for all}\ 
    x,y\in \Omega 
    \
    \text{and}\ 
    \lambda\in (0,1). 
  \end{gather}
\end{definition}

Clearly, the line segment 
$[a,b]:=\left\{ \lambda a+(1-\lambda)b\,\mid \, \lambda\in [0,1] \right\}$ is contained in $\Omega$ for all $a,b\in \Omega$ if and only if $\Omega$ is a convex set.
%

Next we define convex functions. 
%

The concept of convex functions is closely related to convex sets.
%  
 
The line segment between two points on the graph of a convex function lies on or above and does not intersect the graph.
%

In other words: The area above the graph of a convex function $f$ is a convex set, i.e. the \textit{epigraph}
$\text{epi}(f):=\left\{ (x,\alpha)\in \R^n\times\R\,\mid\, f(x)\le \alpha\right\}$ is a convex set in $\R^{n+1}$.
%

Often an equivalent characterisation of convex functions is more useful.
%

\begin{theorem}
  The convexity of a function $f:\R^n\to \overline{\R}$ on $\R^n$ is equivalent to the following statement:

  For all $x,y\in \R^n$ and $\lambda\in(0,1)$ we have 
    \begin{align}
      f(\lambda x + (1-\lambda)y)\le \lambda f(x)+(1-\lambda)f(y).
    \end{align}
\end{theorem}


\begin{definition}
  \emph{proper convex function}
\end{definition}


%%%%%%%%%%%%%%%%%%%%%%
% CONJUGATE CALCULUS %
%%%%%%%%%%%%%%%%%%%%%%
\section{Conjugate Calculus}
When studying different primal problems such as \eqref{primal_weighting_binary} we often turn to the dual instead.
Therefore we need some reliable tools.
Beign able to compute specific convex conjugates is one tool required.


\begin{definition}
  \label{ def_convex_conjugate }
  \emph{(Convex conjugate)}
  Given a function
  $
    f:
    \R^n \to \overline{\R}
  $
  ,
  the 
  \textbf{convex conjugate}
  $
    f^*:
    \R^n \to \overline{\R}
  $
  of $f$ is defined as
  \begin{gather}
    f^*(x^*)
    :=
    \sup_{ x \in \R^n }
    (x^*)^T x - f(x)
  \end{gather}
\end{definition}

Note that $f$ in Definition~\ref{ def_convex_conjugate }
does not have to be convex. On the other hand, the convex conjugate is always convex:

\begin{proposition}
  Let  
  $
    f:
    \R^n \to ( - \infty, \infty ]
  $
  be a proper function. 
  Then its convex conjugate
  $
    f^*:
    \R^n \to ( - \infty, \infty ]
  $
  is convex.
\end{proposition}

% conjugate chain rule %
 %%%%%%%%%%%%%%%%%%%%%%

\begin{theorem}
  \emph{(Conjugate Chain Rule)}
  \label{cvxa_conjugate_chain_rule}
  Let 
  $
    A:
      \R^m \to \R^n
  $
  be a linear map (matrix)
  and
  $
    g:
      \R^n \to (-\infty, \infty]
  $
  a proper convex function. If
  $
    \text{Im}(A) \cap \text{ri}(\text{dom}(g))
    \neq
    \emptyset
  $
  it follows
  \begin{gather}
    ( g \circ A )^* ( x^* )
    =
    \inf_
          { y^* \in ( A^* )^{ -1 } ( x^* )}
                                          g^*( y^* )
                                          .
  \end{gather}
  Furthermore, 
    for any 
      $
        x^* \in \text{dom}( g \circ A)^*
      $
        there exists
          $
            y^* \in ( A^* )^{ -1 } ( x^* )
          $
            such that
              $
                ( g \circ A)^* ( x^* )
                =
                g^*( y^* )
              $.
\end{theorem}

% conjugate sum rule %
 %%%%%%%%%%%%%%%%%%%%

\begin{definition}
  \emph{(Infimal convolution)}
  Given functions
  $
    f_i:
    \R^n \to (-\infty, \infty]
  $
  for $ i = 1, \ldots, n $
  the \textbf{infimal convolution} of these functions as defined as
  \begin{gather}
    (f_1 \square \ldots \square f_m)(x)
    :=
    \inf_{
    \begin{smallmatrix}
      x_i \in \R^n \\
      \sum_{i = 1}^{m} 
        x_i
      =
      x
    \end{smallmatrix}
    }
    \sum_{i = 1}^{m}
      f_i(x_i)
  \end{gather}
\end{definition}


\begin{theorem}
  Let
  $
    f,g:
    \R^n \to (-\infty, \infty]
  $
  be proper convex functions 
  and
  $
  \text{ri}\left( \text{dom}(f) \right)
  \cap
  \text{ri}\left( \text{dom}(g) \right)
  \neq 
  \emptyset
  .
  $
  Then we have the conjugate sum rule
  \begin{gather}
    ( f + g )^*(x^*)
    =
    ( f^* \square g^*)(x^*)
  \end{gather}
  for all $x^* \in \R^n$.
  Moreover, the infimum in 
  $
    ( f^* \square g^*)(x^*)
  $
  is attained, i.e., for any
  $
    x^* \in \text{dom}(f+g)^*
  $
  there exists vectors $x_1^*, x_2^*$
  for which
  \begin{gather}
    (f+g)^*(x^*)
    =
    f^*(x_1^*)
    +
    g^*(x_2^*),
    \quad
    x^* = x_1^* + x_2^*.
  \end{gather}
\end{theorem}


%%%%%%%%%%%%%%%%%%%
% FENCHEL DUALITY %
%%%%%%%%%%%%%%%%%%%
\section{Fenchel Duality}


Given 
proper convex functions $f, g : \R^n \to \overline{\R}$ 
and
a matrix $A \in \R^{n \times n}$,
we define 
the primal minimization problem as follows:

\begin{gather}
  \label{cvxa_primal_problem}
  \text{minimize}
  \quad
  f(x) + g(Ax) 
  \quad
  \text{subject to}
  \quad
  x \in \R^n.
\end{gather}

The Fenchel dual problem is then

\begin{gather}
  \label{cvxa_dual_problem}
  \text{maximize}
  \quad
  -f^* \left( A^T y \right) - g^*(-y) 
  \quad
  \text{subject to}
  \quad
  y \in \R^n.
\end{gather}

\begin{theorem}
  \label{cvxa_fenchel_theorem}
  Let 
  $f, g : \R^n \to \overline{\R}$ 
  be proper convex functions
  and
  $0 \in \text{ri}(\text{dom}(g) - A (\text{dom}(f)) )$
  .
  Then the optimal values of \eqref{cvxa_primal_problem} and \eqref{cvxa_dual_problem} are equal, 
  i.e.
  \begin{gather}
    \inf_{x \in \R^n} 
    \left\{ f(x) + g(Ax) \right\}
    =
    \sup_{y \in \R^n} \left\{   -f^* \left( A^T y \right) - g^*(-y) \right\}
    .
  \end{gather}
\end{theorem}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{lemma}
  \label{syu_1_result}
  Let 
  $f : \R^n \to (-\infty, \infty]$ 
  be convex.
  Then 
  for all $y \in \R^n$ and $C>0$ 
    \begin{gather}
      \label{7060_0}
      \inf_{\norm{\Delta}=C} f(y+\Delta) - f(y) \ge 0 \quad
      \Longrightarrow
      \quad
    \exists y^* \in \R^n
    \colon
    y^* \,\text{is global minimum of $f$ and}\,
      \norm{y^* - y} \le C.
    \end{gather}
\end{lemma}
\begin{proof}
  Since 
  $\mathcal{C}:=\left\{ \norm{\Delta}\le C \right\}$
  is convex
  $f$ has a local minimum in 
  $
    y + \mathcal{C}
    :=
    \left\{ 
      y + \Delta \,
      \mid \,
      \norm{\Delta}\le C
    \right\}
    .
  $
  Suppose towards a contradiction that
  $
    y^* \in 
            y + \mathcal{C}
  $
  is a local minimum, but not a global minimum 
  and
  the left-hand side of 
  \eqref{7060_0} is true.
  Then it holds
  \begin{gather}
    \label{7060_3}
    f(x) < f(y^*)
    \quad
    \text{for some}\ 
    x 
    \in 
    \R^n 
    \setminus 
      y + \mathcal{C}
    .
  \end{gather}
  Furthermore since $y + \mathcal{C}$ is compact and contains $y^*$,
  the line segment $\mathcal{L}[y^*,x]$ contains a point on the boundary of 
  $y + \mathcal{C}$, i.e.
  \begin{gather}
    \label{7060_4}
    \theta x + (1 - \theta) y^* = y + \Delta_x
    \quad
    \text{for some}\ 
    \theta \in (0,1)\ 
    \text{and}\ 
    \Delta_x \ 
    \text{with}\ 
    \norm{\Delta_x}=C
    .
  \end{gather}
    It follows
    \begin{align}
      \label{7060_5}
      \begin{split}
      f(y^*)
      \le
      f(y)
      \le
      f(y + \Delta_x)
      &=
      f(
        \theta x + (1 - \theta) y^*
      )
      \\
      &\le
      \theta f(x)
      + 
      (1 - \theta)
      f(y^*)
      <
      f(y^*)
      ,
      \end{split}
    \end{align}
    which is a contradiction.
    Thus every local minimum of $f$ in $y + \mathcal{C}$ is also a global minimum.
    The first inequality is due to
    $y^*$ being a local minimum of $f$ in
    $
      y + \mathcal{C},
    $
    the second inequality is due to the left-hand side of 
    \eqref{7060_0} being true,
    the equality is due to \eqref{7060_4},
    the third inequality is due to the convexity of $f$
    and the strict inequality is due to \eqref{7060_3}.
    %It follows the right-hand side of \eqref{7060_0}.
\end{proof}


