\begin{theorem}
  \emph{(Slutzky's theorem)}
  Let
  $
  (E,d)
  $
  be a metric space 
  and let
  $X,X_1,X_2,\ldots$
  and
  $Y_1,Y_2,\ldots$
  be random variables with values in $E$.
  Assume
  $X_n\to X$ in distribution and 
  $d(X_n,Y_n)\to 0$ in probability. Then 
  $Y_n\to X$ in distribution.
\end{theorem}
\begin{proof}
  \cite[Theorem~13.8]{Klenke2020}
\end{proof}

\begin{ftheorem}
  Under conditions 
the stochastic process
\begin{gather}
    \sqrt{N}
    \left( 
  \frac{1}{N}
    \sum_{i=1}^{n} 
    w_i^\dagger
    \mathbf{1}
    _{\left\{ Y_i\le z \right\}}
    -
    \P[Y(1)\le z]
    \right)
    _{z\in\R}
    \,.
  \end{gather}
  converges in
  $l^\infty(\R)$
  to a Gaussian process with mean 0 and covariance ??.
\end{ftheorem}
\begin{proof}
  For fixed $z\in\R$ we use the following error decomposition.
  Recall $\pi(x):=\P[T=1|X=x]$ and
  $
  w(x)
  :=
  (
  f^{'}
  )^{-1}
  \left( 
    \inner{B(x)}{\lambda^\dagger}
    +
    \lambda_0^\dagger
  \right)
  $,
  where $(\lambda^\dagger,\lambda_0^\dagger)$ is the optimal dual solution.
  We also write
  $
  F_{Y(1)}(z|x)
  =
  \P[Y(1)\le z|X=x]
  $
  and
  $
  F_{Y(1)}(z)
  =
  \P[Y(1)\le z]
  $.
\begin{align*}
  &
  \sqrt{N}
\left( 
    \frac{1}{N}
    \sum_{i=1}^{n} 
    w(X_i)
    \mathbf{1}_{\left\{ Y_i \le z \right\}}
    \ 
    -
    \ 
    \P[Y(1)\le z]
\right)
    \\
  &
  \ 
  =
  \ 
  \sqrt{N}
  \sum_{k=1}^{N} 
  \left[ 
  \frac{1}{N}
  \left( 
    \sum_{i=1}^{n} 
    w(X_i)
    B_k(X_i)
    -
    \sum_{i=1}^{N} 
    B_k(X_i)
  \right)
  F_{Y(1)}(z|X_k)
  \right]
  %%%% 1 %%%%
  \\
  &
  \qquad
  +
  \ 
  \sqrt{N}
    \sum_{i=1}^{N} 
    \left[ 
  \frac{
    T_i\cdot w(X_i) -1 }{N}
    \left( 
  F_{Y(1)}(z|X_i)
    \ 
    -
    \ 
    \sum_{k=1}^{N} 
    B_k(X_i)
    \cdot
  F_{Y(1)}(z|X_k)
    \right)
    \right]
  %%%% 2 %%%%
  \\
  &
  \qquad
  +
  \ 
  \sqrt{N}
  \left( 
  \frac{1}{N}
    \sum_{i=1}^{N} 
    \left[ 
    T_i
    \left( 
    w(X_i) 
    -
    \frac{1}{\pi(X_i)}
    \right)
    \left( 
    \mathbf{1}_{\left\{ Y_i \le z \right\}}
    -
  F_{Y(1)}(z|X_i)
    \right)
    \right]
  \right)
  %%%% 3 %%%%
  \\
  &
  \qquad
  +
  \ 
  \sqrt{N}
  \left( 
  \frac{1}{N}
    \sum_{i=1}^{N} 
    \frac{T_i}{\pi(X_i)}
    \left( 
    \mathbf{1}_{\left\{ Y_i \le z \right\}}
    -
  F_{Y(1)}(z|X_i)
    \right)
    \ 
    +
    \ 
    \left( 
  F_{Y(1)}(z|X_i)
    -
  F_{Y(1)}(z)
    \right)
  \right)
  \\
  %%%%%%%%%%%%%%%
  &
  \ 
  =:
  \ 
  R_1(z)
  +
  R_2(z)
  +
  R_3(z)
  +
  R_4(z)
  \end{align*}
  We show that $\sup_{z\in\R}\left| R_i(z) \right|\to 0 $ in
  probability for $i=1,2,3$.
  The term $(R_4)_{z\in\R}$ is $\P$-Donsker and determines the covariance of the limiting process.
  \subsection*{Analysis of $R_1$}
  By Theorem~\ref{dual_solution_th}
  it holds $w_i^\dagger=w(X_i)$ for $i\in \left\{ 1,\ldots,n \right\}$, that is, for $i\le n$ we can identify $w(X_i)$ with the optimal
  solution to 
  problem~\ref{bw:1:primal}. 
  Thus the constraints of the problem apply.
  \begin{gather}
      \left| 
      \frac{1}{N} 
      \left( 
      \sum_{i = 1}^{n} 
      w(X_i)
      B_k(X_i)
      -
      \sum_{i=1}^{N} 
      B_k(X_i)
      \right)
    \right|
    \ 
    \le 
    \ 
    \delta_k
    \qquad
    \text{for all}\ 
    k \in \left\{ 1, \ldots, N \right\}
    \,.
  \end{gather}
  Note, that the first sum goes over $\left\{ 1,\ldots,n \right\}$ while the second sum goes over $\left\{ 1,\ldots,N \right\}$.
  A second, equivalent version of the constraints is
  \begin{gather}
      \left| 
      \frac{1}{N} 
      \left( 
      \sum_{i = 1}^{N} 
      T_i
      w(X_i)
      B_k(X_i)
      -
      \sum_{i=1}^{N} 
      B_k(X_i)
      \right)
    \right|
    \ 
    \le 
    \ 
    \delta_k
    \qquad
    \text{for all}\ 
    k \in \left\{ 1, \ldots, N \right\}
    \,.
  \end{gather}
  Now both sums go over $\left\{ 1,\ldots,N \right\}$ and the
  indicator of treatment $T_i$ takes care that in the first sum only the terms with $i\le n$ are effective. 
  Having this flexibility with the versions helps. I regard the first version as suitable for non-probabilistic computations, although $n$ is of course a random variable. On the other hand, the second version is more honest, exactly telling the dependence on the indicator of treatment. This version is useful in probabilistic computations. 

  Let's bound $R_1$.
  \begin{align}
    \label{R_1:1}
    \begin{split}
    \sup_{z\in\R}
    \left| 
    R_1(z)
    \right|
    &
    \ 
    =
    \ 
    \sup_{z\in\R}
    \left| 
  \sqrt{N}
  \sum_{k=1}^{N} 
  \left[ 
  \frac{1}{N}
  \left( 
    \sum_{i=1}^{n} 
    w(X_i)
    B_k(X_i)
    -
    \sum_{i=1}^{N} 
    B_k(X_i)
  \right)
  F_{Y(1)}(z|X_k)
  \right]
    \right|
    \\
    &
    \ 
    \le
    \ 
  \sqrt{N}
  \sum_{k=1}^{N} 
  \left| 
  \frac{1}{N}
  \left( 
    \sum_{i=1}^{n} 
    w(X_i)
    B_k(X_i)
    -
    \sum_{i=1}^{N} 
    B_k(X_i)
  \right)
  \right|
    \sup_{z\in\R}
  F_{Y(1)}(z|X_k)
  \\
    &
    \ 
    \le
    \ 
  \sqrt{N}
  \norm{\delta}_1
    \end{split}
  \end{align}
  Playing around with norm equivalences we discover that 
  $\sqrt{N}\norm{\delta}_1\to 0$ for $N\to \infty$ is the weakest
  (natural) assumption to
  control $R_1$.
  Indeed, other ways to continue the second row in \eqref{R_1:1} are
  \begin{gather*}
    (\,\cdots)
    \ 
  \le
    \ 
  \sqrt{N}
  \norm{\delta}_2
  \left( 
  \sum_{k=1}^{N} 
  \left( 
    \sup_{z\in\R}
  F_{Y(1)}(z|X_k)
  \right)^2
\right)^{1/2}
\ 
\le
\ 
N
  \norm{\delta}_2\,,
  \end{gather*}
  by the Cauchy-Schwarz inequality and
  $
  F_{Y(1)}\in [0,1]
  $,
or
\begin{gather*}
  (\,\cdots)
  \ 
  \le
  \ 
  \sqrt{N}
  \norm{\delta}_\infty
  \sum_{k=1}^{N} 
    \sup_{z\in\R}
  F_{Y(1)}(z|X_k)
  \ 
  \le
  \ 
  N^{3/2}
  \norm{\delta}_\infty
  \,.
\end{gather*}
Since $\delta\in \R^N$, however, it holds
\begin{gather*}
  \sqrt{N}\norm{\delta}_1
  \ 
  \le
  \ 
  N\norm{\delta}_2
  \ 
  \le
  \ 
  N^{3/2}\norm{\delta}_\infty
  \,.
\end{gather*}
With hindsight, the assumption 
  $\sqrt{N}\norm{\delta}_1\to 0$ for $N\to \infty$ 
  also 
  suffices 
  to control the second (or first) occurrence of a term, that we control by assumptions on $\delta$.
This is the \textbf{second term} of \eqref{c:1}, where we estimate
\begin{gather*}
  \inner{\delta}{\left| \Delta \right|}
  \ 
  =
  \ 
  \sum_{k=1}^{N} 
  \delta_k
  \left| \Delta_k \right|
  \ 
  \le
  \ 
  \norm{\delta}_1
  \norm{\Delta}_\infty
  \ 
  \le
  \ 
  \norm{\delta}_1
  \norm{\Delta}_2
  \ 
  \le
  \ 
  \norm{\delta}_1
  \varepsilon
  \ 
  \to
  \ 
  0
  \quad
  \text{for}\ 
  N\to \infty
  \,.
\end{gather*}
\subsection*{Analysis of $R_2$}
In the original paper \cite{Wang2019} the authors derive concrete learning rates for the weights and employ them in bounding this term. They obtain a multiplied learning rate, which is sufficiently fast. Their approach, however, calls for concrete learning rates of the weights. Arguably, the process of deriving such rates is the most complicated part of the paper. 
I found out, that we don't need concrete rates for the weights. 
Consistency of the weights is enough and gives us an (arbitrarily slow but sufficient) learning rate to establish the results.
We don't even need rates for the weights to control $R_2$.
They only play a role in bounding $R_3$.
Nevertheless, we use the 
second constraint of Problem~\eqref{bw:1:primal} 
\begin{gather}
  1
  \ 
  =
  \ 
\frac{1}{N}\sum_{i=1}^{n}w_i^\dagger
  \ 
  =
  \ 
\frac{1}{N}\sum_{i=1}^{n}w(X_i)
  \ 
=
  \ 
\frac{1}{N}\sum_{i=1}^{N}T_iw(X_i)
\,.
\end{gather}
To this end, we note that
\begin{align*}
&
  \sup_{z\in\R}
  \left| 
  F_{Y(1)}(z|X_i)
  -
  \sum_{k=1}^{N} 
  B_k(X_i)
  \cdot
  F_{Y(1)}(z|X_k)
  \right|
  \\
  &
  \ 
  \le
  \ 
  \sum_{k=1}^{N} 
  \frac{\mathbf{1}_{\left\{ X_k\in A_N(X_i) \right\}}}
  {
    \sum_{j=1}^{N} 
\mathbf{1}_{\left\{ X_j\in A_N(X_i) \right\}}
  }
  \sup_{z\in\R}
  \left| 
  F_{Y(1)}(z|X_i)
  -
  F_{Y(1)}(z|X_k)
  \right|
  \\
  &
  \ 
  \le
  \ 
  \sup_{z\in\R}
  \omega
  \left( 
    F_{Y(1)}(z|\cdot)
    ,h_N
  \right)
  \,,
\end{align*}
where $\omega$ is the modulus of continuity and $h_N$ is the width of
the partition $\mathcal{P}_N=\left\{A_{1,N},A_{2,N},\ldots  \right\}$.
There are many (more concrete, yet stronger) assumptions on the regularity of
$F_{Y(1)}$ and the width of the partition $h_N$ that give us
\begin{gather}
  \sqrt{N}
  \sup_{z\in\R}
  \omega
  \left( 
    F_{Y(1)}(z|\cdot)
    ,h_N
  \right)
  \to
  0
  \qquad
  \text{for}\ 
  N\to \infty
  \,.
\end{gather}
But we shall keep this more general (and abstract) assumption.
We conclude
\begin{align*}
&
  \sup_{z\in\R}
  \left| 
  R_2(z)
  \right|
  \\
  &
  \ 
  \le
  \ 
  \sqrt{N}
    \sum_{i=1}^{N} 
    \left[ 
  \frac{
    T_i\cdot w(X_i) -1 }{N}
  \sup_{z\in\R}
    \left| 
  F_{Y(1)}(z|X_i)
    \ 
    -
    \ 
    \sum_{k=1}^{N} 
    B_k(X_i)
    \cdot
  F_{Y(1)}(z|X_k)
    \right|
    \right]
    \\
    &
  \ 
    \le
  \ 
    \sqrt{N}
  \sup_{z\in\R}
  \omega
  \left( 
    F_{Y(1)}(z|\cdot)
    ,h_N
  \right)
  \sum_{i=1}^{N} 
  \frac{
    T_i\cdot w(X_i) +1 }{N}
    \\
    &
  \ 
    =
  \ 
    2
    \sqrt{N}
  \sup_{z\in\R}
  \omega
  \left( 
    F_{Y(1)}(z|\cdot)
    ,h_N
  \right)
  \ 
  \to 
  \ 
  0\,.
\end{align*}
\subsection*{Analysis of $R_3$}
We will apply theory of empirical processes to bound
\begin{gather}
  R_3(z)
  =
  \sqrt{N}
  \left( 
  \frac{1}{N}
    \sum_{i=1}^{N} 
    \left[ 
    T_i
    \left( 
    w(X_i) 
    -
    \frac{1}{\pi(X_i)}
    \right)
    \left( 
    \mathbf{1}_{\left\{ Y_i \le z \right\}}
    -
  F_{Y(1)}(z|X_i)
    \right)
    \right]
  \right)
\end{gather}
in probability. Why don't we use simple concentration inequalities such as Bernstein's  or Markov's inequality? The reason is, that
the weights
$
  w(x)
  :=
  (
  f^{'}
  )^{-1}
  \left( 
    \inner{B(x)}{\lambda^\dagger}
    +
    \lambda_0^\dagger
  \right)
  $
  depend (thorough $B$ and $(\lambda^\dagger,\lambda_0^\dagger)$)
  on the whole data set $D:=(T_i,X_i)_{i=1,\ldots,N}$. Thus, it is more honest to write
  $w(x,D)$ instead. This captures the whole dependence on probabilities. Note, that $(Y_i)_{i=1,\ldots N}$ are independent of $w$ given $D$. 
  A standard computation shows
  \begin{gather}
    \E
    \left[ 
    \frac{T}{\pi(X)}
    \left( 
    \mathbf{1}_{\left\{ Y(T) \le z \right\}}
    -
  F_{Y(1)}(z|X)
    \right)
    \right]
    =0
    \,.
  \end{gather}
  Furthermore
  \begin{align*}
    &
    \E
    \left[ 
      Tw(X,D)
    \left( 
    \mathbf{1}_{\left\{ Y(T) \le z \right\}}
    -
  F_{Y(1)}(z|X)
    \right)
    \right]
    \\
    &
    \ 
    =
    \ 
    \E
    \left[ 
      \E
      \left[ 
      w(X,D)
      \left( 
    \mathbf{1}_{\left\{ Y(1) \le z \right\}}
    -
  F_{Y(1)}(z|X)
      \right)
  |T=1,X,D
      \right]
    \right]
    \\
    &
    \ 
    =
    \ 
    \E
    \left[ 
      w(X,D)
      \E
      \left[ 
    \mathbf{1}_{\left\{ Y(1) \le z \right\}}
    -
  F_{Y(1)}(z|X)
  |X,D
      \right]
    \right]
    \\
    &
    \ 
    =
    \ 
    \E
    \left[ 
      w(X,D)
      \E
      \left[ 
    \mathbf{1}_{\left\{ Y(1) \le z \right\}}
    -
  F_{Y(1)}(z|X)
  |X
      \right]
    \right]
    \\
    &
    \ 
    =
    \ 
    0
  \end{align*}
  The second equality is due to the assumption of $(Y(0),Y(1))\perp T |X$.
  The third equality is due to $X\perp D$.
  Next we define
  the (random) function
  $f_D^z$ by
  \begin{gather}
    f_{D}^z(T,X,Y(T))
    :=
    T
    \left( 
    w(D,X)- \frac{1}{\pi(X)}
    \right)
    \left( 
    \mathbf{1}_{\left\{ Y(T) \le z \right\}}
    -
  F_{Y(1)}(z|X)
    \right)
    \,.
  \end{gather}
  We just showed
  $
  \E
  [
    f_{D}^z(T,X,Y(T))
  ]
  =
  0
  $
  for all $z\in\R$.
  Thus
  \begin{gather}
    R_3(z)
    =
    G_N
    f_D^z
    \,.
  \end{gather}
  By the consistency of the weights there exists a learning rate $(\varepsilon_N)$ such that
  \begin{gather}
    \P
    \left[ 
      \left| 
      w(X,D)
      -
      \frac{1}{\pi(X)}
      \right|
      \le
      \varepsilon_N
    \right]
    \to 1 
    \qquad
    \text{for}\ 
    N\to \infty
    \,.
  \end{gather}
  Let
  $
  \mathcal{F}_N
  :=
  \varepsilon_N B_{\mathcal{F}}$.
  It holds
  \begin{gather}
    \P
    \left[ 
    f_D^z
    \in
  \mathcal{F}_N
  \ \forall\,z\in\R
    \right]
    =
    \P
    \left[ 
      \sup_{z\in\R}
      \left| 
    f_D^z
      \right|
      \le
      \varepsilon_N
    \right]
    \to 1
  \end{gather}
  Then the lemma applies?.
\end{proof}

