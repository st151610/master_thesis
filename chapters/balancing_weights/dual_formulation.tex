\begin{ftheorem*}
  The dual of 
  Problem~\ref{bw:1:primal}
  is the unconstrained optimization problem 
  \begin{gather*}
    \underset{\lambda \in \R^n}{\mathrm{minimize}}
    \qquad
    \frac{1}{n}
    \sum_{i = 1}^{n} 
    T_i 
    \cdot
    f^*
    (
      m_n(\lambda|X_i)
    )
    -
      m_n(\lambda|X_i)
    \ 
    +
    \ 
    \inner{\delta}{\left| \lambda \right|}
    \,,
  \end{gather*}
  where
  \begin{gather*}
  f^*
  \,
  \colon
  \, 
  \R
  \ 
  \to
  \ 
  \R
  \,
  ,
  \qquad 
  x^*
  \ 
  \mapsto
  \ 
    x^*
    \!
    \cdot
    (f^{'})^{-1}(x^*)
  \ 
    -
  \ 
    f
    \left( 
      (f^{'})^{-1}(x^*)
    \right)
  \end{gather*}
  is the Legendre transformation of $f$,
  the vector
  $
    B(X_i)
    =
    \left[ 
      B_1(X_i)
      ,
      \ldots
      ,
      B_n(X_i)
    \right]
    ^\top
  $
  denotes 
  the $n$ basis functions of the covariates 
  of unit $i\in \left\{ 1, \ldots, n \right\}$
  and
  $
    \left| \lambda \right|
    =
    \left[ 
      \left| \lambda_1 \right|
      ,
      \ldots
      ,
      \left| \lambda_n \right|
    \right]
    ^\top
    ,
  $
  where $\left| \,\cdot\, \right|$
  is the absolute value of a real-valued scalar.
  Moreover,
  if $\lambda^\dagger$
  is an optimal solution of the above problem 
  then
  for all $i$ with $T_i=1$
  \begin{gather*}
    w_i^\dagger
    \ 
    =
    \ 
    (f^{'})^{-1}
    \left( 
      m_n
      (
      \lambda^\dagger
      |
      X_i
      )
    \right)
  \end{gather*}
  is part of the optimal solution to 
  Problem~\ref{bw:1:primal}
  .
\end{ftheorem*}
\subsection*{Plan of proof}
  We bring 
  Problem~\ref{bw:1:primal}
  in the form of ts.
  Then we apply the results of the ts chapter.
  We wait to conclude on the weights.
  Then we eliminate the non-negativity constraints
  on the dual variables leveraging convexity and optimality.
%%%%%%%%%%%%%%%  
%%%% PROOF %%%%
%%%%%%%%%%%%%%%
\begin{proof}
  We consider the vector of basis functions of the covariates 
  of unit $i\in \left\{ 1, \ldots, n \right\}$, that is,
\begin{align*}
    B(X_i)
    &
    \ 
    :=
    \ 
    \left[ 
      B_1(X_i)
      ,
      \ldots
      ,
      B_n(X_i)
    \right]
    ^\top
    \,,
    \intertext{the constraints vector}
d
    &
    \ 
    :=
    \ 
    \begin{bmatrix}
      0_n
      \\
      \pm n
      \\
      -n\cdot\delta 
      \pm\,
      \sum_{i = 1}^{n} B_k(X_i)
    \end{bmatrix}
    \,,
    \intertext{
  the matrix of the basis functions with indicators of treatment
  }
    T\mathbf{B}(\mathbf{X})
    &
    \ 
    :=
    \ 
    \begin{bmatrix}
      T_1B(X_1), \ldots, T_nB(X_n)
    \end{bmatrix}
    \intertext{
      and
  the constraint matrix with indicator of treatment
    }
    T\mathbf{A}
    &
    \ 
    :=
    \ 
    \begin{bmatrix}
      \mathrm{diag}
      [T_1,\ldots,T_n]
      \\
      \pm
      [T_1,\ldots,T_n]
      \\
      \pm\,T\mathbf{B}(\mathbf{X})
    \end{bmatrix}
        \,.
  \end{align*}
  Next we write
  Problem~\ref{bw:1:primal}
  in the form of ts.
 \begin{gather*}
    \underset{w_1, \ldots, w_n \in \R}{\mathrm{minimize}}
    \qquad
    \sum_{i = 1}^{n} T_i \cdot f(w_i)
    \\
    \mathbf{A}w 
    \ 
    \ge
    \ 
    d
    \,,
\end{gather*}
The convex conjugate of the objective function 
of 
  Problem~\ref{bw:1:primal}
is
\begin{gather*}
  [x^*_1,\ldots,x^*_n]^\top
  \ 
  \mapsto
  \ 
  \sum_{T_i=1} f^*(x^*_i)
  +
  \sum_{T_i=0} 
  \chi_{\left\{ 0 \right\}}(x^*_i)
  \,,
\end{gather*}
where we define the characteristic function $\chi$ by
\begin{gather*}
  \chi_{\left\{ 0 \right\}}
  (t)
  \ 
  =
  \ 
  \infty
  \cdot
  (
  1-\mathbf{1}_{\left\{ 0 \right\}}(t)
  )
  \ 
  =
  \ 
  \begin{cases}
    0,& \text{if}\ t=0,\\
    \infty,& \text{else}\,.
  \end{cases}
\end{gather*}
Note that the $i$-th column of $T\mathbf{A}$,
which we denote as 
$T\mathbf{A}_i$,
vanishes if 
$T_i=0$. 
In the subsecquent analysis
this prevents the characteristic function from blowing up.
We consider the form of the dual in ts to get
\begin{gather*}
  \sum_{T_i=1}  f^*(T\mathbf{A}_{ i}^\top\lambda)
  +
  \sum_{T_i=0} 
  \chi_{\left\{ 0 \right\}}
(T\mathbf{A}_{i}^\top\lambda)
  =
  \sum_{i=1}^n T_i f^*(\mathbf{A}_{ i}^\top\lambda)
  \,.
\end{gather*}
where
$\mathbf{A}$
 is as $T\mathbf{A}$ without the indicators of treatment.
 It is important, that in the final form the indicators of treatment are outside the argument of $f^*$ and we have no singularity for 
 $T_i=0$.
The corresponding dual problem in \cite{Tseng1991} is then
\begin{gather*}
  \underset
  {\lambda_1,\ldots,\lambda_{K}\ge 0}
  {
  \mathrm{maximize}
  }
  \quad
  -
  \sum_{i=1} 
  ^n
  T_i
  \cdot
  f^*
(\mathbf{A}_{ i}^\top\lambda)
  \ 
  +
  \ 
  \inner{\lambda}{d}
  \,.
\end{gather*}
  Next we want to remove the non-negativity constraints on $\lambda$.
  To this end we write
  \begin{gather}
    \lambda
    :=
    \begin{bmatrix}
      \rho_1,
      \ldots,
      \rho_n,
      \ 
      \lambda_0^+,
      \lambda_0^-,
      \ 
      \lambda_1^+,
      \ldots,
      \lambda_n^+,
      \ 
      \lambda_1^-,
      \ldots,
      \lambda_n^-
    \end{bmatrix}
    ^\top
    \,.
  \end{gather}
  We expand the objective function $G$ of the dual problem.
  \begin{align*}
    G
    (
    \rho,
    \lambda_0^\pm,
    \lambda^\pm
    )
    =
  &-
  \sum_{i=1} 
  ^n
  T_i
  \cdot
  f^*
  \left( 
\rho_i
\ 
+
\ 
\lambda_0^+
\!
-
\lambda_0^-
\ 
+
\ 
\inner
{B(X_i)}
{\,\lambda^+ \!\!- \lambda^-}
  \right)
  \\
  &+
  \ 
  n
  \cdot
  (
\lambda_0^+
\!
-
\lambda_0^-
  )
  \ 
+
  \ 
  \sum_{i=1} 
  ^n
\inner
{B(X_i)}
{\,\lambda^+ \!\!- \lambda^-}
  \ 
-
  \ 
  n
  \cdot
\inner
{\delta}
{\,\lambda^+ \!+ \lambda^-}
  \end{align*}
  To illustrate the procedure, we show 
  for all $i \in \left\{ 1,\ldots,n \right\}$


\begin{alignat*}{2}
  \text{either}
  &
  &&
  \qquad
      \lambda_i^+ > 0
  \\
  \text{or}
  &
  &&
  \qquad
      \lambda_i^- > 0
  \,.
\end{alignat*}
Assume towards a contradiction that 
there exists
$i \in \left\{ 1,\ldots,n \right\}$
such that
$
      \lambda_0^+ > 0
$
and
$
      \lambda_0^- > 0
$ 
and that 
$\lambda$ is optimal.
Consider
  \begin{gather}
    \tilde{\lambda}
    \ 
    :=
    \ 
    \begin{bmatrix}
      \rho
      ,
      \ 
      \lambda_0^\pm,
      \ 
      \lambda_1^+,
      \ldots,
      \ 
      \lambda_i^+
      \!
      -
      (
      \lambda_i^+
      \!
      \land
      \lambda_i^-
      ),
      \ldots
      \lambda_n^+,
      \ 
      \lambda_1^-,
      \ldots,
      \lambda_i^-
      \!
      -
      (
      \lambda_i^+
      \!
      \land
      \lambda_i^-
      ),
      \ldots,
      \lambda_n^-
    \end{bmatrix}
    ^\top
    \,.
  \end{gather}
  Since 
  $
      \lambda_i^\pm
      -
      (
      \lambda_i^+
      \!
      \land
      \lambda_i^-
      )
      \ge 
      0
  $,
  the perturbed vector $\tilde{\lambda}$ is in the domain of the 
  optimization problem.
  But 
  \begin{align}
  G(\tilde{\lambda})
  -
  G(\lambda)
  \ 
  =
  \ 
  2
  n
  \cdot
  \delta_i
  \cdot
      (
      \lambda_i^+
      \!
      \land
      \lambda_i^-
      )
  \ 
  >
  \ 
  0
  \,,
  \end{align}
  which contradicts the optimality of $\lambda$.
  Likewise we can show
\begin{alignat*}{2}
  \text{either}
  &
  &&
  \qquad
      \lambda_i^+ > 0
  \\
  \text{or}
  &
  &&
  \qquad
      \lambda_i^- > 0
  \,.
\end{alignat*}
But then 
$
\lambda^\pm_i
\ge 0
$
collapses to
$
\lambda_i\in \R
$ 
for 
$i\in \left\{ 0,\ldots,n \right\}$, that is,
$ \lambda_i=\lambda_i^+\!-\lambda_i^- $.
Note that
$ |\lambda_i|=\lambda_i^+\!+\lambda_i^- $.
Likewise we can see, that 
$
\lambda_0
=
\lambda_0^+
-
\lambda_0^-
\in \R
$
removes the constraint on $\lambda_0^\pm$.
Let us take this into account for $G$. We get
  \begin{align*}
    G
    (
    \rho,
    \lambda_0,
    \lambda
    )
    =
  &-
  \sum_{i=1} 
  ^n
  T_i
  \cdot
  f^*
  \left( 
\rho_i
\ 
+
\ 
\lambda_0
\ 
+
\ 
\inner
{B(X_i)}
{\lambda}
  \right)
  \\
  &+
  \ 
  n
  \cdot
\lambda_0
  \ 
+
  \ 
  \sum_{i=1} 
  ^n
\inner
{B(X_i)}
{\lambda}
  \ 
-
  \ 
  n
  \cdot
\inner
{\delta}
{|\lambda|}
\,.
  \end{align*}
Next we show, that $\rho=0$.
Suppose there exists 
$
i\in \left\{ 1,\ldots, n \right\}
$
such that 
$
\rho_i>0
$
and
$
  T_i
  \cdot
  (f^{'})^{-1}
  \left( 
\rho_i
\ 
+
\ 
\lambda_0
\ 
+
\ 
\inner
{B(X_i)}
{\lambda}
  \right)
  <
  0
$.
It follows
\begin{gather}
  G(0,\lambda_0,\lambda)
  -
  G(\rho_i,\lambda_0,\lambda)
  \ge
  T_i
  \cdot
  (f^{'})^{-1}
  \left( 
\rho_i
\ 
+
\ 
\lambda_0
\ 
+
\ 
\inner
{B(X_i)}
{\lambda}
  \right)
(-\rho_i)
>0,
\end{gather}
which contradicts the optimality of $\lambda$.
Suppose
$
  T_i
  \cdot
  (f^{'})^{-1}
  \left( 
\rho_i
\ 
+
\ 
\lambda_0
\ 
+
\ 
\inner
{B(X_i)}
{\lambda}
  \right)
  >
  0
$.
Then the claim yields to a perturbation argument as in ts.
Thus
To eliminate the constraints for $\rho$ 
we use a similar argument as in the complementary slackness
section of the ts chapter.
Thus we have complementary slackness of 
$\rho_i$ and
$
  T_i
  \cdot
  (f^{'})^{-1}
  \left( 
\rho_i
\ 
+
\ 
\lambda_0
\ 
+
\ 
\inner
{B(X_i)}
{\lambda}
  \right)
$.
But then
every
optimal solution $\lambda$ remains optimal by taking $\rho=0$.

Dividing the optimization problem by $n$ and reversing it, we get

\begin{gather*}
  \underset
  {\lambda_0,\ldots,\lambda_{n}\in \R}
  {
    \mathrm{minimize}
  }
  \quad
  \frac{1}{n}
\sum_{i=1} 
  ^n
  \left[ 
    \,
  T_i
  \cdot
  f^*
  \!
  \left( 
\lambda_0
+
\inner
{B(X_i)}
{\lambda}
  \right)
  \ 
-
\ 
  \left( 
\lambda_0
+
\inner
{B(X_i)}
{\lambda}
  \right)
  \,
  \right]
  \ 
+
\ 
\inner
{\delta}
{|\lambda|}
  \,.
\end{gather*}

\end{proof}
