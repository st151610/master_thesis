We settle for partitioning estimates, 
although other (bounded) universal consistent basis functions
would work as well.

We leverage (weak) universal consistency
of partitioning estimates~\cite{Gyorfi2002}.
To this end, we choose the basis functions as
\begin{gather*}
  B_k(x)
  :=
  \frac{
  \mathbf{1}_{X_k \in A_n(x)}
  }{
  \sum_{j=1}^{n} 
  \mathbf{1}_{X_j \in A_n(x)}
  }
  \,,
  \qquad
  k=
  1,\ldots,n
  \,.
\end{gather*}
The euclidian norm of 
$
  B(x)
$
is bounded by one.
\begin{gather*}
  \norm{B(x)}^2
  =
  \sum_{k=1}^{n} 
  \left( 
  \frac{
  \mathbf{1}_{X_k \in A_n(x)}
  }{
  \sum_{j=1}^{n} 
  \mathbf{1}_{X_j \in A_n(x)}
  }
  \right)
  ^2
  \le
  \sum_{k=1}^{n} 
  \frac{
  \mathbf{1}_{X_k \in A_n(x)}
  }{
  \sum_{j=1}^{n} 
  \mathbf{1}_{X_j \in A_n(x)}
  }
  =1
  \,.
\end{gather*}
The oracle parameters are
$
  \left[ 
    f^{'}
    \left( 
      \frac{1}{\pi_i}
    \right)
  \right] _ { i = 1,\ldots,n }
$
and
$
  \left[ 
    Y_i(1)
  \right] _ { i = 1,\ldots,n }
$
.
It even holds
\begin{gather*}
  \lambda ^ \dagger _ i \to 
    f^{'}
    \left( 
      \frac{1}{\pi_i}
    \right)
\end{gather*}
in probability.

 \begin{align*}
   G(\lambda)
   &
   \ 
   :=
   \ 
      \frac{1}{n}
      \sum_{j = 1}^{n} 
        T_i
        \cdot
        f^* 
        \left( 
          \inner
          {B(X_i)}
          {\lambda}
        \right)
      -
          \inner
          {B(X_i)}
          {\lambda}
      +
      \inner
      { | \lambda | }
      { \delta }
      \\
   &
   \ 
   =:
   \ 
      L( \lambda )
      +
      \inner
      { | \lambda | }
      { \delta }
 \end{align*} 
 The consistency of 
 $
  \lambda ^ \dagger
 $
 relates to the difference being nonnegative.
 For all 
 $
  \varepsilon > 0
 $
 it holds
 \begin{gather*}
   \P
   \left[ 
     \norm
     {
      \lambda ^ \dagger
      -
      f ^ { ' }
      \left( 
        1 / \pi 
      \right)
     }
     \le
     \varepsilon
   \right]
   =
   \P
   \left[ 
     \inf _ { \norm{\Delta} = \varepsilon }
     G
     (
      f ^ { ' }
      \left( 
        1 / \pi 
      \right)
      +
      \Delta
     )
     -
     G
     (
      f ^ { ' }
      \left( 
        1 / \pi 
      \right)
     )
     \ge 
     0
   \right]
   \,.
 \end{gather*}
 The term on the right-hand side.
 \begin{align*}
   &
     G
     (
      f ^ { ' }
      \left( 
        1 / \pi 
      \right)
      +
      \Delta
     )
     \ 
     -
     \ 
     G
     (
      f ^ { ' }
      \left( 
        1 / \pi 
      \right)
     )
     %%%%%%%%%%%%% 1 %%%%%%%%%%%%%%
     \\
     &
     \quad
     \ge
     \ 
     \inner
     {
     \nabla 
     L
     (
      f ^ { ' }
      \left( 
        1 / \pi 
      \right)
     )
     }
     {
     \Delta
     }
     \ 
     +
     \ 
     \inner
     {
      | 
      f ^ { ' }
      \left( 
        1 / \pi 
      \right)
      +
      \Delta
     |
     \ 
      -
     \ 
     | 
      f ^ { ' }
      \left( 
        1 / \pi 
      \right)
     |
     }
     { \delta }
     %%%% 2 %%%%
     \\
     &
     \quad
     \ge
     \ 
     -
     \norm{\Delta}
     \left( 
     \norm{\delta}
     \ 
     +
     \ 
     \norm{B(X_i)}
     \cdot
     \frac{1}{n}
     \sum_{i=1}^{n} 
     \left| 
     \,
      1
     \ 
      -
     \ 
        T_i
        \cdot
     (f ^ { ' }) ^ { -1 }
          \inner
          {B(X_i)}
          {
      f ^ { ' }
      \left( 
        1 / \pi 
      \right)
          }
          \,
     \right|
     \right)
     %%%% 3 %%%%
     \\
     &
     \quad
     \ge
     \ 
     -
     \varepsilon
     \left( 
     \norm{\delta}
     \ 
     +
     \ 
     \max _ { i = 1,\ldots,n }
     \left| 
     \,
      1
      /
      \pi_i
     \ 
      -
     \ 
     (f ^ { ' }) ^ { -1 }
          \inner
          {B(X_i)}
          {
      f ^ { ' }
      \left( 
        1 / \pi 
      \right)
          }
          \,
     \right|
     +
     \frac{1}{n}
     \sum_{i=1}^{n} 
     \left| 
     \,
     1
     \ 
      -
     \ 
      T_i
      /
      \pi_i
     \right|
     \right)
     %%%% 4 %%%%
     \\
     &
     \quad
     \ge
     \ 
     -
     \varepsilon
     \left( 
     \varepsilon _ {\delta}
     \ 
     +
     \ 
     \omega (
     (f ^ { ' }) ^ { -1 }
     ,
     \varepsilon _ m
     )
     \ 
     +
     \ 
     \varepsilon _ {\mathrm{WLLN}}
     \right)
     %%%% 5 %%%%
     \\
     &
     \quad
     \ge
     \ 
     - \varepsilon _ G
     \,,
 \end{align*}
 with probability tending to 1.
 %%%% convergence G %%%%
 It follows
 for all $\varepsilon, \varepsilon_G > 0 $
 \begin{gather*}
   \P
   \left[ 
     \inf _ { \norm{\Delta} = \varepsilon }
     G
     (
      f ^ { ' }
      \left( 
        1 / \pi 
      \right)
      +
      \Delta
     )
     -
     G
     (
      f ^ { ' }
      \left( 
        1 / \pi 
      \right)
     )
     \ge 
     - \varepsilon_G
   \right]
   \ 
   \to 
   \ 
   1
   \,
 \end{gather*}
 for $ n \to \infty $.
 %%%% convergence lambda %%%%
 Thus, for all $ \varepsilon > 0 $
 it holds
 \begin{gather*}
   \P
   \left[ 
     \norm
     {
      \lambda ^ \dagger
      -
      f ^ { ' }
      \left( 
        1 / \pi 
      \right)
     }
     \le
     \varepsilon
   \right]
   =
   \P
   \left[ 
     \inf _ { \norm{\Delta} = \varepsilon }
     G
     (
      f ^ { ' }
      \left( 
        1 / \pi 
      \right)
      +
      \Delta
     )
     -
     G
     (
      f ^ { ' }
      \left( 
        1 / \pi 
      \right)
     )
     \ge 
     0
   \right]
   \ 
   \to 
   \ 
   1
   \,,
 \end{gather*}
 for $ n \to \infty $.
 Furthermore,
 \begin{align*}
   T_i
   \cdot
   \left| 
   w_i
   -
   1 / \pi_i
   \right|
   & 
   \ 
   =
   \ 
   T_i
   \cdot
   \left| 
   T_i
   (
      f ^ { ' }
   )
   ^ {-1}
   \inner
   {T_i B(X_i)}
   { \lambda ^ \dagger }
   -
     (f ^ { ' }) ^ { -1 }
     \left( 
      f ^ { ' }
      \left( 
        1 / \pi _ i
      \right)
     \right)
   \right|
   %%%% 1 %%%%
   \\
   & 
   \ 
   \le
   \ 
   \left| 
   (
      f ^ { ' }
   )
   ^ {-1}
   \inner
   {B(X_i)}
   { \lambda ^ \dagger }
   -
   (
      f ^ { ' }
   )
   ^ {-1}
          \inner
          {B(X_i)}
          {
      f ^ { ' }
      \left( 
        1 / \pi 
      \right)
          }
   \right|
   %%%% 1 a %%%%
   \\
   &
   \qquad
   +
   \ 
   \left| 
   (
      f ^ { ' }
   )
   ^ {-1}
          \inner
          {B(X_i)}
          {
      f ^ { ' }
      \left( 
        1 / \pi 
      \right)
          }
   -
     (f ^ { ' }) ^ { -1 }
     \left( 
      f ^ { ' }
      \left( 
        1 / \pi _ i
      \right)
     \right)
   \right|
   %%%% 2 %%%%
   \\
   &
   \ 
   \le
   \ 
   \omega
   \left( 
     (f ^ { ' }) ^ { -1 }
     ,
     \norm{ \lambda ^ \dagger - f ^ {'} (1/\pi) }
   \right)
   \ 
   +
   \ 
   \omega
   \left( 
     (f ^ { ' }) ^ { -1 }
     ,
     \varepsilon _ m
   \right)
   %%%% 3 %%%%
   \\
   &
   \ 
   \le
   \ 
   \omega
   \left( 
     (f ^ { ' }) ^ { -1 }
     ,
     \varepsilon _ \dagger
   \right)
   \ 
   +
   \ 
   \omega
   \left( 
     (f ^ { ' }) ^ { -1 }
     ,
     \varepsilon _ m
   \right)
   %%%% 4 %%%%
   \\
   &
   \ 
   \le
   \ 
   \varepsilon
 \end{align*}
 with probability tending to 1.

 Next we consider the estimate.
\begin{align*}
  \left| 
    \frac{1}{n}
    \sum_{i=1}^{n} 
    w_i
    T_i
    Y_i
    -
    \E[Y(1)]
  \right|
  &
  \ 
  \le
  \ 
  \left|  
  \frac{1}{n}
    \sum_{i=1}^{n} 
    (
    w_i T_i
    -
    1
    )
    \inner
    {B(X_i))}
    { \mathbf{Y}(1) }
  \right|
  %%%% 1 %%%%
  \\
  &
  \qquad
  +
  \ 
  \left|  
  \frac{1}{n}
    \sum_{i=1}^{n} 
    (
    w_i T_i
    -
    1
    )
    \left( 
    \E[Y(1)| X_i]
    -
    \inner
    {B(X_i))}
    { \mathbf{Y}(1) }
    \right)
  \right|
  %%%% 2 %%%%
  \\
  &
  \qquad
  +
  \ 
  \left|  
  \frac{1}{n}
    \sum_{i=1}^{n} 
    T_i
    \cdot
    (
    w_i 
    -
    1/\pi_i
    )
    \left( 
      Y_i
    -
    \E[Y(1)| X_i]
    \right)
  \right|
  %%%% 3 %%%%
  \\
  &
  \qquad
  +
  \ 
  \left|  
  \frac{1}{n}
    \sum_{i=1}^{n} 
    T_i
    /\pi_i
    \left( 
      Y_i
    -
    \E[Y(1)| X_i]
    \right)
    +
    \left( 
    \E[Y(1)| X_i]
    -
    \E[Y(1)]
    \right)
  \right|
  %%%% 4 %%%%
  \\
  &
  \ 
  \le
  \ 
  C_Y
  \varepsilon _ \delta
  +
  C_w
  \varepsilon _ m
  +
  C_{\E Y}
  \varepsilon _ w
  +
  \varepsilon _ {\mathrm{WLLN}}
  %%%% 4 %%%%
  \\
  &
  \ 
  \le
  \ 
  \varepsilon
\end{align*}
with probability tending to 1.
\begin{ftheorem}
  Let 
  $Y$
  be bounded
  and 
  $
    \E
    \left[ 
      f ^ { ' }
\left( 
  1 / \pi(X)
\right)
^2
    \right]
    <
    \infty
  $.
  Then 
  \begin{gather*}
    \frac{1}{n}
    \sum_{i=1}^{n} 
    w_i
    T_i
    Y_i
  \end{gather*}
  is a consistent estimator for $\E[Y(1)]$,
  that is, for all 
  $
    \varepsilon > 0
  $
  it holds
  \begin{gather*}
    \P
    \left[ 
      \left| 
    \frac{1}{n}
    \sum_{i=1}^{n} 
    w_i
    T_i
    Y_i
    -
    \E[Y(1)]
      \right|
    \ge 
    \varepsilon
    \right]
    \ 
    \to
    \ 
    0
  \end{gather*}
  for 
  $n\to \infty$.
\end{ftheorem}
\begin{remark}
  The entropy 
  \begin{gather*}
    f \colon (0, \infty)
    \to 
    \R
    \,,
    \quad
    x \mapsto x\log x
  \end{gather*}
  is a prevailing choice.
  Then the requirement on $\pi$ is
  \begin{gather*}
    \E
    [
    (
    \log \pi(X)
    )
    ^2
    ]
    <
    \infty
    \,.
  \end{gather*}
  This is a very weak requirement.
  Likewise, the requirement for the variance
  \begin{gather*}
    f \colon 
    \R
    \to 
    \R
    \,,
    \quad
    x \mapsto (x-1/n)^2
  \end{gather*}
  is
  \begin{gather*}
    \E
    [
    1/
    \pi(X)
    ^2
    ]
    <
    \infty
    \,.
  \end{gather*}
\end{remark}

\begin{remark}
  Both partitioning estimates are covered by \cite[ยง4]{Gyorfi2002},
  because we only estimate quantities depending on $X$,
  that is 
  $
  f^{-1}(1/\pi(X))
  $
  and 
  $
  \E[Y(1)|X]
  $.
  The corresponding coefficients in the partitioning estimate are then
  $
  f^{-1}(1/\pi_i)
  $
  and
  $
  Y_i(1)
  $,
  which is the potential outcome of unit $i$ under treatment.
  Note, that both quantities are generally unknown to us, but we can 
  nevertheless leverage there existence in the proof.
\end{remark}
\begin{takeaways}
  To ensure double robustness, we leverage primal and dual optimization
  problem. The dual problem solves propensity score estimation and the 
  primal problem the bias of the final estimate.
  Partitioning estimates as in \cite{Gyorfi2002}. 
\end{takeaways}

