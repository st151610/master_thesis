read the paper \cite{Zhao2017a} for discussion of double robustness for balancing weights and \cite{Hahn1998} for semiparametric efficiency bounds.


What is the best rate we can achieve?

\begin{theorem}
  \emph{(Hartman-Winter)}
  Let 
  $
    X_1,
    X_2,
    \ldots
  $
  be i.i.d. real random variables with 
  $
    \E[X_1]=0
    \ 
    \text{and}
    \ 
    \mathbf{Var}
    [X_1]
    = 1.
  $
  Let
  $
    S_n
    :=
    X_1
    +
    \ldots
    +
    X_n,
    \ 
    n\in \mathbb{N}
    .
  $
  Then
  \begin{gather}
    \limsup_{n\to\infty}
    \frac{S_n}{
      \sqrt{
        2n
        \log
        \log
        n
      }
    }
    =
    1
    \quad
    \text{a.s.}
  \end{gather}
\end{theorem}
\begin{proof}
  \cite[Theorem~22.11]{Klenke2020}
\end{proof}


We calculate mean and variance

\begin{align}
  \E
  \left[ 
    (T/\pi(X))
    Y(T)
  \right]
  &=
  \E
  \left[ 
    Y(1)
    /\pi(X)
    \vert
    T=1
  \right]
  \P[T=1]
  \\
  &=
  \int_\mathcal{X}
  \left[ 
    Y(1)
    \vert
    T=1,
    X=x
  \right]
  \cdot
  \P[T=1]
  /
  \pi(x)
  \P_{X|T}(dx|1)
  \\
  &=
  \int_\mathcal{X}
  \left[ 
    Y(1)
    \vert
    X=x
  \right]
  \P_X(dx)
  =
  \E[Y(1)]
  .
\end{align}
The third equality is due to weak unconfoundedness and Bayes rule.
With the same arguments as above it follows
$
  \mathbf{Var}
  [
    (T/\pi(X))
    Y(T)
  ]
    =
    \E[
    (Y(1))^2
    /
    \pi(X)
    ]
    -
    \E[Y(1)]^2
    =:
    \mathbf{V}^*
    .
$
We readily calculate the learning rate 
\begin{gather}
  \sqrt{
    \mathbf{V}^*
    \frac{\log\log n}{n}
  }
  .
\end{gather}
