%read the paper \cite{Zhao2017a} for discussion of double robustness for balancing weights and \cite{Hahn1998} for semiparametric efficiency bounds.


\subsection{Learning Rates of the weighted mean}

We begin the discussion with an
uncomplicated question:
What is the speed of convergence in the weak law of large numbers?
The next statement gives a clear-cut answer to this question:
The arithmetic mean of square-integrable i.i.d. random variables 
learns with rate $n^{-1/2}$.
Furthermore, it is easy to prove using
Bienaym√©'s formula and Chebyshev's inequality (cf. \cite[Theorem~5.14]{Klenke2020}).


\begin{theorem*}
  Let 
  $
    X_1,X_2,\ldots
  $
  be i.i.d, square-integrable random variables with 
  $
    V:=
    \mathbf{Var}[X_1]
    <\infty
  $.
  Then, for any $\tau \in (0,1]$, we have
  \begin{gather}
   \P
   \left[
     \left| 
   \frac{1}{n}
   \sum_{i=1}^{n}
   (X_i - \E[X_i])
     \right|
     \le
     \sqrt{V}
     \frac{1}{\sqrt{\tau}}
     \frac{1}{\sqrt{n}}
   \right]
   \ge
   1-\tau\,.
  \end{gather}
\end{theorem*}

We implicitly claim that 


We calculate mean and variance

\begin{align}
  \begin{split}
  \E
  \left[ 
    Y(T)
    \cdot
    (T\,/\,\pi(X))
  \right]
  &\ =\ 
  \E
  \left[ 
    Y(1)
    \,
    /
    \,
    \pi(X)
    \,
    \vert
    \,
    T=1
  \right]
  \cdot
  \P[T=1]
  \\
  &\ =\ 
  \int_\mathcal{X}
  \E
  \left[ 
    Y(1)
    \,
    \vert
    \,
    X=x
    ,
    T=1
  \right]
  \cdot
  (
  \P[T=1]
  \,
  /
  \,
  \pi(x)
  )
  \,
  \P_{X|T}(dx\,|\,1)
  \\
  &\ =\ 
  \int_\mathcal{X}
  \left[ 
    Y(1)
    \vert
    X=x
  \right]
  \P_X(dx)
  =
  \E[Y(1)]
  .
\end{split}
\end{align}
The third equality is due to weak unconfoundedness and Bayes rule.
With the same arguments as above it follows
$
  \mathbf{Var}
  [
    (T/\pi(X))
    Y(T)
  ]
    =
    \E[
    (Y(1))^2
    /
    \pi(X)
    ]
    -
    \E[Y(1)]^2
    =:
    \mathbf{V}^*
    .
$
We readily calculate the learning rate 
\begin{gather}
  \sqrt{
    \mathbf{V}^*
    \frac{\log\log n}{n}
  }
  .
\end{gather}
