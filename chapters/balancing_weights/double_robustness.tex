%read the paper \cite{Zhao2017a} for discussion of double robustness for balancing weights and \cite{Hahn1998} for semiparametric efficiency bounds.


\subsection{Learning Rates of the weighted mean}

What is the speed of convergence in the weak law of large numbers?
The next statement gives a clear-cut answer:
The arithmetic mean of independent, identically distributed, square-integrable random variables 
learns with rate $n^{-1/2}$.
Furthermore, the statement is easy to prove using
Bienaym√©'s formula and Chebyshev's inequality (cf. \cite[Theorem~5.14]{Klenke2020}).


\begin{theorem*}
  Let 
  $
    X_1,X_2,\ldots
  $
  be i.i.d, square-integrable random variables with 
  $
    V:=
    \mathbf{Var}[X_1]
    <\infty
  $.
  Then, for any $\tau \in (0,1]$ and all $n\in\mathbb{N}$, we have
  \begin{gather}
   \P
   \left[
     \left| 
   \frac{1}{n}
   \sum_{i=1}^{n}
   (X_i - \E[X_i])
     \right|
     \le
     \sqrt{V}
     \frac{1}{\sqrt{\tau}}
     \frac{1}{\sqrt{n}}
   \right]
   \ge
   1-\tau\,.
  \end{gather}
\end{theorem*}
\begin{reflection*}
  Bernsteins's inequality yields better confidence.
\end{reflection*}

\todo[color=red!40, inline]{What about more refined concentration inequalities?cf Steinwart p225f}
Deriving learning rates in this way, we make an implicit assumption.
We assume that \textit{observed outcomes of the treated}
follow the same distribution as
\textit{
marginal potential outcomes under treatment
}.
In other words, we require
\begin{gather}
  Y(1)\,|\,T=1
  \ 
  \sim 
  \ 
  \P_
  {
  Y(1)
  }
  \,.
\end{gather}
In practice, virtually every scenario violates this assumption.
Compare the health of an asthmatic nonsmoker with that of an otherwise healthy smoker.
\todo[color=green!40,inline]{Find reference to more confounded scenarios.}
  Indeed, any unbalanced external influence on both $T$ and $Y(1)$ ruins the 
above assessment, simply by imposing 
\begin{gather}
  \E[Y(T)\,|\, T=1]\ \neq\  \E[Y(1)]
  \,.
\end{gather}
Statisticians have wrestled with this issue for nearly a century.

In experimental studies we usually specify treatment assignment as opposed to merely observing a unit receiving treatment. 

The next statement makes use of the propensity score.

\begin{theorem*}
  Consider the weighted mean estimator with weights
  \begin{gather}
    w_i
    \ 
    =
    \ 
    \frac{1}{n}
    \frac{T_i}{\pi(X_i)}
    \,.
  \end{gather}
  Denote
  $
    V:=
    \E[
    (
      Y(1)
    )^2
    \,/\,
    \pi(X)
    ]
    -
    \E[Y(1)]^2
  $.  
  Assume that \textit{weak unconfoundedness} holds.
  Then, for any $\tau \in (0,1]$ and all $n\in\mathbb{N}$, we have
  \begin{gather}
   \P
   \left[
     \left| 
   \sum_{i=1}^{n}
   w_iY_i - \E[Y(1)]
     \right|
     \le
     \sqrt{V}
     \frac{1}{\sqrt{\tau}}
     \frac{1}{\sqrt{n}}
   \right]
   \ge
   1-\tau\,.
  \end{gather}

\end{theorem*}
\begin{proof}
We want to reinforce coherent use of the weak law of large numbers.
To this end, we verify 
\begin{align*}
  n\,\E[w(T,X)\,Y(T)]
  \ 
  &=
  \ 
  \E[Y(1)]
  \,,
  \\
  n^2\,
  \mathbf{Var}[w(T,X)\,Y(T)]
  \ 
  &=
  \ 
    \E[
    (
      Y(1)
    )^2
    \,/\,
    \pi(X)
    ]
    -
    \E[Y(1)]^2
    \,.
\end{align*}
Essentially, the random weight $w(T,X)$ acts on $Y(T)$ through $T\,/\,\pi(X)$.
It does so by inducing independence of observed outcome $Y(T)$ and treatment $T$.
This requires that weak unconfoundedness holds, i.e., 
\begin{gather}
  (Y(0),Y(1))\perp\!\!\!\perp T\, |\, X\,.
\end{gather}
To showcase the details we added an $n$ and $n^2$ factor in the above display.
The calculations go as follows.
\begin{align}
  \begin{split}
  n\,\E[w(T,X)\,Y(T)]
  &\ 
  =
  \ 
  \E
  \left[ 
    Y(T)
    \cdot
    (T\,/\,\pi(X))
  \right]
  \\
  &\ =\ 
  \E
  \left[ 
    Y(1)
    \,
    /
    \,
    \pi(X)
    \,
    \vert
    \,
    T=1
  \right]
  \cdot
  \P[T=1]
  \\
  &\ =\ 
  \int_\mathcal{X}
  \E
  \left[ 
    Y(1)
    \,
    \vert
    \,
    X=x
    ,
    T=1
  \right]
  \cdot
  (
  \P[T=1]
  \,
  /
  \,
  \pi(x)
  )
  \,
  \P_{X|T}(dx\,|\,1)
  \\
  &\ =\ 
  \int_\mathcal{X}
  \left[ 
    Y(1)
    \vert
    X=x
  \right]
  \P_X(dx)
  =
  \E[Y(1)]
  .
\end{split}
\end{align}
The first equality holds because of the definition of the weights.
The second, third and last equality stem from 
$
  T\in \left\{ 0,1 \right\}
$,
and the law of total expectation, applied with $T$ and $X$.
The fourth equality is justified by the assumption of weak unconfoundedness.
The density transformation is due to Bayes's Theorem.
With slight modifications in the above argument, it follows
\begin{gather}
  n^2
  \E
  \left[ 
  \Big( 
    Y(T)
    \cdot
    (T\,/\,\pi(X))
  \Big)
  ^2
  \right]
  \ 
    =
    \ 
    \E
    \Big[
    (Y(1))^2
    \,
    /
    \,
    \pi(X)
    \Big]
    \,.
\end{gather}
We omit the details.
Invoking the weak law of large numbers finishes the proof.
\end{proof}

We started by asking an easy question, so it is time for a more challenging one: How do we proceed in deriving learning rates if the propensity score is unknown.
How do we generally procede?
[what has been done in the past. why are some methods obsolete]
A naive answer would be: We hope to select a proper model and try to estimate the propensity score.
Stunningly, a lot of practicioners still settle for obsolete methods when it comes to propensity score analysis.


Next, we consider the event that we have a consistent estimator of the 
propensity score and the distribution of the covariate vector $X$ has compact support.
In this case, there exists a constant $C_\pi\in (0,1/2)$ such that 
\begin{gather}
  C_\pi \ \le\  \pi(x) \ \le\  1-C_\pi 
  \qquad
  \text{for all}\ 
  x \in \mathcal{X}\,.
\end{gather}
Furthermore, 

\begin{align}
  \begin{split}
  &\sum_{i=1}^{n}
  \,
  w_i\,T_i\,Y_i
  \ 
  -
  \ 
  \E[Y(1)]
  \\
  &\quad=
    \ 
    \frac{1}{n}
    \,
  \sum_{i=1}^{n}
  \,
    \frac{T_i}{\pi(X_i)}
    \,
  \left(
    \,
    Y_i
    \ 
    -
    \ 
  \E[Y(1)]
  \,
  \right)
  \ +
  \ 
  \sum_{i=1}^{n}
  \,
  \left( 
    w_i
    \ 
    -
    \ 
    \frac{1}{n}\,
    \frac{1}{\pi(X_i)}
  \right)
  \,
  T_i
  \,
  Y_i
  \,.
  \end{split}
\end{align}

Previously we bounded the first term, so let us seek a bound for the second term.
To this end, we shall use maximal inequalities from the theory of empirical processes.
Consider
the random function
\todo[color=red!40,inline]{Why deviate from normal concentration inequalities? $w_i$ are not independent, e.g., if they sum to zero.}
\begin{gather}
  f_w(T,X,Y)
  \ 
  :=
  \ 
  \left( 
    w(T,X)
    \ 
    -
    \ 
    \frac{1}{n}\,
    \frac{1}{\pi(X)}
  \right)
  \,
  T
  \,
  Y
\end{gather}
The weights $w$ are random, so $f_w$ is random as well. 
The next assumptions reduce the technical task to a minimum.

      $
        B_\mathcal{F}
        :=
        \left\{ 
          f\in \mathcal{F}
          \colon
          \norm{f}_\infty
          \le 
          1
        \right\}
      $
\begin{assumptions*}
  \begin{enumerate}[label={(\roman*)}]
    \item
      There exists a function class $\mathcal{F}$ such that $f_w\in \mathcal{F}$ and
      $
        \log N_{[\,]}(\varepsilon, B_\mathcal{F}, L_2(\P))
        \le
        C
        (1/\varepsilon)^{1/k} 
      $
      for some $k>1/2$.
  \end{enumerate}
\end{assumptions*}


\begin{align}
  \P[
  \mathbb{G}_n f_w \le t
  ]
  \ge
  \P[
  f_w \in \mathcal{F}_n
  \ \text{and}\ 
  \norm{\G_n}_{\mathcal{F}_n}^*
  \le t
  ]
  =
  \P[
  \norm{\G_n}_{\mathcal{F}_n}^*
  \le t
  |
  f_w \in \mathcal{F}_n
  ]
  \cdot
  \P[
  f_w \in \mathcal{F}_n
  ]
  \\
  \ge
  \P[
  f_w \in \mathcal{F}_n
  ]
  -
  \P[
  f_w \in \mathcal{F}_n
  ]
  \frac{
    \E[
  \norm{\G_n}_{\mathcal{F}_n}^*
  |
  f_w \in \mathcal{F}_n
  ]
  }{t}
  \\
  =
  \P[
  f_w \in \mathcal{F}_n
  ]
  +
  \P[
  f_w \notin \mathcal{F}_n
  ]
  \frac{
    \E[
  \norm{\G_n}_{\mathcal{F}_n}^*
  |
  f_w \notin \mathcal{F}_n
  ]
  }{t}
  -
  \frac{
    \E[
  \norm{\G_n}_{\mathcal{F}_n}^*
    ]
  }{t}
  \\
  \ge
  \P[
  f_w \in \mathcal{F}_n
  ]
  -
  \frac{
    \E[
  \norm{\G_n}_{\mathcal{F}_n}^*
    ]
  }{t}
  =
  \P[
  f_w \in \mathcal{F}_n
  ]
  -
  \frac{
    \E^*[
  \norm{\G_n}_{\mathcal{F}_n}
    ]
  }{t}
\end{align}


