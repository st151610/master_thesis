%read the paper \cite{Zhao2017a} for discussion of double robustness for balancing weights and \cite{Hahn1998} for semiparametric efficiency bounds.


\subsection{Learning Rates of the weighted mean}

We begin the discussion with an
uncomplicated question:
What is the speed of convergence in the weak law of large numbers?
The next statement gives a clear-cut answer to this question:
The arithmetic mean of independent, identically distributed, square-integrable random variables 
learns with rate $n^{-1/2}$.
Furthermore, it is easy to prove using
Bienaym√©'s formula and Chebyshev's inequality (cf. \cite[Theorem~5.14]{Klenke2020}).


\begin{theorem*}
  Let 
  $
    X_1,X_2,\ldots
  $
  be i.i.d, square-integrable random variables with 
  $
    V:=
    \mathbf{Var}[X_1]
    <\infty
  $.
  Then, for any $\tau \in (0,1]$ and all $n\in\mathbb{N}$, we have
  \begin{gather}
   \P
   \left[
     \left| 
   \frac{1}{n}
   \sum_{i=1}^{n}
   (X_i - \E[X_i])
     \right|
     \le
     \sqrt{V}
     \frac{1}{\sqrt{\tau}}
     \frac{1}{\sqrt{n}}
   \right]
   \ge
   1-\tau\,.
  \end{gather}
\end{theorem*}

Deriving learning rates in this way, we make an implicit assumption.
We assume that
observed outcomes of the treated follow the same distribution as
marginal potential outcomes under treatment.
In other words, we require
\begin{gather}
  Y(1)\,|\,T=1
  \ 
  \sim 
  \ 
  \P_
  {
  Y(1)
  }
  \,.
\end{gather}
Virtually every scenario of interest violates this assumption.
Indeed, any external influence on both $T$ and $Y(1)$ can ruin the 
above assessment simply by imposing 
\begin{gather}
  \E[Y(T)]\ \neq\  \E[Y(1)]
  \,.
\end{gather}
Statisticians have wrestled with this idea for nearly a century.

We calculate mean and variance

\begin{align}
  \begin{split}
  \E
  \left[ 
    Y(T)
    \cdot
    (T\,/\,\pi(X))
  \right]
  &\ =\ 
  \E
  \left[ 
    Y(1)
    \,
    /
    \,
    \pi(X)
    \,
    \vert
    \,
    T=1
  \right]
  \cdot
  \P[T=1]
  \\
  &\ =\ 
  \int_\mathcal{X}
  \E
  \left[ 
    Y(1)
    \,
    \vert
    \,
    X=x
    ,
    T=1
  \right]
  \cdot
  (
  \P[T=1]
  \,
  /
  \,
  \pi(x)
  )
  \,
  \P_{X|T}(dx\,|\,1)
  \\
  &\ =\ 
  \int_\mathcal{X}
  \left[ 
    Y(1)
    \vert
    X=x
  \right]
  \P_X(dx)
  =
  \E[Y(1)]
  .
\end{split}
\end{align}
The third equality is due to weak unconfoundedness and Bayes rule.
With the same arguments as above it follows
$
  \mathbf{Var}
  [
    (T/\pi(X))
    Y(T)
  ]
    =
    \E[
    (Y(1))^2
    /
    \pi(X)
    ]
    -
    \E[Y(1)]^2
    =:
    \mathbf{V}^*
    .
$
We readily calculate the learning rate 
\begin{gather}
  \sqrt{
    \mathbf{V}^*
    \frac{\log\log n}{n}
  }
  .
\end{gather}
