%read the paper \cite{Zhao2017a} for discussion of double robustness for balancing weights and \cite{Hahn1998} for semiparametric efficiency bounds.


\subsection{Learning Rates of the weighted mean}

What is the speed of convergence in the weak law of large numbers?
The next statement gives a clear-cut answer:
The arithmetic mean of independent, identically distributed, square-integrable random variables 
learns with rate $n^{-1/2}$.
Furthermore, the statement is easy to prove using
Bienaym√©'s formula and Chebyshev's inequality (cf. \cite[Theorem~5.14]{Klenke2020}).


\begin{theorem*}
  Let 
  $
    X_1,X_2,\ldots
  $
  be i.i.d, square-integrable random variables with 
  $
    V:=
    \mathbf{Var}[X_1]
    <\infty
  $.
  Then, for any $\tau \in (0,1]$ and all $n\in\mathbb{N}$, we have
  \begin{gather}
   \P
   \left[
     \left| 
   \frac{1}{n}
   \sum_{i=1}^{n}
   (X_i - \E[X_i])
     \right|
     \le
     \sqrt{V}
     \frac{1}{\sqrt{\tau}}
     \frac{1}{\sqrt{n}}
   \right]
   \ge
   1-\tau\,.
  \end{gather}
\end{theorem*}
\begin{reflection*}
  Bernsteins's inequality yields better confidence.
\end{reflection*}

\todo[color=red!40, inline]{What about more refined concentration inequalities?cf Steinwart p225f}
Deriving learning rates in this way, we make an implicit assumption.
We assume that \textit{observed outcomes of the treated}
follow the same distribution as
\textit{
marginal potential outcomes under treatment
}.
In other words, we require
\begin{gather}
  Y(1)\,|\,T=1
  \ 
  \sim 
  \ 
  \P_
  {
  Y(1)
  }
  \,.
\end{gather}
In practice, virtually every scenario violates this assumption.
Compare the health of an asthmatic nonsmoker with that of an otherwise healthy smoker.
\todo[color=green!40,inline]{Find reference to more confounded scenarios.}
  Indeed, any unbalanced external influence on both $T$ and $Y(1)$ ruins the 
above assessment, simply by imposing 
\begin{gather}
  \E[Y(T)\,|\, T=1]\ \neq\  \E[Y(1)]
  \,.
\end{gather}
Statisticians have wrestled with this issue for nearly a century.

In experimental studies we usually specify treatment assignment as opposed to merely observing a unit receiving treatment. 

The next statement makes use of the propensity score.

\begin{theorem*}
  Consider the weighted mean estimator with weights
  \begin{gather}
    w_i
    \ 
    =
    \ 
    \frac{1}{n}
    \frac{T_i}{\pi(X_i)}
    \,.
  \end{gather}
  Denote
  $
    V:=
    \E[
    (
      Y(1)
    )^2
    \,/\,
    \pi(X)
    ]
    -
    \E[Y(1)]^2
  $.  
  Assume that \textit{weak unconfoundedness} holds.
  Then, for any $\tau \in (0,1]$ and all $n\in\mathbb{N}$, we have
  \begin{gather}
   \P
   \left[
     \left| 
   \sum_{i=1}^{n}
   w_iY_i - \E[Y(1)]
     \right|
     \le
     \sqrt{V}
     \frac{1}{\sqrt{\tau}}
     \frac{1}{\sqrt{n}}
   \right]
   \ge
   1-\tau\,.
  \end{gather}

\end{theorem*}
\begin{proof}
We want to reinforce coherent use of the weak law of large numbers.
To this end, we verify 
\begin{align*}
  n\,\E[w(T,X)\,Y(T)]
  \ 
  &=
  \ 
  \E[Y(1)]
  \,,
  \\
  n^2\,
  \mathbf{Var}[w(T,X)\,Y(T)]
  \ 
  &=
  \ 
    \E[
    (
      Y(1)
    )^2
    \,/\,
    \pi(X)
    ]
    -
    \E[Y(1)]^2
    \,.
\end{align*}
Essentially, the random weight $w(T,X)$ acts on $Y(T)$ through $T\,/\,\pi(X)$.
It does so by inducing independence of observed outcome $Y(T)$ and treatment $T$.
This requires that weak unconfoundedness holds, i.e., 
\begin{gather}
  (Y(0),Y(1))\perp\!\!\!\perp T\, |\, X\,.
\end{gather}
To showcase the details we added an $n$ and $n^2$ factor in the above display.
The calculations go as follows.
\begin{align}
  \begin{split}
  n\,\E[w(T,X)\,Y(T)]
  &\ 
  =
  \ 
  \E
  \left[ 
    Y(T)
    \cdot
    (T\,/\,\pi(X))
  \right]
  \\
  &\ =\ 
  \E
  \left[ 
    Y(1)
    \,
    /
    \,
    \pi(X)
    \,
    \vert
    \,
    T=1
  \right]
  \cdot
  \P[T=1]
  \\
  &\ =\ 
  \int_\mathcal{X}
  \E
  \left[ 
    Y(1)
    \,
    \vert
    \,
    X=x
    ,
    T=1
  \right]
  \cdot
  (
  \P[T=1]
  \,
  /
  \,
  \pi(x)
  )
  \,
  \P_{X|T}(dx\,|\,1)
  \\
  &\ =\ 
  \int_\mathcal{X}
  \left[ 
    Y(1)
    \vert
    X=x
  \right]
  \P_X(dx)
  =
  \E[Y(1)]
  .
\end{split}
\end{align}
The first equality holds because of the definition of the weights.
The second, third and last equality stem from 
$
  T\in \left\{ 0,1 \right\}
$,
and the law of total expectation, applied with $T$ and $X$.
The fourth equality is justified by the assumption of weak unconfoundedness.
The density transformation is due to Bayes's Theorem.
With slight modifications in the above argument, it follows
\begin{gather}
  n^2
  \E
  \left[ 
  \Big( 
    Y(T)
    \cdot
    (T\,/\,\pi(X))
  \Big)
  ^2
  \right]
  \ 
    =
    \ 
    \E
    \Big[
    (Y(1))^2
    \,
    /
    \,
    \pi(X)
    \Big]
    \,.
\end{gather}
We omit the details.
Invoking the weak law of large numbers finishes the proof.
\end{proof}

We started by asking an easy question, so it is time for a more challenging one: How do we proceed in deriving learning rates if the propensity score is unknown.
How do we generally procede?
[what has been done in the past. why are some methods obsolete]
A naive answer would be: We hope to select a proper model and try to estimate the propensity score.
Stunningly, a lot of practicioners still settle for obsolete methods when it comes to propensity score analysis.


Next, we consider the event that we have a consistent estimator of the 
propensity score and the distribution of the covariate vector $X$, 
along with that of the outcome $Y$, 
has compact support.
Then there exists a constant $C_\pi\in (0,1/2)$ such that 
\begin{gather}
  C_\pi \ \le\  \pi(x) \ \le\  1-C_\pi 
  \qquad
  \text{for all}\ 
  x \in \mathcal{X}\,.
\end{gather}

We also have
confidence $c_\tau$ and learning rate $(\varepsilon_n)$

\begin{gather}
  \P
  \left[ 
    T
    \norm{
      n w(T,X)
      -
      1/\pi(X)
    }
    \le
    c_\P
    c_{\tau}
    \varepsilon_n
  \right]
\end{gather}

Furthermore, 

Previously we bounded the first term, so let us seek a bound for the second term.
To this end, we shall use maximal inequalities from the theory of empirical processes.
Consider
the random function
  \todo[color=red!40,inline]{Why deviate from normal concentration inequalities? $w_i$ are not independent, e.g., if they sum to zero.}
\begin{gather}
\end{gather}
The weights $w$ are random, so $f_w$ is random as well. 
The next assumptions reduce the technical task to a minimum.

      $
      $
\begin{assumptions*}
  \begin{enumerate}[label={(\roman*)}]
    \item

  \end{enumerate}
\end{assumptions*}


\begin{align}
  \P[
  \mathbb{G}_n f_w \le t
  ]
  \ge
  \P[
  f_w \in \mathcal{F}_n
  \ \text{and}\ 
  \norm{\G_n}_{\mathcal{F}_n}^*
  \le t
  ]
  =
  \P[
  \norm{\G_n}_{\mathcal{F}_n}^*
  \le t
  |
  f_w \in \mathcal{F}_n
  ]
  \cdot
  \P[
  f_w \in \mathcal{F}_n
  ]
  \\
  \ge
  \P[
  f_w \in \mathcal{F}_n
  ]
  -
  \P[
  f_w \in \mathcal{F}_n
  ]
  \frac{
    \E[
  \norm{\G_n}_{\mathcal{F}_n}^*
  |
  f_w \in \mathcal{F}_n
  ]
  }{t}
  \\
  =
  \P[
  f_w \in \mathcal{F}_n
  ]
  +
  \P[
  f_w \notin \mathcal{F}_n
  ]
  \frac{
    \E[
  \norm{\G_n}_{\mathcal{F}_n}^*
  |
  f_w \notin \mathcal{F}_n
  ]
  }{t}
  -
  \frac{
    \E[
  \norm{\G_n}_{\mathcal{F}_n}^*
    ]
  }{t}
  \\
  \ge
  \P[
  f_w \in \mathcal{F}_n
  ]
  -
  \frac{
    \E[
  \norm{\G_n}_{\mathcal{F}_n}^*
    ]
  }{t}
  =
  \P[
  f_w \in \mathcal{F}_n
  ]
  -
  \frac{
    \E^*[
  \norm{\G_n}_{\mathcal{F}_n}
    ]
  }{t}
\end{align}

  \todo[color=red!40, inline]{derive concentration with markov ineq and entropy.}

It follows
\begin{gather}
  \P^*\left[ 
    \norm{\G_n}_{\mathcal{F}_n}
    \le c_n \frac{1}{\tau}
  \right]
  \ge
  1-\tau
   \,.
\end{gather}
The confidence $1/\tau$ is substandard. Improvements may involve Bernstein like concentration for empirical processes (cf. \cite[Section~2.14.2]{vaart2013})

\todo[color=red!40,inline]{Streamline the term}
\begin{gather}
  \sqrt{V}\frac{1}{\sqrt{\tau}}\frac{1}{\sqrt{n}}
  +
  \sqrt{C_\mathcal{F}}
  C_\P
  \frac{C_\tau}{\tau}
  \frac{\varepsilon_n}{\sqrt{n}}
  +
  C_Y
  (
  C_\P 
  C_{\tau_n}
  \varepsilon_n
  +
  (n
  +
  \frac{1}{C_\pi}
  )
  \tau_n
  )
\end{gather}
For this we require
existence of a rate $(\tau_n) \subset (0,1)
$
such that

$
\tau_n\cdot n \to 0
$
and
$
\varepsilon_n
\cdot
  C_{\tau_n}
  \to 0
$.
Take as learning rate 

$
\tau_n
:=
\inf
\left\{ 
t \in (0,1)
\colon
\frac{
\varepsilon_n
\cdot
  C_{t}
}{n}
  \le
  t
\right\}
$
We then achieve learning rate for the weighted mean of $\tau_n\cdot n$.
Best is $C_\tau=1$, when we recover the learnrate of the estimator $\varepsilon_n$.
and a uniform constant is 


\todo[color=red!40,inline]{Give proof}
\begin{assumptions*}
  Let the following hold.
  \begin{enumerate}[label={(\roman*)}]
    \item
      There exists $C_Y\ge 1$ such that $|Y(1)|\le C_Y$ almost surely. 
    \item
      There exists $C_\pi > 0$ such that $C_\pi < \pi(X)$ almost surely.
    \item
      There exists a function class $\mathcal{F}$ 
      with unit ball 
      $
        B_\mathcal{F}
        :=
        \left\{ 
          f\in \mathcal{F}
          \colon
          \norm{f}_\infty
          \le 
          1
        \right\}
      $
      such that  
      $
          \log N_{[\,]}(\varepsilon, B_\mathcal{F}, L_2(\P))
        \le
        C_{\mathcal{F}}
        (1/\varepsilon)^{1/k} 
      $
      for some $k>1/2$ and some constant $C_\mathcal{F}\ge 1$.
    \item
      The random function $f_w$ defined by
      $
  f_w(T,X,Y)
  \ 
  :=
  \ 
  \left( 
   n\, w(X)
    \ 
    -
    \ 
    \frac{1}{\pi(X)}
  \right)
  \,
  T
  \,
  Y
      $
      satisfies
      $f_w\in \mathcal{F}$
      almost surely
      .
  \item 
    There exist a learning rate 
    $
    (r_n)
    $,
    confidence constants
    $
    (\gamma_\tau)
    $
    and 
    uniform constant
    $
    C_w\ge 1
    $
    such that
    for all $\tau \in(0,1]$
    and all $n \in \mathbb{N}$
    it holds
    $
    \P
    \left[ 
    \norm{w(X) - \frac{1}{\pi(X)}}_\infty
    \le
    C_w
    c_\tau
    \varepsilon_n
    \right]
    \ge 
    1-\tau
    $
    \item
      There exists $\alpha > 1$ 
  such that 
  $
  \varepsilon_n \cdot c_{n^{-\alpha}}
  \to 
  0
  $
  as 
  $
  n\to \infty
  $
  and 
  $
  \varepsilon_n \cdot c_{n^{-\alpha}}
  \le 
  1
  $
  for all 
  $n \in \mathbb{N}$.
  \end{enumerate}
\end{assumptions*}
\begin{theorem*}
  Let the assumptions.
  Then the weighted mean learns with rate
  $
    (\varepsilon_n)
  $
  defined by
  \begin{gather*}
    \varepsilon_n
    \ 
    :=
    \ 
    \inf
    \left\{ \,t \,\in\,  (\,0,1]
      \ 
      \colon
      \ 
  r_n \cdot \gamma_{\,t/n}
  \ 
  \le
  \ 
  t
  \,
    \right\}
    \ 
    \land 
    \ 
    1
    \qquad
    \text{for all}
    \ 
    n\in \mathbb{N}
    \,.
  \end{gather*}
  Furthermore, it has confidence constants
  $
  (c_\tau)
  $
  given by
\begin{gather}
  c_\tau = \gamma_\tau /\tau
\end{gather}
  and uniform constant
  \begin{gather}
C_\P    
=
\max
\left\{ 
  \sqrt{C_\mathcal{F}}
  C_w
  ,
  C_Y
  C_w
  ,
  \frac{C_Y}{C_\pi}
\right\}
  \end{gather}
\end{theorem*}

\begin{proof}
  We consider the following error decomposition.
\begin{align}
  \begin{split}
  &\sum_{i=1}^{n}
  \,
  w_i\,T_i\,Y_i
  \ 
  -
  \ 
  \E[Y(1)]
  \\
  &\quad=
    \ 
    \frac{1}{n}
    \,
  \sum_{i=1}^{n}
  \,
    \frac{T_i}{\pi(X_i)}
    \,
  \left(
    \,
    Y_i
    \ 
    -
    \ 
  \E[Y(1)]
  \,
  \right)
  \ +
  \ 
  \sum_{i=1}^{n}
  \,
  \left( 
    w_i
    \ 
    -
    \ 
    \frac{1}{n}\,
    \frac{1}{\pi(X_i)}
  \right)
  \,
  T_i
  \,
  Y_i
  \,.
  \\
  & 
  \quad
  =
  \ 
   \frac{1}{n}
    \,
  \sum_{i=1}^{n}
  \,
    \frac{T_i}{\pi(X_i)}
    \,
  \left(
    \,
    Y_i
    \ 
    -
    \ 
  \E[Y(1)]
  \,
  \right)
  \ 
  +
  \ 
  \frac{1}{\sqrt{n}}
  \,
  \G_n f_w
  \ 
  -
  \ 
  \E[
  \,
  f_w(T,X,Y)
  \,
  ]
  \,.
  \end{split}
\end{align}
We already bounded the first term. 
To bound the remaining terms we will use the learning rates of $w$.
To this end, we employ maximal inequalities for empirical processes to bound the second term.
We bound the third term by the law of total expectation and balancing learning rates and confidence.
\subsubsection*{2nd term}
Denote
$
  \mathcal{F}_{n,\tau}
  :=
  (
  C_Y
  C_w
  \gamma_\tau
  r_n
  )
  \cdot
  B_\mathcal{F}
  =:
  \delta_{n,\tau}
  \cdot
  B_\mathcal{F}
$.
It holds by maximal inequalities
\begin{align*}
  \E^*\left[ 
    \norm{\G_n}_{\mathcal{F}_{n,\tau}}
  \right]
  &
  \ 
  \le
  \ 
  \int_0^{\delta{n,\tau}}
  \sqrt{
    \log 
    N_{[\,]}
    \left( 
      \varepsilon / \delta_{n,\tau}
    ,
    B_{\mathcal{F}}
    ,
    L_2(\P)
    \right)
  }
  d\varepsilon
  \\
  &
  \ 
  \le
  \ 
  \int_0^{\delta_{n,\tau}}
  \left( 
    \frac{\delta_{n,\tau}}{\varepsilon}
  \right)^{1/(2k)}
  d \varepsilon
  \ 
  =
  \ 
  \delta_{n,\tau}
  \,.
\end{align*}
For $t>0$,
Markov's inequality gives
\begin{gather}
  \P\left[
  \norm{\G_n}^*_{\mathcal{F}_{n,\tau}}
   \ge t
  \right]
  \ 
  \le
  \ 
  \frac{1
  }{t}
  \,
    \E\left[
  \norm{\G_n}^*_{\mathcal{F}_{n,\tau}}
    \right]
  \ 
  \le
  \ 
  \frac{1
  }{t}
  \,
    \E^*\left[
  \norm{\G_n}_{\mathcal{F}_{n,\tau}}
    \right]
    \le
    \frac{\delta_{n,\tau}}{t}
    \,,
\end{gather}
and consequently
\begin{gather}
  \P\left[
  \norm{\G_n}^*_{\mathcal{F}_{n,\tau}}
  \le \frac{1}{\tau}
  C_Y
  C_w
  \gamma_\tau
  r_n
  \right]
  \ 
  \ge
  \ 
  1-\tau
\end{gather}



\end{proof}
