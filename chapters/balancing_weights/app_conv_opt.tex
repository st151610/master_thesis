\begin{assumption}
  \begin{enumerate}[label={(\roman*)}]
    Assume that the map 
    $
      f: \R \to \overline{\R}
    $
    has the following properties.
    \item
      $
        f 
        \ 
        \text{is strictly convex.}
      $
    \item
      $
        f
        \ 
        \text{is lower-semicontinuous and continuously differentiable on}
        \ 
        \mathrm{int}(\mathrm{dom}(f))
        .
      $
    \item
      $
        \text{The derivative of}\ 
        f\ 
        \text{on}\ 
        \mathrm{int}(\mathrm{dom}(f))
        \ 
        \text{is a diffeomorphism.}
      $
    \item
      $
        \text{The Legendre transformation}
        \ 
        f^*
        \ 
        \text{of}\ 
        f
        \ 
        \text{is finite}
        .
      $
    \item
      $
        \text{The function}\
        x\mapsto xt - f(x)
        \ 
        \text{takes its supremum on}
        \ 
        \mathrm{int}(\mathrm{dom}(f))
        \ 
        \text{for all}
        \ 
        t\in \R.
      $
  \end{enumerate}
\end{assumption}
Assumption (i),(ii) and (iv) are from \cite{Tseng1991}. See corresponding section in the thesis.
Assumption (iii) and (v) is needed to get a meaningful analytic expression for the weights after establishing the properties pointed out in \cite{Tseng1991}.

\todo[color=red!40, inline]{Elaborate on assumptions. Give as a counterexample 
  \[
  f(x):=
  (1+\delta_{x\ge 0})
  (x^2
  +x\cdot \lambda)
\]
}

We consider the following optimization problem.
\todo[color=orange!40, inline]{Consider Non-negativity constraints only for $f(x)=x\log x$. Incorporating Non-negativity constraint for general $f$ is complicated or impossible. Maybe find counterexample.}
  \begin{gather*}
    \underset{w_1, \ldots, w_n \in \R}{\mathrm{minimize}}
    \qquad
    \sum_{i = 1}^{n} T_i f(w_i)
  \end{gather*}
subject to the constraints
\begin{gather*}
%    w_i T_i \ge 0,
%   \qquad 
%    i = 1, \ldots, n,
%   \\
%  \sum_{ i = 1 }^{n}
%    w_i T_i
%  =
%  1
%  \\
    \left| 
      \frac{1}{n} 
      \sum_{i = 1}^{n} 
      (
      w_i T_i 
      - 
      1
      )
      \cdot
      B_k(X_i)
    \right|
    \ 
    \le 
    \ 
    \delta_k,
    \qquad
    k = 1, \ldots, K
\end{gather*}
\pagebreak
~\begin{ftheorem}
  Under Assumption,
  the dual of the above Problem is the unconstrained optimization problem 
  \begin{gather*}
    \underset{\lambda \in \R^K}{\mathrm{minimize}}
    \qquad
    \frac{1}{n}
    \sum_{i = 1}^{n} 
    T_i 
    \cdot
    f^*
    (
      \inner{B(X_i)}{\lambda}
    )
    -
    \inner{B(X_i)}{\lambda}
    \ 
    +
    \ 
    \inner{\delta}{\left| \lambda \right|}
    \,,
  \end{gather*}
  where
  \begin{gather*}
  f^*
  \,
  \colon
  \, 
  \R
  \ 
  \to
  \ 
  \R
  \,
  ,
  \qquad 
  t 
  \ 
  \mapsto
  \ 
    t\,(f^{'})^{-1}(t)
  \ 
    -
  \ 
    f
    \left( 
      (f^{'})^{-1}(t)
    \right)
  \end{gather*}
  is the Legendre transformation of $f$,
  $
    B(X_i)
    =
    \left[ 
      B_1(X_i)
      ,
      \ldots
      ,
      B_K(X_i)
    \right]
    ^\top
  $
  denotes the $K$ basis functions of the covariates 
  of unit $i\in \left\{ 1, \ldots, n \right\}$
  and
  $
    \left| \lambda \right|
    =
    \left[ 
      \left| \lambda_1 \right|
      ,
      \ldots
      ,
      \left| \lambda_K \right|
    \right]
    ^\top
    ,
  $
  where $\left| \,\cdot\, \right|$
  is the absolute value of a real-valued scalar.
  Moreover, if $\lambda^\dagger$
  is an optimal solution then
  \begin{gather*}
    w_i^*
    \ 
    =
    \ 
    (f^{'})^{-1}
    \left( 
      \inner{B(X_i)}{\lambda^\dagger}
    \right)
    \quad
    \text{for all}\ 
    i
    \ 
    \text{with}\ 
    T_i=1
  \end{gather*}
  are uniquely part of any optimal solution to (P)
  .
\end{ftheorem}
\begin{proof}
  We prove the following Lemma at the end of the section.
  \begin{lemma}
    The dual of the optimization problem is
  \begin{gather*}
    \underset{\lambda \in \R^{2K}}{\mathrm{minimize}}
    \qquad
    \frac{1}{n}
    \sum_{i = 1}^{n} 
    T_i 
    \cdot
    f^*
    (
    \inner{Q_{\bullet i}}{\lambda}
    )
    -
    \inner{Q_{\bullet i}}{\lambda}
    \ 
    +
    \ 
    \inner{d}{\lambda}
  \end{gather*}
  subject to
  \begin{gather*}
    \lambda_k \ge 0
    \quad
    \text{for all}\ 
    k \in \left\{ 1, \ldots, K \right\}
    ,
  \end{gather*}
  where
  \begin{gather*}
    \mathbf{Q}
    :=
    \begin{bmatrix}
     % \mathbf{I}_n\\
     % \mathbf{B}(\mathbf{X})\\
      \pm \, \mathbf{B}(\mathbf{X})
    \end{bmatrix}
    ,
    \qquad
    \mathbf{B}(\mathbf{X})
    :=
    \begin{bmatrix}
      B(X_1), \ldots, B(X_n)
    \end{bmatrix}
    ,
    \qquad
    \text{and}
    \qquad
    d
    :=
    \begin{bmatrix}
      %0_n\\
      \delta \\
      \delta
    \end{bmatrix}
    .
  \end{gather*}

  \end{lemma}
  \begin{proof}
    First we disentangle the box constraints.
    To this end, we get
    \begin{gather*}
    \underset{w_1, \ldots, w_n \in \R}{\mathrm{minimize}}
    \qquad
    \sum_{i = 1}^{n} T_i f(w_i)
    \\
      \pm\,\sum_{i = 1}^{n} w_i T_i B_k(X_i)
      \ 
      \ge
      \ 
      -n\cdot\delta_k
      \ 
      \pm 
      \ 
      \sum_{i = 1}^{n} B_k(X_i)
      \,,
      \qquad
      k=1,\ldots,K
      \,.
    \end{gather*}
    The corresponding matrix notation is
 \begin{gather*}
    \underset{w_1, \ldots, w_n \in \R}{\mathrm{minimize}}
    \qquad
    \sum_{i = 1}^{n} T_i \cdot f(w_i)
    \\
    \mathbf{Q}w 
    \ 
    \ge
    \ 
    d
    \,,
\end{gather*}
where
\begin{align*}
    \mathbf{Q}
    &
    \ 
    :=
    \ 
    \begin{bmatrix}
      \pm\,T\mathbf{B}(\mathbf{X})
    \end{bmatrix}
    ,
    \\
    T\mathbf{B}(\mathbf{X})
    &
    \ 
    :=
    \ 
    \begin{bmatrix}
      T_1B(X_1), \ldots, T_nB(X_n)
    \end{bmatrix}
    ,
    \\
    d
    &
    \ 
    :=
    \ 
    \begin{bmatrix}
      -n\cdot\delta 
      \pm\,
      \sum_{i = 1}^{n} B_k(X_i)
    \end{bmatrix}
    \,.
  \end{align*}
The convex conjugate is
\begin{gather*}
  \sum_{T_i=1} T_i f^*(\lambda_i)
  +
  \sum_{T_i=0} 
  \delta_{\left\{ 0 \right\}}(\lambda_i)
  \,,
\end{gather*}
where
\begin{gather*}
  \delta_{\left\{ 0 \right\}}
  (t)
  =
  \begin{cases}
    0,& \text{if}\, t=0,\\
    \infty,& \text{else}\,.
  \end{cases}
\end{gather*}
Note that
\begin{gather*}
  \sum_{T_i=0} 
  \delta_{\left\{ 0 \right\}}
  (
  T_i \cdot \inner{B(X_i)}{\lambda}
  )
  =
  0
  \,.
\end{gather*}

The corresponding dual problem in \cite{Tseng1991} is then
\begin{gather*}
  \underset
  {\lambda_1,\ldots,\lambda_{K}\ge 0}
  {
  \mathrm{minimize}
  }
  \quad
  \sum_{i=1} 
  ^n
  T_i
  \cdot
  f^*
  (
  T_i \cdot\inner{B(X_i)}{\lambda}
  )
  -
  \inner{B(X_i)}{\lambda}
  \ 
  +
  \ 
  n\inner{\lambda}{d}
  \,.
\end{gather*}
  If we keep only the outer $T_i$ and divide by $1/n$ the problem remains the same.
  This concludes the proof.
  \end{proof}

\todo[color=red!40, inline]{Continue with paraphrase of \cite[page 20]{Wang2019}}

\end{proof}
