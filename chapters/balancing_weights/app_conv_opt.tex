\subsection*{Introduction}
%\begin{assumption}
%  \begin{enumerate}[label={(\roman*)}]
%    Assume that the map 
%    $
%      f: \R \to \overline{\R}
%    $
%    has the following properties.
%    \item
%      $
%        f 
%        \ 
%        \text{is strictly convex.}
%      $
%    \item
%      $
%        f
%        \ 
%        \text{is lower-semicontinuous and continuously differentiable on}
%        \ 
%        \mathrm{int}(\mathrm{dom}(f))
%        .
%      $
%    \item
%      $
%        \text{The derivative of}\ 
%        f\ 
%        \text{on}\ 
%        \mathrm{int}(\mathrm{dom}(f))
%        \ 
%        \text{is a diffeomorphism.}
%      $
%    \item
%      $
%        \text{The Legendre transformation}
%        \ 
%        f^*
%        \ 
%        \text{of}\ 
%        f
%        \ 
%        \text{is finite}
%        .
%      $
%    \item
%      $
%        \text{The function}\
%        x\mapsto xt - f(x)
%        \ 
%        \text{takes its supremum on}
%        \ 
%        \mathrm{int}(\mathrm{dom}(f))
%        \ 
%        \text{for all}
%        \ 
%        t\in \R.
%      $
%  \end{enumerate}
%\end{assumption}
%Assumption (i),(ii) and (iv) are from \cite{Tseng1991}. See corresponding section in the thesis.
%Assumption (iii) and (v) is needed to get a meaningful analytic expression for the weights after establishing the properties pointed out in \cite{Tseng1991}.
%
%\todo[color=red!40, inline]{Elaborate on assumptions. Give as a counterexample 
%  \[
%  f(x):=
%  (1+\delta_{x\ge 0})
%  (x^2
%  +x\cdot \lambda)
%\]
%}
%
%We consider the following optimization problem.
%\todo[color=orange!40, inline]{Consider Non-negativity constraints only for $f(x)=x\log x$. Incorporating Non-negativity constraint for general $f$ is complicated or impossible. Maybe find counterexample.}
We consider a partition
$
  \mathcal{P}_n
  =
  \left\{ 
    A_{n,1}
    ,
    A_{n,2}
    ,
    \ldots
  \right\}
$
of $ \R^d $
and define
$ A_n(x) $ to be the cell of $ \mathcal{P}_n $ containing $x$.
Next we define $ m_n $ by
\begin{gather}
  m_n(Y|x)
  \ 
  :=
  \ 
  \frac
  {
    \sum_{k=1}^{n} 
    Y_k
    \cdot
    \mathbf{1}
    _
    {
      \left\{ 
      X_k \in A_n(x)
      \right\}
    }
  }
  {
    \sum_{j=1}^{n} 
    \mathbf{1}
    _
    {
      \left\{ 
      X_j \in A_n(x)
      \right\}
    }
  }
  \,.
\end{gather}
In the terminology of \cite[§4]{Gyorfi2002}
$m_n$ is called a partitioning estimate.
We want to control the sumands.
To this end we define a set of basis functions by
\begin{gather}
  B_k(x)
  \ 
  :=
  \ 
  \frac
  {
    \mathbf{1}
    _
    {
      \left\{ 
      X_k \in A_n(x)
      \right\}
    }
  }
  {
    \sum_{j=1}^{n} 
    \mathbf{1}
    _
    {
      \left\{ 
      X_j \in A_n(x)
      \right\}
    }
  }
  \qquad
  \text{for}\ 
  k \in \left\{ 1,\ldots,n \right\}
  \,.
\end{gather}
This yields
\begin{gather}
  m_n(Y|x)
  =
  \sum_{k=1}^{n} 
  Y_k
  \cdot
  B_k(x)
  \,.
\end{gather}
We consider the objective function
\begin{gather}
  f
  \ 
  \colon
  \ 
  [0,\infty)
  \ 
  \to 
  \ 
  \R
  ,
  \quad
  x
  \ 
  \mapsto
  \ 
  x \log x\,,
\end{gather}
together with 
										%%%%%%%%%%%%%
										%%%% (P) %%%%
										%%%%%%%%%%%%%
\begin{fproblem}
  \label{bw:1:primal}
\begin{align*}
  %%%% objective %%%%
    &\underset{w_1, \ldots, w_n \in \R}
    {\text{minimize}}
    &&\qquad\qquad
    \sum_{i = 1}^{n} 
    T_i
    f(w_i)
    &&&
    \\
    %%%% w_i T_i >= 0 %%%%
    &\text{subject to}
    &&\qquad\qquad
    w_i T_i
    \ge
    0
    &&&
    \qquad
    \text{for all}\ 
    i \in \left\{ 1, \ldots, n \right\}
    \,,
    \\
    %%%% 1/n sum w = 1 %%%%
    & 
    &&\qquad\qquad
    \frac{1}{n}
    \sum_{i=1}^{n} 
    T_i w_i
    =1
    \\
    %%%% box constraints %%%%
    & 
    &&\qquad
    \left| 
      \frac{1}{n} 
      \sum_{i = 1}^{n} 
      (
      w_i T_i 
      - 
      1
      )
      \cdot
      B_k(X_i)
    \right|
    \ 
    \le 
    \ 
    \delta_k
    &&&
    \qquad
    \text{for all}\ 
    k \in \left\{ 1, \ldots, n \right\}
    \,.
\end{align*}
\end{fproblem}
%%%%%%%%%%%%%
%%%% (D) %%%%
%%%%%%%%%%%%%
\subsubsection*{Dual Problem}
\begin{ftheorem}
  The dual of Problem~\ref{bw:1:primal} is the unconstrained optimization problem 
  \begin{gather*}
    \underset{\lambda \in \R^n}{\mathrm{minimize}}
    \qquad
    \frac{1}{n}
    \sum_{i = 1}^{n} 
    T_i 
    \cdot
    f^*
    (
      m_n(\lambda|X_i)
    )
    -
      m_n(\lambda|X_i)
    \ 
    +
    \ 
    \inner{\delta}{\left| \lambda \right|}
    \,,
  \end{gather*}
  where
  \begin{gather*}
  f^*
  \,
  \colon
  \, 
  \R
  \ 
  \to
  \ 
  \R
  \,
  ,
  \qquad 
  t 
  \ 
  \mapsto
  \ 
    t\,(f^{'})^{-1}(t)
  \ 
    -
  \ 
    f
    \left( 
      (f^{'})^{-1}(t)
    \right)
  \end{gather*}
  is the Legendre transformation of $f$,
  $
    B(X_i)
    =
    \left[ 
      B_1(X_i)
      ,
      \ldots
      ,
      B_K(X_i)
    \right]
    ^\top
  $
  denotes the $K$ basis functions of the covariates 
  of unit $i\in \left\{ 1, \ldots, n \right\}$
  and
  $
    \left| \lambda \right|
    =
    \left[ 
      \left| \lambda_1 \right|
      ,
      \ldots
      ,
      \left| \lambda_K \right|
    \right]
    ^\top
    ,
  $
  where $\left| \,\cdot\, \right|$
  is the absolute value of a real-valued scalar.
  Moreover, if $\lambda^\dagger$
  is an optimal solution then
  \begin{gather*}
    w_i^*
    \ 
    =
    \ 
    (f^{'})^{-1}
    \left( 
      m_n
      (
      \lambda^\dagger
      |
      X_i
      )
    \right)
    \quad
    \text{for all}\ 
    i
    \ 
    \text{with}\ 
    T_i=1
  \end{gather*}
  are uniquely part of any optimal solution to (P)
  .
\end{ftheorem}
\begin{proof}
  We prove the following Lemma at the end of the section.
  \begin{lemma}
    The dual of the optimization problem is
  \begin{gather*}
    \underset{\lambda \in \R^{2K}}{\mathrm{minimize}}
    \qquad
    \frac{1}{n}
    \sum_{i = 1}^{n} 
    T_i 
    \cdot
    f^*
    (
    \inner{Q_{\bullet i}}{\lambda}
    )
    -
    \inner{Q_{\bullet i}}{\lambda}
    \ 
    +
    \ 
    \inner{d}{\lambda}
  \end{gather*}
  subject to
  \begin{gather*}
    \lambda_k \ge 0
    \quad
    \text{for all}\ 
    k \in \left\{ 1, \ldots, K \right\}
    ,
  \end{gather*}
  where
  \begin{gather*}
    \mathbf{Q}
    :=
    \begin{bmatrix}
     % \mathbf{I}_n\\
     % \mathbf{B}(\mathbf{X})\\
      \pm \, \mathbf{B}(\mathbf{X})
    \end{bmatrix}
    ,
    \qquad
    \mathbf{B}(\mathbf{X})
    :=
    \begin{bmatrix}
      B(X_1), \ldots, B(X_n)
    \end{bmatrix}
    ,
    \qquad
    \text{and}
    \qquad
    d
    :=
    \begin{bmatrix}
      %0_n\\
      \delta \\
      \delta
    \end{bmatrix}
    .
  \end{gather*}

  \end{lemma}
  \begin{proof}
    First we disentangle the box constraints.
    To this end, we get
    \begin{gather*}
    \underset{w_1, \ldots, w_n \in \R}{\mathrm{minimize}}
    \qquad
    \sum_{i = 1}^{n} T_i f(w_i)
    \\
      \pm\,\sum_{i = 1}^{n} w_i T_i B_k(X_i)
      \ 
      \ge
      \ 
      -n\cdot\delta_k
      \ 
      \pm 
      \ 
      \sum_{i = 1}^{n} B_k(X_i)
      \,,
      \qquad
      k=1,\ldots,K
      \,.
    \end{gather*}
    The corresponding matrix notation is
 \begin{gather*}
    \underset{w_1, \ldots, w_n \in \R}{\mathrm{minimize}}
    \qquad
    \sum_{i = 1}^{n} T_i \cdot f(w_i)
    \\
    \mathbf{Q}w 
    \ 
    \ge
    \ 
    d
    \,,
\end{gather*}
where
\begin{align*}
    \mathbf{Q}
    &
    \ 
    :=
    \ 
    \begin{bmatrix}
      \pm\,T\mathbf{B}(\mathbf{X})
    \end{bmatrix}
    ,
    \\
    T\mathbf{B}(\mathbf{X})
    &
    \ 
    :=
    \ 
    \begin{bmatrix}
      T_1B(X_1), \ldots, T_nB(X_n)
    \end{bmatrix}
    ,
    \\
    d
    &
    \ 
    :=
    \ 
    \begin{bmatrix}
      -n\cdot\delta 
      \pm\,
      \sum_{i = 1}^{n} B_k(X_i)
    \end{bmatrix}
    \,.
  \end{align*}
The convex conjugate is
\begin{gather*}
  \sum_{T_i=1} T_i f^*(\lambda_i)
  +
  \sum_{T_i=0} 
  \delta_{\left\{ 0 \right\}}(\lambda_i)
  \,,
\end{gather*}
where
\begin{gather*}
  \delta_{\left\{ 0 \right\}}
  (t)
  =
  \begin{cases}
    0,& \text{if}\, t=0,\\
    \infty,& \text{else}\,.
  \end{cases}
\end{gather*}
Note that
\begin{gather*}
  \sum_{T_i=0} 
  \delta_{\left\{ 0 \right\}}
  (
  T_i \cdot \inner{B(X_i)}{\lambda}
  )
  =
  0
  \,.
\end{gather*}

The corresponding dual problem in \cite{Tseng1991} is then
\begin{gather*}
  \underset
  {\lambda_1,\ldots,\lambda_{K}\ge 0}
  {
  \mathrm{minimize}
  }
  \quad
  \sum_{i=1} 
  ^n
  T_i
  \cdot
  f^*
  (
  T_i \cdot\inner{B(X_i)}{\lambda}
  )
  -
  \inner{B(X_i)}{\lambda}
  \ 
  +
  \ 
  n\inner{\lambda}{d}
  \,.
\end{gather*}
  If we keep only the outer $T_i$ and divide by $1/n$ the problem remains the same.
  This concludes the proof.
  \end{proof}

\todo[color=red!40, inline]{Continue with paraphrase of \cite[page 20]{Wang2019}}
%%%%%%%%%%%
%%%% λ %%%%
%%%%%%%%%%%
\subsection*{Consistency of the dual variables}
%%%%%%%%%%%
%%%% ω %%%%
%%%%%%%%%%%
\subsection*{Consistency of the weights}
%%%%%%%%%%%
%%%% E[Y] %%%%
%%%%%%%%%%%
\subsection*{Consistency of the weighted mean}
\end{proof}
