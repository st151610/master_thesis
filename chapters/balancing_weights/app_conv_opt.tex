\subsection*{Introduction}
%\begin{assumption}
%  \begin{enumerate}[label={(\roman*)}]
%    Assume that the map 
%    $
%      f: \R \to \overline{\R}
%    $
%    has the following properties.
%    \item
%      $
%        f 
%        \ 
%        \text{is strictly convex.}
%      $
%    \item
%      $
%        f
%        \ 
%        \text{is lower-semicontinuous and continuously differentiable on}
%        \ 
%        \mathrm{int}(\mathrm{dom}(f))
%        .
%      $
%    \item
%      $
%        \text{The derivative of}\ 
%        f\ 
%        \text{on}\ 
%        \mathrm{int}(\mathrm{dom}(f))
%        \ 
%        \text{is a diffeomorphism.}
%      $
%    \item
%      $
%        \text{The Legendre transformation}
%        \ 
%        f^*
%        \ 
%        \text{of}\ 
%        f
%        \ 
%        \text{is finite}
%        .
%      $
%    \item
%      $
%        \text{The function}\
%        x\mapsto xt - f(x)
%        \ 
%        \text{takes its supremum on}
%        \ 
%        \mathrm{int}(\mathrm{dom}(f))
%        \ 
%        \text{for all}
%        \ 
%        t\in \R.
%      $
%  \end{enumerate}
%\end{assumption}
%Assumption (i),(ii) and (iv) are from \cite{Tseng1991}. See corresponding section in the thesis.
%Assumption (iii) and (v) is needed to get a meaningful analytic expression for the weights after establishing the properties pointed out in \cite{Tseng1991}.
%
%\todo[color=red!40, inline]{Elaborate on assumptions. Give as a counterexample 
%  \[
%  f(x):=
%  (1+\delta_{x\ge 0})
%  (x^2
%  +x\cdot \lambda)
%\]
%}
%
%We consider the following optimization problem.
%\todo[color=orange!40, inline]{Consider Non-negativity constraints only for $f(x)=x\log x$. Incorporating Non-negativity constraint for general $f$ is complicated or impossible. Maybe find counterexample.}
We consider a partition
$
  \mathcal{P}_n
  =
  \left\{ 
    A_{n,1}
    ,
    A_{n,2}
    ,
    \ldots
  \right\}
$
of $ \R^d $
and define
$ A_n(x) $ to be the cell of $ \mathcal{P}_n $ containing $x$.
Next we define $ m_n $ by
\begin{gather}
  m_n(Y|x)
  \ 
  :=
  \ 
  \frac
  {
    \sum_{k=1}^{n} 
    Y_k
    \cdot
    \mathbf{1}
    _
    {
      \left\{ 
      X_k \in A_n(x)
      \right\}
    }
  }
  {
    \sum_{j=1}^{n} 
    \mathbf{1}
    _
    {
      \left\{ 
      X_j \in A_n(x)
      \right\}
    }
  }
  \,.
\end{gather}
In the terminology of \cite[ยง4]{Gyorfi2002}
$m_n$ is called a partitioning estimate.
We want to control the sumands.
To this end we define a set of basis functions by
\begin{gather}
  B_k(x)
  \ 
  :=
  \ 
  \frac
  {
    \mathbf{1}
    _
    {
      \left\{ 
      X_k \in A_n(x)
      \right\}
    }
  }
  {
    \sum_{j=1}^{n} 
    \mathbf{1}
    _
    {
      \left\{ 
      X_j \in A_n(x)
      \right\}
    }
  }
  \qquad
  \text{for}\ 
  k \in \left\{ 1,\ldots,n \right\}
  \,.
\end{gather}
This yields
\begin{gather}
  m_n(Y|x)
  =
  \sum_{k=1}^{n} 
  Y_k
  \cdot
  B_k(x)
  \,.
\end{gather}
We consider the objective function
\begin{gather}
  f
  \ 
  \colon
  \ 
  [0,\infty)
  \ 
  \to 
  \ 
  \R
  ,
  \quad
  x
  \ 
  \mapsto
  \ 
  x \log x\,,
\end{gather}
together with 
										%%%%%%%%%%%%%
										%%%% (P) %%%%
										%%%%%%%%%%%%%
\begin{fproblem}
  \label{bw:1:primal}
\begin{align*}
  %%%% objective %%%%
    &\underset{w_1, \ldots, w_n \in \R}
    {\text{minimize}}
    &&\qquad\qquad
    \sum_{i = 1}^{n} 
    T_i
    f(w_i)
    &&&
    \\
    %%%% w_i T_i >= 0 %%%%
    &\text{subject to}
    &&\qquad\qquad
    w_i T_i
    \ge
    0
    &&&
    \qquad
    \text{for all}\ 
    i \in \left\{ 1, \ldots, n \right\}
    \,,
    \\
    %%%% 1/n sum w = 1 %%%%
    & 
    &&\qquad\qquad
    \frac{1}{n}
    \sum_{i=1}^{n} 
    T_i w_i
    =1
    \\
    %%%% box constraints %%%%
    & 
    &&\qquad
    \left| 
      \frac{1}{n} 
      \sum_{i = 1}^{n} 
      (
      w_i T_i 
      - 
      1
      )
      \cdot
      B_k(X_i)
    \right|
    \ 
    \le 
    \ 
    \delta_k
    &&&
    \qquad
    \text{for all}\ 
    k \in \left\{ 1, \ldots, n \right\}
    \,.
\end{align*}
\end{fproblem}
%%%%%%%%%%%%%
%%%% (D) %%%%
%%%%%%%%%%%%%
\subsubsection*{Dual Problem}
\begin{ftheorem}
  The dual of Problem~\ref{bw:1:primal} is the unconstrained optimization problem 
  \begin{gather*}
    \underset{\lambda \in \R^n}{\mathrm{minimize}}
    \qquad
    \frac{1}{n}
    \sum_{i = 1}^{n} 
    T_i 
    \cdot
    f^*
    (
      m_n(\lambda|X_i)
    )
    -
      m_n(\lambda|X_i)
    \ 
    +
    \ 
    \inner{\delta}{\left| \lambda \right|}
    \,,
  \end{gather*}
  where
  \begin{gather*}
  f^*
  \,
  \colon
  \, 
  \R
  \ 
  \to
  \ 
  \R
  \,
  ,
  \qquad 
  t 
  \ 
  \mapsto
  \ 
    t\,(f^{'})^{-1}(t)
  \ 
    -
  \ 
    f
    \left( 
      (f^{'})^{-1}(t)
    \right)
  \end{gather*}
  is the Legendre transformation of $f$,
  $
    B(X_i)
    =
    \left[ 
      B_1(X_i)
      ,
      \ldots
      ,
      B_K(X_i)
    \right]
    ^\top
  $
  denotes the $K$ basis functions of the covariates 
  of unit $i\in \left\{ 1, \ldots, n \right\}$
  and
  $
    \left| \lambda \right|
    =
    \left[ 
      \left| \lambda_1 \right|
      ,
      \ldots
      ,
      \left| \lambda_K \right|
    \right]
    ^\top
    ,
  $
  where $\left| \,\cdot\, \right|$
  is the absolute value of a real-valued scalar.
  Moreover, if $\lambda^\dagger$
  is an optimal solution then
  \begin{gather*}
    w_i^*
    \ 
    =
    \ 
    (f^{'})^{-1}
    \left( 
      m_n
      (
      \lambda^\dagger
      |
      X_i
      )
    \right)
    \quad
    \text{for all}\ 
    i
    \ 
    \text{with}\ 
    T_i=1
  \end{gather*}
  are uniquely part of any optimal solution to (P)
  .
\end{ftheorem}
\begin{proof}
  We prove the following Lemma at the end of the section.
  \begin{lemma}
    The dual of the optimization problem is
  \begin{gather*}
    \underset{\lambda \in \R^{2K}}{\mathrm{minimize}}
    \qquad
    \frac{1}{n}
    \sum_{i = 1}^{n} 
    T_i 
    \cdot
    f^*
    (
    \inner{Q_{\bullet i}}{\lambda}
    )
    -
    \inner{Q_{\bullet i}}{\lambda}
    \ 
    +
    \ 
    \inner{d}{\lambda}
  \end{gather*}
  subject to
  \begin{gather*}
    \lambda_k \ge 0
    \quad
    \text{for all}\ 
    k \in \left\{ 1, \ldots, K \right\}
    ,
  \end{gather*}
  where
  \begin{gather*}
    \mathbf{Q}
    :=
    \begin{bmatrix}
     % \mathbf{I}_n\\
     % \mathbf{B}(\mathbf{X})\\
      \pm \, \mathbf{B}(\mathbf{X})
    \end{bmatrix}
    ,
    \qquad
    \mathbf{B}(\mathbf{X})
    :=
    \begin{bmatrix}
      B(X_1), \ldots, B(X_n)
    \end{bmatrix}
    ,
    \qquad
    \text{and}
    \qquad
    d
    :=
    \begin{bmatrix}
      %0_n\\
      \delta \\
      \delta
    \end{bmatrix}
    .
  \end{gather*}

  \end{lemma}
  \begin{proof}
    First we disentangle the constraints.
    To this end, we get
    \begin{gather*}
      \pm\,\sum_{i = 1}^{n} w_i T_i B_k(X_i)
      \ 
      \ge
      \ 
      -n\cdot\delta_k
      \ 
      \pm 
      \ 
      \sum_{i = 1}^{n} B_k(X_i)
      \,,
      \qquad
      k=1,\ldots,K
      \,.
    \end{gather*}
    The corresponding matrix notation is
 \begin{gather*}
    \underset{w_1, \ldots, w_n \in \R}{\mathrm{minimize}}
    \qquad
    \sum_{i = 1}^{n} T_i \cdot f(w_i)
    \\
    \mathbf{Q}w 
    \ 
    \ge
    \ 
    d
    \,,
\end{gather*}
where
\begin{align*}
    T\mathbf{Q}
    &
    \ 
    :=
    \ 
    \begin{bmatrix}
      \mathrm{diag}
      [T_1,\ldots,T_n]
      \\
      \pm
      [T_1,\ldots,T_n]
      \\
      \pm\,T\mathbf{B}(\mathbf{X})
    \end{bmatrix}
    ,
    \\
    T\mathbf{B}(\mathbf{X})
    &
    \ 
    :=
    \ 
    \begin{bmatrix}
      T_1B(X_1), \ldots, T_nB(X_n)
    \end{bmatrix}
    ,
    \\
    d
    &
    \ 
    :=
    \ 
    \begin{bmatrix}
      0_n
      \\
      \pm n
      \\
      -n\cdot\delta 
      \pm\,
      \sum_{i = 1}^{n} B_k(X_i)
    \end{bmatrix}
    \,.
  \end{align*}
The convex conjugate is
\begin{gather*}
  \sum_{T_i=1} T_i f^*(\lambda_i)
  +
  \sum_{T_i=0} 
  \delta_{\left\{ 0 \right\}}(\lambda_i)
  \,,
\end{gather*}
where
\begin{gather*}
  \delta_{\left\{ 0 \right\}}
  (t)
  =
  \begin{cases}
    0,& \text{if}\, t=0,\\
    \infty,& \text{else}\,.
  \end{cases}
\end{gather*}
Note that the $i$-th column of $T\mathbf{Q}$ vanishes if 
$T_i=0$. Likewise, in the columns with $T_i=1$ we can ommit $T_i$.
In the ts chapter, the dual problem features
\begin{gather}
  f^*(A^\top p)
\end{gather}
which by example is here
\begin{gather*}
  \sum_{T_i=1} T_i f^*(T\mathbf{Q}_{\bullet i}^\top\lambda)
  +
  \sum_{T_i=0} 
  \delta_{\left\{ 0 \right\}}
(T\mathbf{Q}_{\bullet i}^\top\lambda)
  =
  \sum_{i=1}^n T_i f^*(\mathbf{Q}_{\bullet i}^\top\lambda)
  \,,
\end{gather*}
where
\begin{align*}
    \mathbf{Q}
    &
    \ 
    :=
    \ 
    \begin{bmatrix}
      \mathbf{I}_n
      \\
      \pm
      \mathrm{1}_n
      \\
      \pm\,\mathbf{B}(\mathbf{X})
    \end{bmatrix}
    \\
    \mathbf{B}(\mathbf{X})
    &
    \ 
    :=
    \ 
    \begin{bmatrix}
      B(X_1), \ldots, B(X_n)
    \end{bmatrix}
\end{align*}


The corresponding dual problem in \cite{Tseng1991} is then
\begin{gather*}
  \underset
  {\lambda_1,\ldots,\lambda_{K}\ge 0}
  {
  \mathrm{maximize}
  }
  \quad
  -
  \sum_{i=1} 
  ^n
  T_i
  \cdot
  f^*
(\mathbf{Q}_{\bullet i}^\top\lambda)
  \ 
  +
  \ 
  \inner{\lambda}{d}
  \,.
\end{gather*}
  $1/n$ the problem remains the same.

  Next we want to remove the non-negativity constraints on $\lambda$.
  To this end we write
  \begin{gather}
    \lambda
    :=
    \begin{bmatrix}
      \rho_1,
      \ldots,
      \rho_n,
      \ 
      \lambda_0^+,
      \lambda_0^-,
      \ 
      \lambda_1^+,
      \ldots,
      \lambda_n^+,
      \ 
      \lambda_1^-,
      \ldots,
      \lambda_n^-
    \end{bmatrix}
    ^\top
    \,.
  \end{gather}
  We expand the objective function $G$ of the dual problem.
  \begin{align*}
    G
    (
    \rho,
    \lambda_0^\pm,
    \lambda^\pm
    )
    =
  &-
  \sum_{i=1} 
  ^n
  T_i
  \cdot
  f^*
  \left( 
\rho_i
\ 
+
\ 
\lambda_0^+
\!
-
\lambda_0^-
\ 
+
\ 
\inner
{B(X_i)}
{\lambda^+ \!- \lambda^-}
  \right)
  \\
  &+
  \ 
  n
  \cdot
  (
\lambda_0^+
\!
-
\lambda_0^-
  )
  \ 
+
  \ 
\inner
{B(X_i)}
{\lambda^+ \!- \lambda^-}
  \ 
-
  \ 
  n
  \cdot
\inner
{\delta}
{\lambda^+ \!+ \lambda^-}
  \end{align*}
  To illustrate the procedure, we show 
  for all $i \in \left\{ 1,\ldots,n \right\}$


\begin{alignat*}{2}
  \text{either}
  &
  &&
  \qquad
      \lambda_i^+ > 0
  \\
  \text{or}
  &
  &&
  \qquad
      \lambda_i^- > 0
  \,.
\end{alignat*}
Assume towards a contradiction that 
there exists
$i \in \left\{ 1,\ldots,n \right\}$
such that
$
      \lambda_0^+ > 0
$
and
$
      \lambda_0^- > 0
$ 
and that 
$\lambda$ is optimal.
Consider
  \begin{gather}
    \tilde{\lambda}
    \ 
    :=
    \ 
    \begin{bmatrix}
      \rho
      ,
      \ 
      \lambda_0^\pm,
      \ 
      \lambda_1^+,
      \ldots,
      \ 
      \lambda_i^+
      \!
      -
      (
      \lambda_i^+
      \!
      \land
      \lambda_i^-
      ),
      \ldots
      \lambda_n^+,
      \ 
      \lambda_1^-,
      \ldots,
      \lambda_i^-
      \!
      -
      (
      \lambda_i^+
      \!
      \land
      \lambda_i^-
      ),
      \ldots,
      \lambda_n^-
    \end{bmatrix}
    ^\top
    \,.
  \end{gather}
  Since 
  $
      \lambda_i^\pm
      -
      (
      \lambda_i^+
      \!
      \land
      \lambda_i^-
      )
      \ge 
      0
  $,
  the perturbed vector $\tilde{\lambda}$ is in the domain of the 
  optimization problem.
  But 
  \begin{align}
  G(\tilde{\lambda})
  -
  G(\lambda)
  \ 
  =
  \ 
  2
  n
  \cdot
  \delta_i
  \cdot
      (
      \lambda_i^+
      \!
      \land
      \lambda_i^-
      )
  \ 
  >
  \ 
  0
  \,,
  \end{align}
  which contradicts the optimality of $\lambda$.
  Likewise we can show
\begin{alignat*}{2}
  \text{either}
  &
  &&
  \qquad
      \lambda_i^+ > 0
  \\
  \text{or}
  &
  &&
  \qquad
      \lambda_i^- > 0
  \,.
\end{alignat*}
But then 
$
\lambda^\pm_i
\ge 0
$
collapses to
$
\lambda_i\in \R
$ 
for 
$i\in \left\{ 0,\ldots,n \right\}$, that is,
$ \lambda_i=\lambda_i^+\!-\lambda_i^- $.
Note that
$ |\lambda_i|=\lambda_i^+\!+\lambda_i^- $.
Likewise we can see, that 
$
\lambda_0
=
\lambda_0^+
-
\lambda_0^-
\in \R
$
removes the constraint on $\lambda_0^\pm$.
Let us take this into account for $G$. We get
  \begin{align*}
    G
    (
    \rho,
    \lambda_0,
    \lambda
    )
    =
  &-
  \sum_{i=1} 
  ^n
  T_i
  \cdot
  f^*
  \left( 
\rho_i
\ 
+
\ 
\lambda_0
\ 
+
\ 
\inner
{B(X_i)}
{\lambda}
  \right)
  \\
  &+
  \ 
  n
  \cdot
\lambda_0
  \ 
+
  \ 
\inner
{B(X_i)}
{\lambda}
  \ 
-
  \ 
  n
  \cdot
\inner
{\delta}
{|\lambda|}
\,.
  \end{align*}
Next we show, that $\rho=0$.
Suppose there exists 
$
i\in \left\{ 1,\ldots, n \right\}
$
such that 
$
\rho_i>0
$
and
$
  T_i
  \cdot
  (f^{'})^{-1}
  \left( 
\rho_i
\ 
+
\ 
\lambda_0
\ 
+
\ 
\inner
{B(X_i)}
{\lambda}
  \right)
  <
  0
$.
It follows
\begin{gather}
  G(0,\lambda_0,\lambda)
  -
  G(\rho_i,\lambda_0,\lambda)
  \ge
  T_i
  \cdot
  (f^{'})^{-1}
  \left( 
\rho_i
\ 
+
\ 
\lambda_0
\ 
+
\ 
\inner
{B(X_i)}
{\lambda}
  \right)
(-\rho_i)
>0,
\end{gather}
which contradicts the optimality of $\lambda$.
Suppose
$
  T_i
  \cdot
  (f^{'})^{-1}
  \left( 
\rho_i
\ 
+
\ 
\lambda_0
\ 
+
\ 
\inner
{B(X_i)}
{\lambda}
  \right)
  >
  0
$.
Then the claim yields to a perturbation argument as in ts.
Thus
To eliminate the constraints for $\rho$ 
we use a similar argument as in the complementary slackness
section of the ts chapter.
Thus we have complementary slackness of 
$\rho_i$ and
$
  T_i
  \cdot
  (f^{'})^{-1}
  \left( 
\rho_i
\ 
+
\ 
\lambda_0
\ 
+
\ 
\inner
{B(X_i)}
{\lambda}
  \right)
$.
But then
every
optimal solution $\lambda$ remains optimal by taking $\rho=0$.

Dividing the optimization problem by $n$ and reversing it, we get

\begin{gather*}
  \underset
  {\lambda_0,\ldots,\lambda_{n}\in \R}
  {
    \mathrm{minimize}
  }
  \quad
  \frac{1}{n}
\sum_{i=1} 
  ^n
  \left[ 
  T_i
  \cdot
  f^*
  \left( 
\lambda_0
\ 
+
\ 
\inner
{B(X_i)}
{\lambda}
  \right)
  -
\inner
{B(X_i)}
{\lambda}
  \right]
\ 
-
\lambda_0
  \ 
+
\inner
{\delta}
{|\lambda|}
  \,.
\end{gather*}
  \end{proof}

  \todo[color=red!40, inline]{What to do about intercept $\lambda_0$?}
%%%%%%%%%%%
%%%% ฮป %%%%
%%%%%%%%%%%
\subsection*{Consistency of the dual variables}
%%%%%%%%%%%
%%%% ฯ %%%%
%%%%%%%%%%%
\subsection*{Consistency of the weights}
%%%%%%%%%%%
%%%% E[Y] %%%%
%%%%%%%%%%%
\subsection*{Consistency of the weighted mean}
\end{proof}
