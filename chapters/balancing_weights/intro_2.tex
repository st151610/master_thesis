Let $X$ be a $d$-dimensional random variable from which we sample
$N\in\mathbb{N}$ independent and identically distributed copies
$X_1,\ldots,X_N$. Furthermore, let $T\in \left\{ 0,1 \right\}$ 
denote the indicator of treatment and let $T_1,\ldots,T_N$ be i.i.d. 
copies.
Let $(Y(0),Y(1))\in[-M,M]^2$ be a real-valued bounded random variables with 
$M\in\R$.
We call $X$ the covariate vector, $T$ the indicator of treatment
and
$
(Y(0),Y(1))
$
the marginal potential outcomes under treatment.
Before we begin the analysis we order the i.i.d. copies, such that
the first $n$ units received treatment and the remaining $N-n$ units 
are in the control group.

We want know more about the (potential) outcome under treatment 
in the population.
But we observe the outcome $Y$ depending on $T$,
that is
\begin{gather}
  Y_i \sim Y|T_i
  \,.
\end{gather}
In the presence of confounders we have
\begin{gather}
  Y|T=1 \nsim Y(1)
  \,.
\end{gather}
We want to create weights
$w_1,\ldots,w_n$ for the treated, such that for large $N$ we get
\begin{gather}
  w_i \cdot Y_i \sim Y(1)
  \,.
\end{gather}
We extend ideas from entropy balancing weights~\cite{Wang2019}.

To be precise in our statements, we introduce some notation.
\subsection*{Basis Functions}
We adopt ideas from \cite{Gyorfi2002}. Another angle would be sieve estimates\cite{Newey1997a} where the number of basis functions can grow slower than $N$.
We need a different notion of
Consistency as is given 
in\cite[Definitien~1.1]{Gyorfi2002}.
\begin{gather}
  \E
  \left[ 
    \int_\mathcal{X}
    \left| 
    \sum_{k=1}^{N} 
    B_k(x)
    \cdot
    m(X_k)
    -
    m(x)
    \right|
    ^2
    \P_X(dx)
  \right]
  \to
  0
  \qquad
  \text{as}
  \ 
  n\to \infty
  \,.
\end{gather}
Nevertheless we leverage (weak) universal consistency 
of regression basis as
in\cite[Definitien~1.3]{Gyorfi2002}.
\begin{theorem}
  Assume
  $\E[m(X)^2]<\infty$
  and the basis function are 
(weak) universal consistency in the sense of 
\cite[Definitien~1.3]{Gyorfi2002}.
Then it holds for all $\varepsilon>0$
\begin{gather}
  \P
  \left[ 
    \left| 
    \sum_{k=1}^{N} 
    B_k(X)
    \cdot
    m(X_k)
    -
    m(X)
    \right|
    \ge
    \varepsilon
  \right]
  \to
  0
  \qquad
  \text{as}
  \ 
  n\to \infty
  \,.
\end{gather}
\end{theorem}
\begin{proof}
  By Markov's inequality it holds
  \begin{align*}
    &
  \P
  \left[ 
    \left| 
    \sum_{k=1}^{N} 
    B_k(X)
    \cdot
    m(X_k)
    -
    m(X)
    \right|
    \ge
    \varepsilon
  \right]
  \\
  &
  \ 
  \le
  \ 
  \frac
  {
  \E
  \left[ 
    \left| 
    \sum_{k=1}^{N} 
    B_k(X)
    \cdot
    m(X_k)
    -
    m(X)
    \right|
    ^2
  \right]
  }
  {\varepsilon^2}
  \\
  &
  \ 
  =
  \ 
  \frac
  {
  \E
  \left[ 
    \E
    \left[ 
    \left| 
    \sum_{k=1}^{N} 
    B_k(X)
    \cdot
    m(X_k)
    -
    m(X)
    \right|
    ^2
    |
    X_1,
    \ldots,
    X_N
    \right]
  \right]
  }
  {\varepsilon^2}
  \\
  &
  \ 
  =
  \ 
  \frac
  {
  \E
  \left[ 
    \int_\mathcal{X}
    \left| 
    \sum_{k=1}^{N} 
    B_k(x)
    \cdot
    m(X_k)
    -
    m(x)
    \right|
    ^2
    \P_X(dx)
  \right]
  }
  {\varepsilon^2}
  \,.
  \end{align*}
  The last equality is due to \cite[(1.2)]{Gyorfi2002}.
  By the weak universal consistency of $B$
  the last expression goes to $0$ as $N\to \infty$.
\end{proof}
Next we partitioning and kernel estimates form \cite[ยง4,ยง5]{Gyorfi2002}.

\subsubsection*{Partitioning Estimates}
We consider a partition
$
  \mathcal{P}_N
  =
  \left\{ 
    A_{N,1}
    ,
    A_{N,2}
    ,
    \ldots
  \right\}
$
of $ \R^d $
and define
$ A_N(x) $ to be the cell of $ \mathcal{P}_N $ containing $x$.
We define $N$ basis functions $B_k$ of the covariates by
\begin{gather*}
  B_k(x)
  :=
  \frac{
  \mathbf{1}_{X_k \in A_N(x)}
  }{
  \sum_{j=1}^{N} 
  \mathbf{1}_{X_j \in A_N(x)}
  }
  \,,
  \qquad
  k=
  1,\ldots,N
  \,.
\end{gather*}
The euclidian norm of the basis functions is bounded above by $1$.
\begin{gather*}
  \norm{B(x)}^2
  =
  \sum_{k=1}^{n} 
  \left( 
  \frac{
  \mathbf{1}_{X_k \in A_n(x)}
  }{
  \sum_{j=1}^{n} 
  \mathbf{1}_{X_j \in A_n(x)}
  }
  \right)
  ^2
  \le
  \sum_{k=1}^{n} 
  \frac{
  \mathbf{1}_{X_k \in A_n(x)}
  }{
  \sum_{j=1}^{n} 
  \mathbf{1}_{X_j \in A_n(x)}
  }
  =1
  \,.
\end{gather*}
Under mild conditions, the basis functions are universally consistent.
\begin{theorem}
  \label{bw:i:bf:pe:th:c}
  If for each sphere $S$ centered at the origin 
  \begin{gather}
    \max
    _
    {
      j\colon
      A_{N,j} 
      \cap
      S
      \neq
      \emptyset
    }
    \mathrm{diam}
    \ 
      A_{N,j} 
      \ 
      \to
      \ 
      0
      \qquad
      \text{for}\ 
      N\to \infty 
  \end{gather}
  and
  \begin{gather}
    \frac
    {
    \#
    \left\{  
      j\colon
      A_{N,j} 
      \cap
      S
      \neq
      \emptyset
    \right\}
    }
    {N}
      \ 
      \to
      \ 
      0
      \qquad
      \text{for}\ 
      N\to \infty 
  \end{gather}
  then the partitioning regression function estimate 
  (definition)
  is
  universally consistent (definition).
\end{theorem}
\begin{proof}
  \cite[Theorem~4.2.]{Gyorfi2002}
\end{proof}
\begin{corollary}
Assume
  $\E[m(X)^2]<\infty$
  and the basis functions $B$ belong to a partitioning estimate.
  Furthermore assume that the conditions of 
  Theorem~\ref{bw:i:bf:pe:th:c} are met.
Then it holds for all $\varepsilon>0$
\begin{gather}
  \P
  \left[ 
    \left| 
    \sum_{k=1}^{N} 
    B_k(X)
    \cdot
    m(X_k)
    -
    m(X)
    \right|
    \ge
    \varepsilon
  \right]
  \to
  0
  \qquad
  \text{as}
  \ 
  n\to \infty
  \,.
\end{gather}
\end{corollary}
\subsubsection*{Kernel Estimates}
Let $K\colon \R^d\to [0,1]$ (bounded kernel) and $h_n>0$ (bandwith).
For examples see \cite[ยง5.1.]{Gyorfi2002}.
We define
\begin{gather}
  B_k(x)
  :=
  \frac
  {
    K \left( \frac{x-X_k}{h_n} \right)
  }
  {
    \sum_{i=1}^{N} 
    K \left( \frac{x-X_i}{h_n} \right)
  }
  \,.
\end{gather}
By the boundedness of the kernel it follows
$\norm{B(x)}\le 1$.
\begin{theorem}
  \label{bw:i:bf:ke:th:c}
  Assume that there are 
  balls
  $S_{0,r}$ of radius $r$ 
  and
  balls
  $S_{0,R}$ of radius $R$ 
  centered at the origin with $0<r\le R$, and a constant $b>0$ such that
  \begin{gather}
    \mathbf{1}_{\left\{ x\in S_{0,R} \right\}}
    \ge
    K(x)
    \ge
    b
    \cdot
    \mathbf{1}_{\left\{ x\in S_{0,r} \right\}}
  \end{gather}
\end{theorem}
(boxed kernel). Then for bandwiths with $h_n\to0$ and
$n\cdot h_n^d\to\infty$ as $n\to \infty$ the kernel estimate is weakly universally consistent.
\begin{corollary}
Assume
  $\E[m(X)^2]<\infty$
  and the basis functions $B$ belong to a kernel estimate.
  Furthermore assume that the conditions of 
  Theorem~\ref{bw:i:bf:ke:th:c} are met.
Then it holds for all $\varepsilon>0$
\begin{gather}
  \P
  \left[ 
    \left| 
    \sum_{k=1}^{N} 
    B_k(X)
    \cdot
    m(X_k)
    -
    m(X)
    \right|
    \ge
    \varepsilon
  \right]
  \to
  0
  \qquad
  \text{as}
  \ 
  n\to \infty
  \,.
\end{gather}
\end{corollary}
\subsection*{Objective Function}
We will consider the sum.
We need $f$ to be strictly convex and its convex conjugate
to be 2nd order smooth and strictly non-decreasing.
\subsubsection*{Negative Entropy}
We define the negative entropy to be
\begin{gather}
  f
  \colon
  [0,\infty)
  \to
  \R
  ,\quad
  w
  \mapsto
  \begin{cases}
    0\quad\text{if}\ w=0,\\
    w\log w\quad
    \text{else}.
  \end{cases}
\end{gather}
It is strictly convex. To compute its Legendre transformation we note, that
\begin{gather}
  (f^{'})^{-1}
  =
  \lambda\mapsto
  e^{\lambda-1}
\end{gather}
Thus
  \begin{align*}
  f^*
  (\lambda)
  &
  \ 
  =
  \ 
  \lambda
    \cdot
    (f^{'})^{-1}(\lambda)
  \ 
    -
  \ 
    f
    \left( 
      (f^{'})^{-1}(\lambda)
    \right)
    \\
  &
  \ 
  =
  \ 
  \lambda
    \cdot
  e^{\lambda-1}
  \ 
    -
  \ 
  e^{\lambda-1}
  \log
  \left( 
  e^{\lambda-1}
  \right)
  \\
  &
  \ 
  =
  \ 
  e^{\lambda-1}
  \,.
  \end{align*}
  Thus $f^*$ is smooth and strictly non-decreasing.


  \subsubsection*{Sample Variance}
We define the sample variance to be
\begin{gather}
  f
  \colon
  \R
  \to
  \R
  ,\quad
  w
  \mapsto
  (w-1/n)^2
\end{gather}
It is strictly convex. To compute its Legendre transformation we note, that
\begin{gather}
  (f^{'})^{-1}
  =
  \lambda\mapsto
  \frac{\lambda}{2}
  +
  \frac{1}{n}
\end{gather}
Thus
  \begin{align*}
  f^*
  (\lambda)
  &
  \ 
  =
  \ 
  \lambda
    \cdot
    \left( 
  \frac{\lambda}{2}
  +
  \frac{1}{n}
    \right)
  \ 
    -
  \ 
    \left( 
    \left( 
  \frac{\lambda}{2}
  +
  \frac{1}{n}
    \right)
    -
    \frac{1}{n}
    \right)
    ^2
    \\
  &
  \ 
  =
  \ 
  \frac{\lambda^2}{4}
  +
  \frac{\lambda}{n}
  \,.
  \end{align*}
  Thus $f^*$ is smooth.
  To eliminate some variables in the optimization problem,
  we need $f^*$ also to be
  strictly non-decreasing. But the sample variance violates this assumption.


\subsection*{Convex Optimization}

We now consider the convex optimization problem for the weights.
We assume the objective function $f$ to be strictly convex, such that its convex conjugate is continuously differentiable. Furthermore we 
assume the bound on the box-constraints $\delta>0$ to converge to $0$ in probability. 
\begin{fproblem}
  \label{bw:1:primal}
\begin{align*}
  %%%% objective %%%%
    &\underset{w_1, \ldots, w_n \in \R}
    {\text{minimize}}
    &&\qquad\qquad
    \sum_{i = 1}^{n} 
    f(w_i)
    &&&
    \\
    %%%% w_i T_i >= 0 %%%%
    &\text{subject to}
    &&\qquad\qquad
    w_i 
    \ge
    0
    &&&
    \qquad
    \text{for all}\ 
    i \in \left\{ 1, \ldots, n \right\}
    \,,
    \\
    %%%% 1/n sum w = 1 %%%%
    & 
    &&\qquad\qquad
    \frac{1}{N}
    \sum_{i=1}^{n} 
    w_i
    =1
    \\
    %%%% box constraints %%%%
    & 
    &&\qquad
    \left| 
      \frac{1}{N} 
      \left( 
      \sum_{i = 1}^{n} 
      w_i
      B_k(X_i)
      -
      \sum_{i=1}^{N} 
      B_k(X_i)
      \right)
    \right|
    \ 
    \le 
    \ 
    \delta_k
    &&&
    \qquad
    \text{for all}\ 
    k \in \left\{ 1, \ldots, N \right\}
    \,.
\end{align*}
\end{fproblem}
To obtain consistency for the weights we will analyse the dual.

