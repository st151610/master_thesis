\begin{theorem}
  Under conditions
  \begin{gather}
    \P
    \left[ 
      \norm{\lambda^\dagger - \lambda^*}
      \le
      C_\P
      C_\tau
      \varepsilon_n
    \right]
    \ge
    1-\tau
    \,,
  \end{gather}
  where 
  $\varepsilon_n$ is the square root of the basis function Learning rate and $C_\tau$ depends on the Concentration Inequality.
  We need bernstein confidence $\sqrt{\log (1/\tau)}$ to preserve minimal Learning rate for $d=1$.
\end{theorem}

\subsection*{Plan of Proof}
The path is similar to that of the previous chapter. 
We will use strong convexity to obtain a quadratic term.
This gives us the flexibility to obtain learning rates.

\begin{theorem}
  \emph{(Bernstein's inequality)}
  Let
  $
  (\Omega,\mathcal{A},\P)
  $ 
  be a probability space, 
  $
  B>0
  $ 
  and
  $
  \sigma>0
  $
  be real numbers,
  and
  $
  n\ge 1
  $
  be an integer.
  Furthermore, 
  let
  $
  X_1,\ldots,X_n
  :
  \Omega
  \to
  \R
  $
  be independent random variables satisfying
  $
  \E[X_i]=0
  $,
  $
  \norm{X_i}_\infty
  \le
  B
  $
  and
  $
  \E[X_i^2]\le \sigma^2
  $
  for all 
  $
  i=1,\ldots,n
  $.
  Then we have
  \begin{gather*}
   \P
    \left[ 
      \,
      \left| 
      \frac{1}{n}
        \sum_{i=1}^{n} 
        X_i
      \right|
      \ 
      \le
      \ 
      \sqrt{
        \,
        \frac{2\,\sigma^2 \log (e/\tau)}{n}
        \,
      }
      \ 
      +
      \
      \frac{2 B \log(e/\tau)}{3n}
      \,
    \right]
    \ 
    \ge
    \ 
    1-\tau
    \qquad
    \text{for all}\ 
    \tau
    >
    0
    \,.
  \end{gather*}
\end{theorem}
\begin{proof}
  See \cite[Theorem~6.12]{Steinwart2008}
  for the one-sided version. 
  The two-sided version, as stated in the above theorem, is an easy consequence. We omit the details.
\end{proof}




\subsection*{Strong Convexity}
For simpler notation we define $B_0(x):=1$.
The Hessian matrix of $g$ as in ??
is
\begin{gather}
  \nabla^2 
  g(\lambda_0^\dagger,\lambda^\dagger)
  =
  \left[ 
    \frac{1}{N}
    \sum_{i=1}^{n} 
    \left( 
    (
    f^{'}
    )
    ^{-1}
    \right)
    ^{'}
    \left( 
      \lambda_0^\dagger
      +
      \inner{B(X_i)}
      {\lambda^\dagger}
    \right)
    B_k(X_i)
    \cdot
    B_l(X_i)
  \right]
  _{0\le k,l\le N}
  \,.
\end{gather}
Since
$
    \left( 
    (
    f^{'}
    )
    ^{-1}
    \right)
    ^{'}
    >0
$
by the strict convexity of $f$, 
$(\lambda_0^\dagger,\lambda^\dagger)\in \Theta$,
where $\Theta$ is compact parameter space,
and the support of $X$ is compact it holds

\begin{gather}
     \left( 
    (
    f^{'}
    )
    ^{-1}
    \right)
    ^{'}
    \left( 
      \lambda_0^\dagger
      +
      \inner{B(X_i)}
      {\lambda^\dagger}
    \right)
    >
    \frac{1}{C}
    \,.
\end{gather}
Consider
\begin{gather}
  \mathbf{B}(X):=
  \begin{bmatrix}
    1 & B_1(X_1) & \cdots & B_N(X_1)\\
    \vdots & \vdots & \ddots & \vdots \\
    1 & B_1(X_n) & \cdots & B_N(X_n)
  \end{bmatrix}
  \in \R^{n\times (N+1)}
\end{gather}

Since $B_k(x)\ge 0$ for all $k$ it holds

\begin{gather}
  \nabla^2 
  g(\lambda_0^\dagger,\lambda^\dagger)
  \succcurlyeq
  \frac{1}{C}
  N^{-1}
  \mathbf{B}(X)^\top
  \mathbf{B}(X)
  \succcurlyeq
  \frac{\lambda_{\min}(
  N^{-1}
  \mathbf{B}(X)^\top
  \mathbf{B}(X)
  )}{C}
  \cdot
  \mathbf{I}
  \succcurlyeq
  \frac{1}
  {
  C \sqrt{\log(1/\tau)}
}
  \cdot
  \mathbf{I}
\end{gather}
with probability greater than $1-\tau$.
Thus $g$ is strongly convex with parameter 
$
\left(
  C \sqrt{\log(1/\tau)}
\right)^{-1}
$
with probability greater than $1-\tau$.
We analyze
$
1-T_i/\pi_i
$.
It holds
$\E[
1-T_i/\pi_i
]=0$.
Furthermore,
\begin{gather}
  \left| 
1
-
\frac{T_i}{\pi_i}
  \right|
  \ 
\le
  \ 
1
\land
\frac{1-\pi_i}{\pi_i}
  \ 
\le
  \ 
1
+
\frac{1-\pi_i}{\pi_i}
  \ 
=
  \ 
\frac{1}{\pi_i}
  \ 
\le
  \ 
\frac{1}{C_\pi}
\qquad
\text{almost surely.}
\end{gather}
Also
\begin{gather}
  \E
  \left[ 
  \left| 
1
-
\frac{T_i}{\pi_i}
  \right|
  ^2
  \right]
\ 
=
\ 
1
-
2
\E
\left[ 
\frac{T_i}{\pi_i}
\right]
+
\E
\left[ 
\frac{T_i}{\pi_i^2}
\right]
\ 
=
\ 
\E
\left[ 
\frac{1}{\pi_i}
\right]
-1
\ 
\le
\ 
\frac{1}{C_\pi}
\,.
\end{gather}
Thus, by Bernstein's inequality, it holds
\begin{gather}
   \P
    \left[ 
      \,
      \left| 
      \frac{1}{n}
        \sum_{i=1}^{n} 
1
-
\frac{T_i}{\pi_i}
      \right|
      \ 
      \lesssim
      \ 
      \sqrt{
        \frac{
        \log (e/\tau)}
      {N}
      }
    \right]
    \ 
    \ge
    \ 
    1-\tau
    \qquad
    \text{for all}\ 
    \tau
    >
    0
    \,.
\end{gather}

Next, we analyze 
\begin{gather}
  \sum_{k=1}^{N} 
  B_k(X_i)\cdot
  f^{'}\left( \frac{1}{\pi_k} \right)
  -
  f^{'}\left( \frac{1}{\pi_i} \right)
  -
  \E[\cdots]
  \,.
\end{gather}
To this end,
\begin{align*}
  &
  \left| 
  \sum_{k=1}^{N} 
  B_k(X_i)\cdot
  f^{'}\left( \frac{1}{\pi_k} \right)
  -
  f^{'}\left( \frac{1}{\pi_i} \right)
  \right|
  \\
  &
  \ 
  \le
  \ 
  \left| 
  \sum_{k=1}^{N} 
  B_k(X_i)\cdot
  \left( 
  f^{'}\left( \frac{1}{\pi_k} \right)
  -
  f^{'}\left( \frac{1}{\pi_i} \right)
  \right)
  \right|
  \ 
  \le
  \ 
  \sum_{k=1}^{N} 
  B_k(X_i)\cdot
  \left| 
  f^{'}\left( \frac{1}{\pi_k} \right)
  -
  f^{'}\left( \frac{1}{\pi_i} \right)
  \right|
  \\
  &
  \ 
  \le
  \ 
  2
  f^{'}\left( \frac{1}{C_\pi} \right)
  \,.
\end{align*}
Also
\begin{align*}
  &
  \E
  \left[ 
  \left| 
  \sum_{k=1}^{N} 
  B_k(X_i)\cdot
  f^{'}\left( \frac{1}{\pi_k} \right)
  -
  f^{'}\left( \frac{1}{\pi_i} \right)
  -
  \E[\cdots]
  \right|^2
\right]^{1/2}
\\
  &
  \ 
\le
  \ 
  \E
  \left[ 
  \left| 
  \sum_{k=1}^{N} 
  B_k(X_i)\cdot
  f^{'}\left( \frac{1}{\pi_k} \right)
  -
  f^{'}\left( \frac{1}{\pi_i} \right)
  \right|^2
\right]^{1/2}
  \ 
+
  \ 
\left| 
\E
\left[ 
  \sum_{k=1}^{N} 
  B_k(X_i)\cdot
  f^{'}\left( \frac{1}{\pi_k} \right)
  -
  f^{'}\left( \frac{1}{\pi_i} \right)
\right]
\right|
\\
  &
  \ 
\lesssim
  \ 
N^{-1/(d+2)}
  \ 
+
  \ 
  2
  f^{'}\left( \frac{1}{C_\pi} \right)
  \,.
\end{align*}
Thus, by Bernstein's inequality, it holds
\begin{gather}
   \P
    \left[ 
      \,
      \left| 
      \frac{1}{N}
  \sum_{i,k=1}^{N} 
  B_k(X_i)\cdot
  f^{'}\left( \frac{1}{\pi_k} \right)
  -
  f^{'}\left( \frac{1}{\pi_i} \right)
  -
  \E[\cdots]
      \right|
      \ 
      \lesssim
      \ 
      \sqrt{
        \frac{
        \log (e/\tau)}
      {N}
      }
    \right]
    \ 
    \ge
    \ 
    1-\tau
    \qquad
    \text{for all}\ 
    \tau
    >
    0
    \,.
\end{gather}

Similar to the analysis of the first and second term we get

\begin{align}
      G
     (
     \lambda^*
      +
      \Delta
      ,
      \lambda^\dagger_0
      +
     \Delta_0
     )
     \ 
     -
     \ 
     G
     (
     \lambda^*,
      \lambda^\dagger_0
     )
     \ge
     \norm{\Delta}
     \left(
     \frac{\norm{\Delta}}
     {
       C \sqrt{\log(1/\tau)}
     }
     -
       \sqrt{
       \frac{\log(1/\tau)}{N}
       }
       -
       N^{-1/(d+2)}
     \right)
\end{align}
with probability greater than $1-\tau$.
Thus for
\begin{gather}
  \norm{\Delta}
  =
  C
  \log(1/\tau)
  N^{-1/(d+2)}
\end{gather}
it holds
\begin{gather}
       G
     (
     \lambda^*
      +
      \Delta
      ,
      \lambda^\dagger_0
      +
     \Delta_0
     )
     \ 
     -
     \ 
     G
     (
     \lambda^*,
      \lambda^\dagger_0
     )
     \ge
0
\end{gather}
with probability greater than $1-\tau$.
We conclude
  \begin{gather}
    \P
    \left[ 
      \norm{\lambda^\dagger - \lambda^*}
    \lesssim
  \log(1/\tau)
  N^{-1/(d+2)}
    \right]
    \ 
    \ge
    \ 
    1-\tau
    \,.
  \end{gather}
