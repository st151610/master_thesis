\subsubsection*{
  Analysis of 
  $
  \E[\max_{i \le n}\norm{A_i}^2]
  $
}
We start from the premise that the fourth moment of 
the random quantities $B_k(X_i)$ and $1/\pi_i$ is uniformly bounded 
in $k$ and $i$.
\begin{assumptions*}
  \begin{enumerate}[label={(\roman*)}]
    \item
  There exists 
  a constant 
  $
  \,
    C_{\!\scriptscriptstyle B}
  \ge
  1
  \,
  $ such that
  \begin{gather*}
  \E\left[
    B_k(X_i)^{4}
    \,
  \right]
  \ 
  \le
  \ 
    C_{\!\scriptscriptstyle B}
    \quad
    \text{
  for all $\,(k,i)\ \in\  \left\{ 1, \ldots, K \right\}\times \left\{ 1, \ldots, n \right\}$.
    }
  \end{gather*}
  \item
  There exists a constant $\,C_\pi \!\ge 1\,$ such that
  \begin{gather*}
  \E \left[ 
    \left(
      \frac{1}{\pi_{i}}
    \right)^{\!4}
    \,
  \right]
  \ 
  \le
  \ 
  C_\pi
  \quad
  \text{
  for all $\,i\ \in\  \left\{ 1, \ldots, n \right\}$.
  }
  \end{gather*}
  \end{enumerate}
\end{assumptions*}
Note, that these assumptions allow for random covariate distributions with unbounded support. The coming example ought to reinforce this observation.
\begin{example*}
  Let us assume a logistic regression model for the propensity score.
  Then there exist coefficients $\,\vartheta \,\in\,  \R^N\,$ and $\,\vartheta_0\,\in\, \R\,$ ($N$ is the number of covariates and $\vartheta_0$ is the intercept of the model) such that
  \begin{align}
    \label{1:5:3a}
    1\,/\,\pi(X)
    &
    \ 
    =
    \ 
    1
    \ 
    +
    \ 
    \exp(
    \,
    \vartheta_0
    \, 
    +
    \,
    \inner{\vartheta}{X}
    \,
    )
    \,,
    \\
    \label{1:5:3b}
  \E \left[ 
    \left(
      \frac{1}{\pi(X)}
    \right)^{\!4}
    \,
  \right]
    &
    \ 
    =
    \ 
  \sum_{j=1}^{4} 
  \,
    \binom{\,4\,}{j}
    \,
    e^{\,j \vartheta_0}
    \,
    M_X(\,j\,\vartheta\,)
    ,
  \end{align}
  where $M_X$ is the momement-generating function of the random vector $X$ (we assume it exists.).
   While the quantity in \eqref{1:5:3a}
   may be unbounded when $X$ has unbounded support, the quantity in \eqref{1:5:3b} remains bounded for reasonable decay rates of the distribution of $X$.
   This may, for example, be the case for the multivariate normal distribution with location parameter $\,\mu\,\in\,\R^N\,$ and covariance matrix 
   $\,\Sigma\,\in\, \mathbb{M}_N\,$. Then the momement-generating function is given by
   \begin{gather*}
     M_{\mathcal{N}(\mu,\,\Sigma)}
     (t)
     \ 
     =
     \ 
     \exp
     \left( 
       \,
       \inner{\,t}{\mu\,}
       +
       \frac{
       \inner{\,t}{\Sigma \,t\,}
       }{2}
       \,
     \right)
     \quad
     \text{for all}\ 
     \,t\ \in\ \R^N
     \,.
   \end{gather*}
   In particular, the expression in~\eqref{1:5:3b} is finite in the event that $X$ follows a multivariate normal distribution.

   Likewise we may give a negative example.
   Let $\,N\,=\,1\,$ and $\,X\,\sim\, \mathrm{Exp}(\lambda)$, that is, only one exponentially distributed covariate is taken into account.
   The momement-generating function is then confined to take arguments $\,t\,<\,\lambda\,$, in which case expression~\eqref{1:5:3b} becomes pointless if $\,4\,\vartheta\,\ge\, \lambda\,$.
\end{example*}

Next, we recall the entity we want to examin.
\begin{gather*}
  A_i
  \ 
  =
  \ 
  \frac{1}{n}
  \,
  \left( 
    \,
    1
    \ 
    -
    \ 
    \frac{T_i}{\pi_i}
    \,
  \right)
  \,
  B(X_i)
  \qquad
  \text{for}
  \ 
  i\in \{1, \ldots, n\}\,.
\end{gather*}
For all
$
  i\in \{1, \ldots, n\}
$
we get the bound
\begin{gather}
  \label{1:5:1}
  \left| 
    \,
    1
    \ 
    -
    \ 
    \frac{T_i}{\pi_i}
    \,
  \right|
  \ 
  \le
  \ 
  \left( 
    \,
  1
  \ 
  \lor
  \ 
  \frac{1-\pi_i}{\pi_i}
  \,
  \right)
  \ 
  \le
  \ 
  1
  \ 
  +
  \ 
  \frac{1-\pi_i}{\pi_i}
  \ 
  =
  \ 
  \frac{1}{\pi_i}
  \,.
\end{gather}
Let
$i^*\in \left\{ 1, \ldots, n \right\}$
be the index where 
$
\norm{A_i}
$
attains its maximum.
\begin{align}
  \label{1:5:2}
  \begin{split}
  \E\left[\max_{i \le n}\norm{A_i}^2\right]
  &
  \ 
  =
  \ 
  \E\left[\norm{A_{i^*}}^2\right]
  \ 
  \le
  \ 
  \E \left[ 
    \left(
      \,
      \frac{
    \norm{B(X_{i^*})}
      }{\pi_{i^*}}
      \,
    \right)
    ^{\!2}
    \,
  \right]
  \ 
  /
  \ 
  n^2
  \\
  &
    \ 
  \le
  \ 
  \E \left[ 
    \left(
      \frac{1}{\pi_{i^*}}
    \right)^{\!4}
    \,
  \right]^{1/2}
  \ 
  \cdot
  \ 
  \E\left[
    \norm{B(X_{i^*})}^4
  \right]^{1/2}
  \,
  /
  \ 
  n^2
  \\
  &
  \ 
  \le
  \ 
  \,
  K
  \,
  /
  \,
  n^2
  \ 
  \cdot
  \ 
  \sqrt{
    \,
    C_\pi
    C_{\!\scriptscriptstyle B}
  }
  \,.
\end{split}
\end{align}
The first inequality comes from the bound \eqref{1:5:1}.
The Cauchy-Schwarz inequality provides the second inequality.
In the last step we use the assumptions made at the start of the section. 
Paying the price of an extra $n$ factor, the 
maximal inequality~\eqref{1:5:2}
yields a bound of the sum, that is,
\begin{gather}
  \sum_{i=1}^{n}
  \,
  \E\left[\norm{A_i}^2\right]
  \ 
  \le
  \ 
  \frac{K}{n}
  \,
  \sqrt{\,C_\pi 
    C_{\!\scriptscriptstyle B}
  }
\end{gather}
\begin{assumption}
  .
\end{assumption}
 
\begin{assumption}
  .
\end{assumption}
\begin{remark}
With Assumption we also get a bound on the fourth moment of 
  $
  \norm{B(X_{i})}
  $. Indeed, by the convexity of 
  $x\mapsto x^2$, the monotonicity and linearity of the expectation it holds   
  \begin{align}
    \begin{split}
  \E[
  \norm{B(X_{i})}^4
  ] 
  &=
  \E
  \left[ 
    \left( 
      \sum_{k=1}^{K}
      B_k^2(X_i)
    \right)^2
  \right]
  =
  K^2
  \E
  \left[ 
    \left( 
      \sum_{k=1}^{K}
      \frac{1}{K}
      B_k^2(X_i)
    \right)^2
  \right]
  \le
  K^2
  \E
  \left[ 
      \sum_{k=1}^{K}
      \frac{1}{K}
      B_k^4(X_i)
  \right]
  \\
  &=
  K
  \sum_{k=1}^{K}
  \E
  \left[ 
      B_k^4(X_i)
  \right]
  \le
  K^2 C_B
  \end{split}
  \end{align}
\end{remark}
\subsubsection*{Analysis of $v(\mathbf{S})$}
We use the fact that 
$
  \norm{A}_2 
  \le
  \norm{A}_F
  =
  \sqrt{
    \sum_{i,j}^{}
    a_{ij}^2
  }
$
It holds
\begin{gather}
  \sum_{i=1}^{n}
  \E[A_iA_i^\top]
  =
  \frac{1}{n^2}
  \sum_{i=1}^{n}
  \E
  \left[ 
    \left( 
      \frac{1-\pi_i}{\pi_i}
    \right)^2
    B(X_i)B(X_i)^\top
  \right]
  =
  \frac{1}{n^2}
  \left( 
    \sum_{i=1}^{n}
    \E
    \left[ 
    \left( 
      \frac{1-\pi_i}{\pi_i}
    \right)^2
    B_k(X_i)B_l(X_i)
    \right]
  \right)
  _{1\le k,l \le K}
  .
\end{gather}
Thus
\begin{align}
  \begin{split}
  &\norm{
  \sum_{i=1}^{n}
  \E[A_iA_i^\top]
  }_2^2
  \\
  &\le
  \norm{
  \sum_{i=1}^{n}
  \E[A_iA_i^\top]
  }_F^2
  =
  \frac{1}{n^4}
  \sum_{k,l=1}^{K}
  \left( 
    \sum_{i=1}^{n}
    \E
    \left[ 
    \left( 
      \frac{1-\pi_i}{\pi_i}
    \right)^2
    B_k(X_i)B_l(X_i)
    \right]
  \right)^2
  \\
  &\le
  \frac{1}{n^4}
  \sum_{k,l=1}^{K}
  \left( 
    \sum_{i=1}^{n}
    \E
    \left[ 
    \left( 
      \frac{1-\pi_i}{\pi_i}
    \right)^4
    \right]
    ^\frac{1}{2}
    \E[
    B_k(X_i)^4
    ]^\frac{1}{4}
    \E[
    B_l(X_i)^4
    ]^\frac{1}{4}
  \right)^2
  \le
  \left(
  \frac{K}{n}
  \right)^2
  C_\pi C_B
  \end{split}
\end{align}
On the other hand
\begin{align}
  \begin{split}
  \norm{
  \sum_{i=1}^{n}
  \E[A^\top_iA_i]
  }_2
  &=
  \sum_{i=1}^{n}
  \E[A^\top_iA_i]
  =
  \frac{1}{n^2}
    \sum_{i=1}^{n}
    \E
    \left[ 
    \left( 
      \frac{1-\pi_i}{\pi_i}
    \right)^2
    \norm{B(X_i)}_2^2
    \right]
    \\
    &\le
  \frac{1}{n^2}
    \sum_{i=1}^{n}
    \E
    \left[ 
    \left( 
      \frac{1-\pi_i}{\pi_i}
    \right)^4
    \right]^\frac{1}{2}
    \norm{B(X_i)}_2^4
    ]^\frac{1}{2}
    \le
    \frac{K}{n}\sqrt{
  C_\pi C_B
    }
  \end{split}
\end{align}
It follows
\begin{gather}
  v(\mathbf{S})
  \le
    \frac{K}{n}\sqrt{
  C_\pi C_B
  }
\end{gather}
Thus we can apply Theorem~\ref{rmineq_rosenthal_pinelis}
to get
\begin{gather}
  \E[\norm{\mathbf{S}}_2]
  \le
  \sqrt{
    2e 
    \frac{K}{n}\sqrt{
  C_\pi C_B
  }
  \log
  (K+1)
  }
  +
  4e
  \frac{\sqrt{K}}{n}
  \sqrt[4]{
  C_\pi C_B
  }
  \log
  (K+1)
  \le
  14
  C_\pi C_B
  \sqrt{
    \frac{K \log(K+1)}{n}
  }
\end{gather}
