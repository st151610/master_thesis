\subsubsection*{
  Analysis of 
  $
  \E[\max_{i \le n}\norm{A_i}^2]
  $
}
We recall the entity we want to examin.
\begin{gather}
  A_i
  \ 
  =
  \ 
  \frac{1}{n}
  \,
  \left( 
    \,
    1
    \ 
    -
    \ 
    \frac{T_i}{\pi_i}
    \,
  \right)
  \,
  B(X_i)
  \qquad
  \text{for}
  \ 
  i\in \{1, \ldots, n\}\,.
\end{gather}
First, note that
\begin{gather}
  \left| 
    \,
    1
    \ 
    -
    \ 
    \frac{T_i}{\pi_i}
    \,
  \right|
  \ 
  \le
  \ 
  \left( 
    \,
  1
  \ 
  \lor
  \ 
  \frac{1-\pi_i}{\pi_i}
  \,
  \right)
  \ 
  \le
  \ 
  1
  \ 
  +
  \ 
  \frac{1-\pi_i}{\pi_i}
  \ 
  =
  \ 
  \frac{1}{\pi_i}
\end{gather}
for all
$
  i\in \{1, \ldots, n\}
$.
The next assumptions bound the random quantities $\pi(X)$ and $B(X)$.

\begin{assumptions*}
  \begin{enumerate}[label={(\roman*)}]
    \item
  \end{enumerate}
\end{assumptions*}


Let
$i^*\in \left\{ 1, \ldots, n \right\}$
be the index where 
$
\norm{A_i}
$
attains its maximum.
\begin{align}
  \begin{split}
  \E\left[\max_{i \le n}\norm{A_i}^2\right]
  &
  \ 
  =
  \ 
  \E\left[\norm{A_{i^*}}^2\right]
  \ 
  \le
  \ 
  n^{-2}
  \,
  \E \left[ 
    \left(
      \,
      \frac{
    \norm{B(X_{i^*})}
      }{\pi_{i^*}}
      \,
    \right)
    ^{\!2}
    \,
  \right]
  \\
  &
    \ 
  \le
  \ 
  n^{-2}
  \,
  \E \left[ 
    \left(
      \frac{1}{\pi_{i^*}}
    \right)^{\!4}
    \,
  \right]^{1/2}
  \ 
  \cdot
  \ 
  \E\left[
    \norm{B(X_{i^*})}^4
  \right]^{1/2}
  \\
  &
  \ 
  \le
  \ 
  n^{-2}
  \,
  K
  \,
  \sqrt{
    \,
    C_\pi
    C_{\!\scriptscriptstyle B}
  }
\end{split}
\end{align}
In the last two steps we applied the Cauchy-Schwarz inequality
and Assumption. Note that
\begin{gather}
  \sum_{i=1}^{n}
  \E[\norm{A_i}^2]\le
  \frac{K}{n}\sqrt{C_\pi C_B}
\end{gather}
\begin{assumption}
  There exists $C_\pi \ge 1$ such that
  $
    \E
    \left[ 
    \left(
      \frac{1-\pi_{i}}{\pi_{i}}
    \right)^4
    \right]
    \le C_\pi
  $
  for all $i\in \left\{ 1, \ldots, n \right\}$
  .
\end{assumption}
 
\begin{remark}
  If we assume a logistic regression model for the propensity score
  it holds for some $\theta \in \R^N$ ($N$ is the number of covariates)
  \begin{gather}
    \label{rmineq_rp_1}
    \frac{1-\pi(X)}{\pi(X)}
    =
    \exp(-\theta X)
    \qquad
    \text{and}
    \qquad
    \E
    \left[ 
    \left(
    \frac{1-\pi(X)}{\pi(X)}
    \right)^4
    \right]
    =
    \E
    [
    \exp(-4\theta X)
    ]
    =
    M_X(-4\theta)
    ,
  \end{gather}
  where $M_X$ is the momement-generating function of $X$.
   While the first quantity in \eqref{rmineq_rp_1}
   may be unbounded when $X$ has unbounded support, the latter quantity in \eqref{rmineq_rp_1} is still bounded for reasonable choices of $X$.
\end{remark}
\begin{assumption}
  There exists $C_B \ge 1$ such that
  $
  \E[
    B_k(X_i)^4
  ]
  \le C_B
  $
  for all $(k,i)\in \left\{ 1, \ldots, K \right\}\times \left\{ 1, \ldots, n \right\}$
  .
\end{assumption}
\begin{remark}
With Assumption we also get a bound on the fourth moment of 
  $
  \norm{B(X_{i})}
  $. Indeed, by the convexity of 
  $x\mapsto x^2$, the monotonicity and linearity of the expectation it holds   
  \begin{align}
    \begin{split}
  \E[
  \norm{B(X_{i})}^4
  ] 
  &=
  \E
  \left[ 
    \left( 
      \sum_{k=1}^{K}
      B_k^2(X_i)
    \right)^2
  \right]
  =
  K^2
  \E
  \left[ 
    \left( 
      \sum_{k=1}^{K}
      \frac{1}{K}
      B_k^2(X_i)
    \right)^2
  \right]
  \le
  K^2
  \E
  \left[ 
      \sum_{k=1}^{K}
      \frac{1}{K}
      B_k^4(X_i)
  \right]
  \\
  &=
  K
  \sum_{k=1}^{K}
  \E
  \left[ 
      B_k^4(X_i)
  \right]
  \le
  K^2 C_B
  \end{split}
  \end{align}
\end{remark}
\subsubsection*{Analysis of $v(\mathbf{S})$}
We use the fact that 
$
  \norm{A}_2 
  \le
  \norm{A}_F
  =
  \sqrt{
    \sum_{i,j}^{}
    a_{ij}^2
  }
$
It holds
\begin{gather}
  \sum_{i=1}^{n}
  \E[A_iA_i^\top]
  =
  \frac{1}{n^2}
  \sum_{i=1}^{n}
  \E
  \left[ 
    \left( 
      \frac{1-\pi_i}{\pi_i}
    \right)^2
    B(X_i)B(X_i)^\top
  \right]
  =
  \frac{1}{n^2}
  \left( 
    \sum_{i=1}^{n}
    \E
    \left[ 
    \left( 
      \frac{1-\pi_i}{\pi_i}
    \right)^2
    B_k(X_i)B_l(X_i)
    \right]
  \right)
  _{1\le k,l \le K}
  .
\end{gather}
Thus
\begin{align}
  \begin{split}
  &\norm{
  \sum_{i=1}^{n}
  \E[A_iA_i^\top]
  }_2^2
  \\
  &\le
  \norm{
  \sum_{i=1}^{n}
  \E[A_iA_i^\top]
  }_F^2
  =
  \frac{1}{n^4}
  \sum_{k,l=1}^{K}
  \left( 
    \sum_{i=1}^{n}
    \E
    \left[ 
    \left( 
      \frac{1-\pi_i}{\pi_i}
    \right)^2
    B_k(X_i)B_l(X_i)
    \right]
  \right)^2
  \\
  &\le
  \frac{1}{n^4}
  \sum_{k,l=1}^{K}
  \left( 
    \sum_{i=1}^{n}
    \E
    \left[ 
    \left( 
      \frac{1-\pi_i}{\pi_i}
    \right)^4
    \right]
    ^\frac{1}{2}
    \E[
    B_k(X_i)^4
    ]^\frac{1}{4}
    \E[
    B_l(X_i)^4
    ]^\frac{1}{4}
  \right)^2
  \le
  \left(
  \frac{K}{n}
  \right)^2
  C_\pi C_B
  \end{split}
\end{align}
On the other hand
\begin{align}
  \begin{split}
  \norm{
  \sum_{i=1}^{n}
  \E[A^\top_iA_i]
  }_2
  &=
  \sum_{i=1}^{n}
  \E[A^\top_iA_i]
  =
  \frac{1}{n^2}
    \sum_{i=1}^{n}
    \E
    \left[ 
    \left( 
      \frac{1-\pi_i}{\pi_i}
    \right)^2
    \norm{B(X_i)}_2^2
    \right]
    \\
    &\le
  \frac{1}{n^2}
    \sum_{i=1}^{n}
    \E
    \left[ 
    \left( 
      \frac{1-\pi_i}{\pi_i}
    \right)^4
    \right]^\frac{1}{2}
    \norm{B(X_i)}_2^4
    ]^\frac{1}{2}
    \le
    \frac{K}{n}\sqrt{
  C_\pi C_B
    }
  \end{split}
\end{align}
It follows
\begin{gather}
  v(\mathbf{S})
  \le
    \frac{K}{n}\sqrt{
  C_\pi C_B
  }
\end{gather}
Thus we can apply Theorem~\ref{rmineq_rosenthal_pinelis}
to get
\begin{gather}
  \E[\norm{\mathbf{S}}_2]
  \le
  \sqrt{
    2e 
    \frac{K}{n}\sqrt{
  C_\pi C_B
  }
  \log
  (K+1)
  }
  +
  4e
  \frac{\sqrt{K}}{n}
  \sqrt[4]{
  C_\pi C_B
  }
  \log
  (K+1)
  \le
  14
  C_\pi C_B
  \sqrt{
    \frac{K \log(K+1)}{n}
  }
\end{gather}
