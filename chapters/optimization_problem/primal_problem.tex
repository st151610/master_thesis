For a 
\begin{align*}
  \text{
(proper) convex function
  }
  \qquad
  \varphi\ \colon\  \R\ \to\ \R\cup \left\{ +\infty \right\}
  \,,
\end{align*}
a vector of $N$ basis functions of the covariates\index{$B$, vector of basis functions of the covariates} 
\begin{gather*}
  B
  \ 
  :=
  \ 
  \left[ 
    B_1,\ldots,B_N
  \right]^\top
  \qquad
  \text{with}\qquad 
  B_k\ \colon\  \R^d\ \to\ \R
  \qquad
  \text{for all}\ k\in \left\{ 1,\ldots,N \right\}
  \,,
\end{gather*}
and a (random) constraints vector
\index{$\delta$,
(random) constraints vector
}
\begin{align*}
\delta\ :=\ [\delta_1,\ldots,\delta_N]^\top
\qquad
  \text{with}\qquad 
  \delta_k\ \colon\  (\Omega,\sigma(D_N),\P)\ \to\ \R
  \qquad
  \text{for all}\ k\in \left\{ 1,\ldots,N \right\}
  \,,
\end{align*}
we consider the following (random) convex optimization problem.
\begin{fproblem}
  \label{bw:1:primal}
\begin{align*}
  %%%% objective %%%%
    &\underset{w_1, \ldots, w_n \in \R}
    {\text{minimize}}
    &&\qquad\qquad
    \sum_{i = 1}^{n} 
    \varphi(w_i)
    &&&
    \\
    %%%% w_i T_i >= 0 %%%%
    &\text{subject to}
    &&\qquad
    w_i 
    \ 
    \ge
    \ 
    0
    &&&
    \text{for all}\ 
    i \in \left\{ 1, \ldots, n \right\}
    \,,
    \\
    %%%% 1/n sum w = 1 %%%%
    & 
    &&
    \qquad
    \frac{1}{N}
    \sum_{i=1}^{n} 
    w_i
    \ 
    =
    \ 
    1
    \\
    %%%% box constraints %%%%
    & 
    &&\qquad
    \left| 
      \frac{1}{N} 
      \left( 
      \sum_{i = 1}^{n} 
      w_i
      \cdot
      B_k(X_i)
      \ 
      -
      \ 
      \sum_{i=1}^{N} 
      B_k(X_i)
      \right)
    \right|
    \ 
    \le 
    \ 
    \delta_k
    &&&
    \text{for all}\ 
    k \in \left\{ 1, \ldots, N \right\}
    \,.
\end{align*}
\end{fproblem}
What is random in Problem~\ref{bw:1:primal}?
First, the dimension of the search space $(w\in\R^n)$ depends on the random variable $n$. 
Thus, we only compute weights for the treated units (the ones with $T_i=1$).
Next, we consider the \textbf{objective function}
\index{$\varphi$, objective function of Problem~\ref{bw:1:primal}}
\begin{gather*}
  w
  \ 
  \mapsto
  \ 
    \sum_{i = 1}^{n} 
    \varphi(w_i)
    \,.
\end{gather*}
The number of summands is random (again $n$).
Note that sometimes we use the equivalent notation
\begin{gather*}
  w
  \ 
  \mapsto
  \ 
    \sum_{i = 1}^{N} 
    T_i
    \cdot
    \varphi(w_i)
    \,,
\end{gather*}
where we set the weights of the untreated (the ones with $T_i=0$) to some arbitrary value in the domain of $\varphi$.
Let's consider the \textbf{constraints}. There is no randomness in the first two constraints.
\begin{gather*}
    w_i 
    \ 
    \ge
    \ 
    0
    \qquad
    \text{for all}\ 
    i \in \left\{ 1, \ldots, n \right\}
    \quad
    \text{and}
    \qquad
    \frac{1}{N}
    \sum_{i=1}^{n} 
    w_i
    \ 
    =
    \ 
    1
    \,.
\end{gather*}
They only make sure that the weights (divided by $N$) form a convex combination.
If, for example, the outcome space $\mathcal{Y}$ is convex, we make sure that a weighted mean estimate of $\E[Y(1)]$ satisfies
\begin{gather*}
  \widehat{Y}(1) 
  \ 
  :=
  \ 
  \frac{1}{N}
  \sum_{i=1}^{n} 
  w_i\cdot Y_i
  \ 
  \in
  \ 
  \mathcal{Y}
\end{gather*}
or that a weighted empirical distribution function satisfies
\begin{gather*}
  \widehat{F}_{Y(1)} 
  \ 
  :=
  \ 
  \frac{1}{N}
  \sum_{i=1}^{n} 
  w_i\cdot \mathbf{1}\left\{ Y_i\le z \right\}
  \ 
  \in
  \ 
  [0,1]
  \,.
\end{gather*}
We talked about the covariate balancing constraint
in the introduction
(we shall call them the \textbf{box constraints}, because of the absolute value).
\begin{gather*}
    \left| 
      \frac{1}{N} 
      \left( 
      \sum_{i = 1}^{n} 
      w_i
      \cdot
      B_k(X_i)
      \ 
      -
      \ 
      \sum_{i=1}^{N} 
      B_k(X_i)
      \right)
    \right|
    \ 
    \le 
    \ 
    \delta_k
    \qquad
    \text{for all}\ 
    k \in \left\{ 1, \ldots, N\right\}
    \,.
\end{gather*}
They are crucial --- we shall discuss their implications as the analysis unfolds.
For now, note that the number of summands in
\begin{gather*}
      \sum_{i = 1}^{n} 
      w_i
      \cdot
      B_k(X_i)
\end{gather*}
is random again, and sometimes we switch to
\begin{gather*}
      \sum_{i = 1}^{N} 
      T_i
      \cdot
      w_i
      \cdot
      B_k(X_i)
      \,.
\end{gather*}
In Section~\ref{sec:basis} we shall specify the vector of basis functions $B$.
Instead of sieve estimators as in \cite{Wang2019}, where the number of basis functions grows slower than $N$ to $\infty$ 
and the basis functions have fixed design,
we shall choose the basis of partitioning estimates as in \cite[ยง4]{Gyorfi2002}.
It depends on the whole data set $D_N$
and therefore has random design.
In Chapter~\ref{ch:consis} we shall see that this choice simplifies the consistency proofs expounded in \cite[Proof of Lemma~2]{Wang2019}.
Finally, note that \cite[Algorithm~1 on page 11]{Wang2019}
is a (random) algorithm to specify $\delta$ based on $D_N$. 
\begin{takeaways}
  In this thesis we analyse the weights of a random constrained convex optimization problem.
  Its distinguishing features are the balancing constraints and the objective function.
  We shall derive and analyse a dual problem that is linked to the initial problem.
\end{takeaways}
