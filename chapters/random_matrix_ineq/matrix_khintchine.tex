\begin{proposition}
  \emph{(Generalized Klein inequality)}
  Let 
  $
    u_1, \ldots, u_n
  $
  and
  $
    v_1, \ldots, v_n
  $
  be real-valued functions on an interval $I$
  of the real line.
  Suppose
  \begin{gather}
    \sum_{k=1}^{n}
    u_k(a)
    v_k(b)
    \ge
    0
    \qquad
    \text{for all}
    \ 
    a,b \in I
    .
  \end{gather}
  Then
  \begin{gather}
    \overline{\mathrm{tr}}
    \left( 
    \sum_{k=1}^{n}
    u_k(\mathbf{A})
    v_k(\mathbf{B})
    \right)
    \ge 0
    \qquad
    \text{for all}
    \ 
    \mathbf{A}, \mathbf{B} \in \mathbb{H}_d(I)
    .
  \end{gather}
\end{proposition}
\begin{proof}
  \emph{\cite[Proposition~3]{Petz1994}}
  Let 
  $
  \mathbf{A} 
    =
    \sum_{i=1}^{d} 
    \lambda_i
    \mathbf{P}_{\bullet i}
    \mathbf{P}_{\bullet i}^*
    \ 
    \text{and}
    \ 
    \mathbf{B} 
    =
    \sum_{j=1}^{d} 
    \mu_j
    \mathbf{Q}_{\bullet j}
    \mathbf{Q}_{\bullet j}^*
  $
  be the orthonormal decompositions of 
  $
    \mathbf{A}
    \ \text{and }
    \mathbf{B}
    .
  $
  Then
  \begin{align}
    \overline{\mathrm{tr}}
    \left( 
    \sum_{k=1}^{n}
    u_k(\mathbf{A})
    v_k(\mathbf{B})
    \right)
    &=
    \sum_{k=1}^{n}
    \sum_{i,j=1}^{d} 
    \overline{\mathrm{tr}}
    \left( 
    u_k(\lambda_i)
    \mathbf{P}_{\bullet i}
    \mathbf{P}_{\bullet i}^*
    v_k(\mu_j)
    \mathbf{Q}_{\bullet j}
    \mathbf{Q}_{\bullet j}^*
    \right)
    \\
    &=
    \sum_{i,j=1}^{d} 
    \overline{\mathrm{tr}}
    \left( 
    \mathbf{P}_{\bullet i}
    \mathbf{P}_{\bullet i}^*
    \mathbf{Q}_{\bullet j}
    \mathbf{Q}_{\bullet j}^*
    \right)
    \sum_{k=1}^{n}
    u_k(\lambda_i)
    v_k(\mu_j)
    \ge
    0
  \end{align}
  by the hypothesis.
  To see that 
  $
  \mathrm{tr}
    \left( 
    \mathbf{P}_{\bullet i}
    \mathbf{P}_{\bullet i}^*
    \mathbf{Q}_{\bullet j}
    \mathbf{Q}_{\bullet j}^*
    \right)
  $
  is non-negative for all $i,j \in \left\{ 1,\ldots,d \right\},$
  we apply a well known extension of von Neumann's trace inequality 
  \cite[Lemma~1]{Ruhe1970},
  namely
  \begin{gather}
    \mathrm{tr}
    (\mathbf{P}\mathbf{Q})
    \ge
    \sum_{i=1}^{d} 
    p_i q_{d-i+1}
    \ge
    0
    \qquad
    \text{for all}
    \ 
    \mathbf{P}
    ,
    \mathbf{Q}
    \in
    \mathbb{H}_d([0,\infty))
    ,
  \end{gather}
  where the eigenvalues
  $
    p_1\ge \ldots \ge p_d
    \ 
    \text{and}
    \ 
    q_1\ge \ldots \ge q_d
  $
  are sorted decreasingly.
\end{proof}




\begin{lemma}
  \emph{(Mean value trace inequality)}
  Let 
  $I$
  be an interval of the real line. Suppose that
  $
    g:
    I \to \R
  $
  is a weakly increasing function and that 
  $
    h:
    I \to \R
  $
  is a function whose derivative $h^{'}$ is convex.
  Then for all matrices 
  $
    \mathbf{A}
    ,
    \mathbf{B}
    \in 
    \mathbb{H}_d(I)
  $
  it holds
  \begin{gather}
    \overline{\mathrm{tr}}
    [
    (
      g(\mathbf{A}) - g(\mathbf{B})
    )
    \cdot
    (
      h(\mathbf{A}) - h(\mathbf{B})
    )
    ]
    \le
    \frac{1}{2}
    \,
    \overline{\mathrm{tr}}
    [
    (
      g(\mathbf{A}) - g(\mathbf{B})
    )
    \cdot
    (
    \mathbf{A}
    -
    \mathbf{B}
    )
    \cdot
    (
    h^{'}(\mathbf{A}) + h^{'}(\mathbf{B})
    )
    ]
    .
  \end{gather}
  When $h^{'}$ is concave, the inequality is reversed. The same result holds for the standard trace.
\end{lemma}
\begin{proof}
  \emph{\cite[Lemma~3.4]{Mackey2014}}
  Fix 
  $
    a,b
    \in
    I
.
  $
  Since 
  $g$
  is
  weakly increasing,
  $
  (
    g(a) - g(b)
  )
  \cdot
  (a-b)
  \ge 0.
  $
  The 
  fundamental theorem of calculus and the convexity of 
  $h^{'}$
  yield the estimate
  \begin{align}
  (
    g(a) - g(b)
  )
  \cdot
  (
    h(a) - h(b)
  )
  &=
  (
    g(a) - g(b)
  )
  \cdot
  (a-b)
  \int_0^1
  h^{'}
  (
    \tau a + (1-\tau) b
  )
  \mathrm{d}\tau
  \\
  &\le
  (
    g(a) - g(b)
  )
  \cdot
  (a-b)
  \int_0^1
  [
  \tau
  h^{'}
  (
     a 
  )
  +
  (1-\tau)
  h^{'}
  (
     b 
  )
  ]
  \mathrm{d}\tau
  \\
  &=
  \frac{1}{2}
  \,
  [
  (
    g(a) - g(b)
  )
  \cdot
  (a-b)
  \cdot  
  (
    h^{'}
    (a)
    +
    h^{'}
    (b)
  )
  ]
  .
  \end{align}
  The inequality is reversed, if $h^{'}$ is concave.
  $
  $
  To apply the Kleins inequality we expand the terms.
  The RHS is
  \begin{align}
    \begin{split}
  &(
    g(a) - g(b)
  )
  \cdot
  (a-b)
  \cdot  
  (
    h^{'}
    (a)
    +
    h^{'}
    (b)
  )
  \\
  &\quad=
  [
g(a)\cdot a \cdot h^{'}(a)
  ]
    +
    [g(a)\cdot a] \cdot h^{'}(b)
  -
  b \cdot[h^{'}(a)\cdot g(a)]
  -
  [b \cdot h^{'}(b)]\cdot g(a)
  \\
  &\qquad
  +
    [
\ \text{the same as above with $a$ and $b$ reversed}\ 
  ]
  (a \rightleftarrows b)
  \\
  \end{split}
  \end{align}
  Taking the trace yields
  \begin{align}
    \begin{split}
&
\mathrm{tr}
  [
    g(\mathbf{A})
    \cdot
    \mathbf{A}
    \cdot
    (
    h^{'}(\mathbf{A})
    +
    h^{'}(\mathbf{B})
    )
  ]
  -
  \mathrm{tr}
  [
    \mathbf{B}
    \cdot
    (
    h^{'}(\mathbf{A})
    +
    h^{'}(\mathbf{B})
    )
    \cdot
    g(\mathbf{A})
  ]
  +
  (\mathbf{A} \rightleftarrows \mathbf{B})
  \\
  &\quad=
\mathrm{tr}
  [
    g(\mathbf{A})
    \cdot
    \mathbf{A}
    \cdot
    (
    h^{'}(\mathbf{A})
    +
    h^{'}(\mathbf{B})
    )
  ]
  -
  \mathrm{tr}
  [
    g(\mathbf{A})
    \cdot
    \mathbf{B}
    \cdot
    (
    h^{'}(\mathbf{A})
    +
    h^{'}(\mathbf{B})
    )
  ]
  +
  (\mathbf{A} \rightleftarrows \mathbf{B})
  \\
  &\quad=
  \mathrm{tr}
  [
    g(\mathbf{A})
    \cdot
    (
    \mathbf{A}
    -
    \mathbf{B}
    )
    \cdot
    (
    h^{'}(\mathbf{A})
    +
    h^{'}(\mathbf{B})
    )
  ]
  +
  (\mathbf{A} \rightleftarrows \mathbf{B})
  \\
  &\quad=
  \mathrm{tr}
  [
  (
    g(\mathbf{A})
    -
    g(\mathbf{B})
  )
    \cdot
    (
    \mathbf{A}
    -
    \mathbf{B}
    )
    \cdot
    (
    h^{'}(\mathbf{A})
    +
    h^{'}(\mathbf{B})
    )
  ].
    \end{split}
  \end{align}
  On the LHS we have only products of two factors which commute under the trace operation. Thus we may use the same expression as in the scalar case without further calculations.
  The result follows immediately from the Klein inequality.
\end{proof}




\begin{proposition}
  \emph{(Hölder inequality for trace)}
  Let 
  $p$ and $q$
  be Hölder conjugate indices.
  Then
  \begin{gather}
    \mathrm{tr}
    (
    \mathbf{BC}
    )
    \le
    \norm{\mathbf{B}}_p
    \norm{\mathbf{C}}_q
    \qquad
    \text{for all}
    \ 
    \mathbf{B}
    ,
    \mathbf{C}
    \in 
    \mathbb{M}_d
    .
  \end{gather}
\end{proposition}
\begin{proof}
  \cite[Corollary~IV.2.6]{Bhatia1997}
\end{proof}


We are now ready to prove the auxiliary theorem.

\begin{theorem}
  \emph{(Matrix BDG inequality)}
  Let
  $
    p = 1
    \ 
    \text{or}\ 
    p \ge 3/2
    .
  $
  Suppose that 
  $
  (
    \mathbf{X}
   , 
   \mathbf{X}^{'}
  )
  $
  is a matrix Stein pair where
  $
   \E
   [ 
    \norm{\mathbf{X}}
    _{2p}^{2p}
   ]
   <
   \infty
   .
  $
  Then
  \begin{gather}
   \E
   [ 
    \norm{\mathbf{X}}
    _{2p}^{2p}
   ]
   ^{1/(2p)}
   \le
   \sqrt{2p - 1}
   \ 
   \E
   [ 
    \norm{\mathbf{\Delta_X}}
    _{p}^{p}
   ]
   ^{1/(2p)}
   ,
  \end{gather}
  where 
  $
    \mathbf{\Delta_X}
  $
  is the conditional variance
  .
\end{theorem}
\begin{proof}
  \emph{\cite[§7.3]{Mackey2014}}
  Suppose that
  $
  (
    \mathbf{X}
    ,
    \mathbf{X}^{'}
  )
  $
  is a matrix Stein pair with scale factor $\alpha.$
  First, observe that the result for $p=1$ already follows from 
  $
    \E[\mathbf{\Delta_X}]
    =
    \E[\mathbf{X}^2]
    .
  $
  Therefore we may assume that $p\ge 3/2.$
  We introduce the notation for the quantity of interest,
  \begin{gather}
    E
    :=
    \E[
    \norm{\mathbf{X}}_{2p}^{2p}
    ]
    =
    \E
    [
    \mathrm{tr}
    (
    \left| \mathbf{X} \right|^{2p}
    )
    ].
  \end{gather}
  We rewrite the expression for $E$
  by peeling off a copy of $\left| \mathbf{X} \right|.$
  This yields
  \begin{gather}
    E
    =
    \E
    [
    \mathrm{tr}
    (
    \left| \mathbf{X} \right|
    \cdot
    \left| \mathbf{X} \right|^{2p-1}
    )
    ]
    =
    \E
    [
    \mathrm{tr}
    (
    \mathbf{X}
    \cdot
    \mathrm{sgn}
    (
      \mathbf{X}
    )
    \cdot
    \left| \mathbf{X} \right|^{2p-1}
    )
    ]
    .
  \end{gather}
  Apply the method of exchangable pairs with 
  $
    \mathbf{F}
    (\mathbf{X})
    =
    \mathrm{sgn}
    (\mathbf{X})
    \cdot
    \left| \mathbf{X} \right|^{2p-1}
  $
  to reach
  \begin{gather}
    E
    =
    \frac{1}{2\alpha}
    \E
    [
    \mathrm{tr}
    (
    (
    \mathbf{X}
    -
    \mathbf{X}^{'}
    )
    \cdot
    (
    \mathrm{sgn}
    (
      \mathbf{X}
    )
    \cdot
    \left| \mathbf{X} \right|^{2p-1}
    -
    \mathrm{sgn}
    (
    \mathbf{X}^{'}
    )
    \cdot
    | \mathbf{X}^{'} |^{2p-1}
    )
    )
    ]
  \end{gather}



  Apply method of exchangeable pairs,
  generalized Klein inequality,
  trace Hölder
\end{proof}




\begin{ftheorem}
  \emph{\cite[Corollary~7.3]{Mackey2014}}
  Suppose that
  $
    p = 1
    \ 
    \text{or}\ 
    p \ge 3/2
    .
  $
  Consider a finite sequence
  $
    (\mathbf{Y}_k)_{k\ge 1}
  $
  of independent, random, Hermitian matrices 
  and a deterministic sequence
  $
    (\mathbf{A}_k)_{k\ge 1}
  $
  for which
  \begin{gather}
    \E[\mathbf{Y}_k]
    =
    0
    \quad 
    \text{and}
    \quad
    \mathbf{Y}_k^2
    \preccurlyeq
    \mathbf{A}_k^2
    \qquad
    \text{almost surely for all}\ 
    k \ge 1.
  \end{gather}
  Then
  \begin{gather}
      \E
      \left[
        \norm{
          \sum_{k\ge 1}
            \mathbf{Y}_k
        }
        _{2p}
        ^{2p}
      \right]
      ^{1/(2p)}
      \le
      \sqrt{
        p - \frac{1}{2}
      }
      \,
      \norm{
        \left( 
          \sum_{k\ge 1}
          (
            \mathbf{A}_k^2
            + 
            \E[
              \mathbf{Y}_k^2
            ]
          )
        \right)
        ^{1/2}
        }
      _{2p}
      .
  \end{gather}
  In particular, when 
  $
    (\xi_k)_{k\ge 1}
  $
  is an independent sequence of Rademacher random variables,
  \begin{gather}
      \E
      \left[
        \norm{
          \sum_{k\ge 1}
            \xi_k
            \mathbf{A}_k
        }
        _{2p}
        ^{2p}
      \right]
      ^{1/(2p)}
      \le
      \sqrt{
        2p - 1
        }
      \,
      \norm{
        \left( 
          \sum_{k\ge 1}
            \mathbf{A}_k^2
        \right)
        ^{1/2}
        }
      _{2p}
      .
  \end{gather}
\end{ftheorem}

\begin{ftheorem}
  Assume $n\ge 3$
  \begin{enumerate}[label={(\roman*)}]
    \item
  Suppose that
  $
    p \ge 1
    ,
  $
  and fix
  $
    r
    \ge
    p
    \lor
    2\log(n)
    .
  $
  Consider a finite sequence
  $
    (\mathbf{S}_k)_{k\ge 1}
  $
  of independent, random, positive-semidefinite matrices 
  with dimension 
  $
    n\times n.
  $
  Then
  \begin{gather}
      \E
      \left[
        \norm{
          \sum_{k\ge 1}
            \mathbf{S}_k
        }
        ^{p}
      \right]
      ^{1/p}
      \le
      \left[ 
        \norm{
          \sum_{k\ge 1}
          \E
          [
            \mathbf{S}_k
          ]
        }^{1/2}
        +
        2
        \sqrt{e r}
        \E
        [
          \max_{k\ge 1}
          \norm{
            \mathbf{S}_k
          }
          ^p
        ]
        ^{1/(2p)}
      \right]
      ^2
      .
  \end{gather}
    \item
  Suppose that
  $
    p \ge 2
    ,
  $
  and fix
  $
    r
    \ge
    p
    \lor
    2\log(n)
    .
  $
  Consider a finite sequence
  $
    (\mathbf{Y}_k)_{k\ge 1}
  $
  of independent, symmetric, random, self-adjoint matrices 
  with dimension 
  $
    n\times n.
  $
  Then
  \begin{gather}
      \E
      \left[
        \norm{
          \sum_{k\ge 1}
            \mathbf{Y}_k
        }
        ^{p}
      \right]
      ^{1/p}
      \le
        \sqrt{e r}
        \norm{
          \left( 
          \sum_{k\ge 1}
          \E
          [
            \mathbf{Y}_k^2
          ]
          \right)
          ^{1/2}
        }
        +
        2
        er
        \E
        [
          \max_{k\ge 1}
          \norm{
            \mathbf{S}_k
          }
          ^p
        ]
        ^{1/p}
      .
  \end{gather}

  \end{enumerate}
\end{ftheorem}
\begin{takeaways}
  This is so amazing
  \lipsum[2]
\end{takeaways}
