  The \textbf{trace} of a square matrix, denoted by $\mathrm{tr},$
  is the sum of its diagonal entries, i.e. 
  $
    \mathrm{tr}(\mathbf{B})
    =
    \sum_{j=1}^{d}b_{jj}
    \quad 
    \text{for}\ 
    \mathbf{B} \in \mathbb{M}_d.
  $
  The trace is unitarily invariant, i.e.
  $
    \mathrm{tr}(\mathbf{B})
    =
    \mathrm{tr}(\mathbf{Q}\mathbf{B}\mathbf{Q}^*)
    \quad 
    \text{for all}
    \ 
    \mathbf{B}\in \mathbb{M}_d
    \ 
    \text{for all unitary}\ 
    \mathbf{Q} \in \mathbb{M}_d.
  $
  In particular, the existence of an eigenvalue value decomposition shows 
  that the trace of a Hermitian matrix equals the sum of its  eigenvalues.
  Let
  $
  f: I\to \R
  $
  where 
  $I\subseteq\R$ 
  is an interval.
  Consider a matrix 
  $\mathbf{A}\in \mathbb{H}_d$
  whose eigenvalues are contained in $I.$
  We define the matrix 
  $
    f(\mathbf{A})\in \mathbb{H}_d
  $
  using an eigenvalue decomposition of $\mathbf{A}:$
  \begin{gather}
    f(\mathbf{A})
    =
    \mathbf{Q}
    \begin{bmatrix}
      f(\lambda_1) &&\\
                   &\ddots&\\
                   && f(\lambda_d)
    \end{bmatrix}
    \mathbf{Q}^*
    \qquad
    \text{where}
    \qquad
    \mathbf{A}
    =
    \mathbf{Q}
    \begin{bmatrix}
      \lambda_1 &&\\
                   &\ddots&\\
                   && \lambda_d
    \end{bmatrix}
    \mathbf{Q}^*
    =
    \sum_{i=1}^{d} 
    \lambda_i
    \mathbf{Q}_{\bullet i}
    \mathbf{Q}_{\bullet i}^*
    .
  \end{gather}
  The definition of $f(\mathbf{A})$ does not depend on which 
  eigenvalue decomposition we choose.
  Any matrix function that arises in this fashion is called a \textbf{standard matrix function}.

  For each $p\ge 1$
  the \textbf{Schatten $p$-norm} is defined as 
  $
    \norm{\mathbf{B}}_p
    :=
    (
    \mathrm{tr}(\left| \mathbf{B} \right|^p)
    )
    ^{1/p}
    \ 
    \text{for}
    \ 
    \mathbf{B} \in \mathbb{M}_d.
  $
  In this setting,
  $
  \left| \mathbf{B} \right|
  :=
  (
    \mathbf{B}^*
    \mathbf{B}
    )^{1/2}
    .
  $
  The \textbf{spectral norm}
  of an Hermitian matrix $\mathbf{A}$
  is defined by the relation
  $
    \norm{\mathbf{A}}
    :=
    \lambda_{\mathrm{max}}(\mathbf{A})
    \lor
    (
    -\lambda_{\mathrm{min}}(\mathbf{B})
    )
    .
  $
  For a general matrix $\mathbf{B},$
  the spectral norm is defined to be the largest singular value:
  $
    \norm{\mathbf{B}}
    :=
    \sigma_1(\mathbf{B})
    .
  $
  The Schatten $p$-norm dominates the spectral norm for all $p\ge 1.$




   
