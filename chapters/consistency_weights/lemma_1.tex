\begin{lemma}
  \label{lem:link_conv_p}
  Let $m$,$N\in\mathbb{N}$ and let 
  $
  g \,:\, \R^N_{\ge 0}\times\R^m \to \overline{\R}
  $ 
  be a continuous and proper convex function.
  Consider 
  \begin{gather*}
    \tilde{S}(\varepsilon)
    :=
    \left\{ 
      (\Delta_\rho,\Delta)
      \in
      \R^N_{\ge 0}\times\R^m
      \ 
      \colon
      \ 
      \norm{
      (\Delta_\rho,\Delta)
      }_2
      =
      \varepsilon
    \right\}
    \qquad
    \text{for}
    \ 
    \varepsilon>0
    \,.
  \end{gather*}
Then 
  for all $y \in \R^m$ and $\varepsilon>0$ 
    \begin{gather}
      \label{696}
      \inf 
      \left\{ 
        g(\Delta_\rho,y+\Delta)
        -
        g(0,y)
      \ 
        \colon
      \ 
      (\Delta_\rho,\Delta)
      \in
    \tilde{S}(\varepsilon)
      \right\}
      \ 
      \ge
      \ 
      0
    \end{gather}
    implies
    the existence of  
    a global minimum
    \begin{gather*}
    (y^*_\rho,y^*)
    \in \,\R^N_{\ge 0}\times\R^m
    \quad
    \text{of $g$ such that}\quad
      \norm{
    (y^*_\rho,y^*)
      - (0,y)}_2 \le \varepsilon
      \,.
    \end{gather*}
\end{lemma}
\begin{proof}
  We start by defining the convex set
  \begin{gather*}
    \tilde{B}(\varepsilon)
    \ 
    :=
    \ 
    \left\{ 
      (\Delta_\rho,\Delta)
      \in
      \R^N_{\ge 0}\times\R^m
      \ 
      \colon
      \ 
      \norm{
      (\Delta_\rho,\Delta)
      }_2
      \le
      \varepsilon
    \right\}
    \qquad
    \text{for}
    \ 
    \varepsilon>0
    \,.
  \end{gather*}
  Then the translation 
  $
  (0,y)
  +
    \tilde{B}(\varepsilon)
  $
  is also convex.
  Assume towards a contradiction that it holds \eqref{696}
  and that there exists 
  \begin{gather}
    \label{698}
  (x^*_\rho,x^*)
  \ 
\in
  \ 
\R^N_{\ge 0}\times\R^m\setminus 
\left(
  (0,y)
  +
    \tilde{B}(\varepsilon)
\right)
\quad
\text{such that}\quad
g
  (x^*_\rho,x^*)
  \ 
  <
  \ 
  g(0,y)
  \,.
  \end{gather}
  Since 
  $
  (0,y)
  +
    \tilde{B}(\varepsilon)
  $
  is bounded, the line segment between 
  $
  (x^*_\rho,x^*)
  $
  and
  $
  (0,y)
  $
  crosses its boundary. The boundary consists of two disjoint sets
  \begin{gather*}
    S_0(\varepsilon)
    :=
    \left\{ 
      (0,y+\Delta)
      \ 
      \colon
      \ 
      \Delta\in\R^m\ 
      \text{and}\ 
      \norm{\Delta}_2<\varepsilon
    \right\}
    \qquad
    \text{and}
    \qquad
    \tilde{S}(\varepsilon)
    \,.
  \end{gather*}
  Clearly, if the line segment does not cross $\tilde{S}(\varepsilon)$ it leaves $\R^N_{\ge 0}\times\R^m$.
  But this is not possible.
  Thus, there exists $(\Delta_\rho,\Delta)\in \tilde{S}(\varepsilon)$ and $\theta\in(0,1)$ such that 
  \begin{gather}
    \label{697}
    \theta 
    \cdot
  (x^*_\rho,x^*)
  \ 
  +
  \ 
  (
  1
  -
\theta
  )
  \cdot
  (0,y)
  \ 
  =
  \ 
  (\Delta_\rho,y+\Delta)
  \,.
  \end{gather}
  It follows
 \begin{align*}
      \begin{split}
      g(0,y)
      \ 
      \le
      \ 
      g
  (\Delta_\rho,y+\Delta)
&
      \ 
      =
      \ 
      g
      \left( 
    \theta 
    \cdot
  (x^*_\rho,x^*)
  \ 
  +
  \ 
  (
  1
  -
\theta
  )
  \cdot
  (0,y)
      \right)
      \\
&
      \ 
      \le
      \ 
    \theta 
    \cdot
      g
  (x^*_\rho,x^*)
  \ 
  +
  \ 
  (
  1
  -
\theta
  )
  \cdot
  g
  (0,y)
  \ 
      <
  \ 
      g(0,y)
  \,    ,
      \end{split}
    \end{align*}
    which is a contradiction.
    The first inequality is due to \eqref{696}, the equality is due to \eqref{697}, the second inequality is due to the convexity of $g$,
    and the strict inequality is due to assumption \eqref{698}.
    Thus, all values outside 
    $(0,y)+\tilde{B}(\varepsilon)$
    are greater or equal $(0,y)$.
    Since 
    $(0,y)+\tilde{B}(\varepsilon)$
    is also compact, the continuous function $g$ has a local minimum
    \begin{gather*}
      (y^*_\rho,y^*)\in
    (0,y)+\tilde{B}(\varepsilon)
    \,.
    \end{gather*}
    But then it holds
    \begin{gather*}
      g
      (y^*_\rho,y^*)
      \ 
      \le
      \ 
      g(0,y)
      \ 
      \le
      \ 
      g
      (x_\rho,x)
      \qquad
      \text{for all}
      \qquad 
      (x_\rho,x)
      \in
\R^N_{\ge 0}\times\R^m\setminus 
\left(
  (0,y)
  +
    \tilde{B}(\varepsilon)
\right)
    \end{gather*}
    and
    \begin{gather*}
      g
      (y^*_\rho,y^*)
      \ 
      \le
      \ 
      g
      (z_\rho,z)
      \qquad
      \text{for all}
      \qquad 
      (z_\rho,z)
      \in
  (0,y)
  +
    \tilde{B}(\varepsilon)
    \,.
    \end{gather*}
    Thus,
    $
      (y^*_\rho,y^*)
    $
    is also a global minimum in
    $
\R^N_{\ge 0}\times\R^m
    $.
Since
    $
      (y^*_\rho,y^*)
      \in
  (0,y)
  +
    \tilde{B}(\varepsilon)
    $
    there exists
    $
    (\Delta_\rho,\Delta)\in
    \tilde{B}(\varepsilon)
    $
    such that 
    \begin{gather*}
      (y^*_\rho,y^*)
      =
      (\Delta_\rho,y+\Delta)
      \qquad
      \text{for some}\ 
      (\Delta_\rho,\Delta)
      \in
      \tilde{B}(\varepsilon)
      \,.
    \end{gather*}
  Thus
  \begin{gather*}
    \norm{
      (y^*_\rho,y^*)
      -
      (0,y)
    }_2
    =
    \norm{
      (\Delta_\rho,\Delta)
    }_2
    \le \varepsilon
    \,.
  \end{gather*}
  This finish the proof.
\end{proof}
\begin{remark}
  I learned of the high-level idea from \cite[page 22]{Wang2019}.
  I adapted it to the needs of the subsequent analysis and provided the details by myself.
  Note, that the hint in \cite[page 22]{Wang2019}
  uses strict inequality in the statement.
  I found out that this can be relaxed.
  It is crucial to my further approach that this holds (only) with inequality, because I use measurability properties to obtain convergence.
\end{remark}
