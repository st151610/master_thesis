On the basis of the (random) objective function $G$ of Problem~\ref{dual} (see Definition~\ref{def:rand_obj_f}) we define, for $\varepsilon>0$, an auxiliary function
   \begin{align*}
     \underline{
     \Delta G^*
     _\varepsilon
     }
     \colon
     &
     \left( \Omega,\sigma(D_N),\P \right)
     \ 
     \to
     \ 
     \overline{\R}
     \\
     &
     \omega
     \ 
     \mapsto
     \ 
   \inf
   \left\{ 
 G
   \left( 
     \omega,
     \left( 
\Delta_\rho,\Delta_0,\lambda^*(\omega)+\Delta
     \right)
   \right)
   -
   G
   \left(
     \omega,
     \left( 
0_N,0,\lambda^*(\omega)
     \right)
   \right)
   \ 
   \colon
   \ 
   \norm{
\Delta_\rho,\Delta_0,\Delta
   }_2
   =\varepsilon
   \right\}
   \end{align*}
\begin{lemma}
\label{lem:meas_inf_G}
  For all $\varepsilon>0$ the function
  $
     \underline{
     \Delta G^*
     _\varepsilon
     }
  $
  is
  $
  \left( 
  \sigma(D_N),\mathcal{B}(\overline{\R})
  \right)
  $-measurable.
\end{lemma}
\begin{proof}
  Let $\varepsilon>0$.
  By Lemma~\ref{lem:caratheo_G}, the function
\begin{align*}
     \Delta G
     _\varepsilon
     \colon
     &
     \Omega
     \times
     \left( 
     \R^N
     \times
    \left( 
    \R^N_{\ge 0}\times\R\times\R^N
    \right)
     \right)
     \ 
     \to
     \ 
     \overline{\R}
     \\
     &
     \left( 
     \omega
     ,
     \left( 
     \lambda,
     \left( 
\Delta_\rho\Delta_0\Delta
     \right)
     \right)
     \right)
     \ 
     \mapsto
     \ 
 G
   \left( 
     \omega,
     \left( 
\Delta_\rho,\Delta_0,\lambda+\Delta
     \right)
   \right)
   -
   G
   \left(
     \omega,
     \left( 
0_N,0,\lambda
     \right)
   \right)
   \end{align*}
is Caratheodory.
Since
  $
  \left\{
   \norm{
\Delta_\rho\\\Delta_0\\\Delta
   }_2
   =\varepsilon
  \right\}
  $
  is compact in 
  $
    \R^N_{\ge 0}\times\R\times\R^N
    $,
  the function
\begin{align*}
  \underline{
     \Delta G
     _\varepsilon
  }
     \colon
     &
     \Omega
     \times
     \R^N
     \ 
     \to
     \ 
     \overline{\R}
     \\
     &
     \left( 
     \omega
     ,
     \lambda
     \right)
     \ 
     \mapsto
     \ 
   \inf
    \left\{ 
 G
   \left( 
     \omega,
     \left( 
\Delta_\rho,\Delta_0,\lambda+\Delta
     \right)
   \right)
   -
   G
   \left(
     \omega,
     \left( 
0_N,0,\lambda
     \right)
   \right)
   \ 
   \colon
   \ 
   \norm{
\Delta_\rho\Delta_0\Delta
   }_2
   =\varepsilon
    \right\}
   \end{align*}
is Caratheodory.
Since 
$
\lambda^*
$
is 
$
\left( 
\sigma
(D_N)
,
\mathcal{B}(\R^N)
\right)
$-
measurable it follows the statement.
\end{proof}
 \begin{lemma}
   \label{lem:conv_dG}
   \label{bw:cd:lem2}
   It holds
   for all $\varepsilon>0$
\begin{gather*}
   \P
   \left[ 
     \underline{
     \Delta G^*
     _\varepsilon
     }
     \ge 
     0
   \right]
   \ 
   \to
   \ 
   1
   \qquad
   \text{for}
   \ 
   N\to\infty
   \,.
\end{gather*}
 \end{lemma}
 \begin{proof}
   Let $\varepsilon>0$
   and 
   $\norm{
   \Delta_\rho,\Delta_0,\Delta
   }_2=\varepsilon$.
   We show
\begin{gather*}
   \P
   \left[ 
     \underline{
     \Delta G^*
     _\varepsilon
     }
     \ge 
     -\tilde{\varepsilon}
   \right]
   \ 
   \to
   \ 
   1
   \qquad
   \ 
   \text{for}
   \ 
   N\to\infty
   \ 
   \text{for all}\ 
   \tilde{\varepsilon}>0
   \,.
\end{gather*}
Then the result follows from the measurability of 
$
     \underline{
     \Delta G^*
     _\varepsilon
     }
$
(see Lemma~\ref{lem:meas_inf_G}).
To this end, note, that
\begin{gather*}
  G(\rho,\lambda_0,\lambda)
  \ 
  =
  \ 
  g(\rho,\lambda_0,\lambda)
  \ 
  +
  \ 
  \inner{\delta}{|\lambda|}
  \qquad
  \text{for all}\ 
  (\rho,\lambda_0,\lambda)
  \in
  \R^N_{\ge 0}
  \times
  \R
  \times
  \R^{N}
  \,,
\end{gather*}
with
\begin{gather*}
  g
  \ 
  :=
  \ 
  (\rho,\lambda_0,\lambda)
  \ 
  \mapsto
  \ 
     \frac{1}{N}
     \left( 
\sum_{i=1} 
  ^N
  T_i
  \cdot
  \varphi^*
  \!
  \left( 
    \rho_i
    +
\lambda_0
+
\inner
{B(X_i)}
{
\lambda
}
  \right)
  \ 
  -
\ 
\lambda_0
-
\inner
{B(X_i)}
{
\lambda
}
     \right)
  \,.
\end{gather*}
Since $\varphi^*$ is continuously differentiable by Lemma~\ref{1165}
(it is always convex),
$g$ is a continuously differentiable convex function with gradient
\begin{align*}
  &
  (\rho,\lambda_0,\lambda)
  \\
  &
  \ 
  \mapsto
  \ 
     \frac{1}{N}
     \left( 
\sum_{i=1} 
  ^N
  T_i
  \cdot
  (
  \varphi^{'}
  )
  ^{-1}
  \!
  \left( 
    \rho_i
    +
\lambda_0
+
\inner
{B(X_i)}
{
\lambda
}
  \right)
  \left[ 
    e_i^\top,1,B(X_i)^\top
  \right]^\top
  \ 
  -
  \ 
  \left[ 
    0_N^\top,1,B(X_i)^\top
  \right]^\top
     \right)
  \,.
\end{align*}
Thus, by \eqref{cv:primer:mvthe},
it holds
\begin{align}
\label{99909}
\begin{split}
  &
  G
   \left( 
\Delta_\rho,\Delta_0,\lambda^*+\Delta
   \right)
   \ 
   -
   \ 
   G
   \left(
0_N,0,\lambda^*
   \right)
   \\
   &
   \ 
   \ge
   \ 
   \frac{1}{N}
  \left( 
\sum_{i=1} 
  ^N
  T_i
  \cdot
  (
  \varphi^{'}
  )
  ^{-1}
  \!
  \left( 
\inner
{B(X_i)}
{
\lambda^*
}
  \right)
  \left[ 
    e_i^\top,1,B(X_i)^\top
  \right]
  \ 
  -
  \ 
  \left[ 
    0_N^\top,1,B(X_i)^\top
  \right]
  \right)
  \begin{bmatrix}
    \Delta_\rho\\\Delta_0\\\Delta
  \end{bmatrix}
    \\
  &
  \qquad
  +
  \ 
  \inner{\delta}
  {|
\lambda^*
+
\Delta
  |
  -
  |
\lambda^*
  |
}
\\
   &
   \ 
   \ge
   \ 
   \frac{1}{N}
\sum_{i=1} 
  ^N
  \left( 
    T_i\cdot
  (
  \varphi^{'}
  )
  ^{-1}
  \!
  \left( 
\inner
{B(X_i)}
{
\lambda^*
}
  \right)
  \ 
  -
  \ 
  1
  \right)
  \left[ 
    e_i^\top,1,B(X_i)^\top
  \right]
  \cdot
  \begin{bmatrix}
    \Delta_\rho\\\Delta_0\\\Delta
  \end{bmatrix}
  +
  \inner{e_i}{\Delta_\rho}
  \\
  &
  \qquad
  +
  \ 
  \inner{\delta}
  {|
\lambda^*
+
\Delta
  |
  -
  |
\lambda^*
  |
}
\\
   &
   \ 
   \ge
   \ 
   -
   \frac{1}{N}
\sum_{i=1} 
  ^N
  \left|
   \left( 
     T_i\cdot
  (
  \varphi^{'}
  )
  ^{-1}
  \!
  \left( 
\inner
{B(X_i)}
{
\lambda^*
}
  \right)
  \ 
  -
  \ 
  1
  \right)
  \left[ 
    e_i^\top,1,B(X_i)^\top
  \right]
  \cdot
  \begin{bmatrix}
    \Delta_\rho\\\Delta_0\\\Delta
  \end{bmatrix}
  \right|
  \\
  &
  \qquad
  -
  \ 
  \inner{\delta}
  {|
\Delta
  |
}
\\
&
\
=:
\ 
-
I_1
\\
&
\qquad
-
\
 I_2
 \,.
\end{split}
\end{align}
Note, that $\Delta_\rho\in \R^N_{\ge 0}$, and thus $\inner{e_i}{\Delta_\rho}\ge 0$ for all $i\in \left\{ 1,\ldots,N \right\}$, where $e_i$ is the $i$-the unit vector.\index{$e_i$, $i$-the unit vector}
\subsection*{Analysis of $I_1$}
By the Cauchy-Schwarz inequality and Lemma~\ref{lem:basis_sum}.\textit{(iii)} it holds
\begin{align*}
  \left|
   \left[ 
    e_i^\top,1,B(X_i)^\top
  \right]
  \cdot
  \begin{bmatrix}
    \Delta_\rho\\\Delta_0\\\Delta
    \end{bmatrix}
  \right|
  \ 
  \le
  \ 
  \norm{
    \Delta_\rho,\Delta_0,\Delta
  }_2
  \ 
  \le
  \ 
  \varepsilon
  \,.
\end{align*}
Furthermore,
\begin{align*}
  &
    \frac{1}{N}
\sum_{i=1} 
  ^N
  \left|
   \left( 
     T_i\cdot
  (
  \varphi^{'}
  )
  ^{-1}
  \!
  \left( 
\inner
{B(X_i)}
{
\lambda^*
}
  \right)
  \ 
  -
  \ 
  1
  \right)
  \right|
  \\
  &
  \ 
  \le
  \ 
    \frac{1}{N}
\sum_{i=1} 
^{N}
\left|
  1-
  \frac
  {T_i}
  {\pi(X_i)}
\right|
\\
&
\qquad
+
    \frac{1}{N}
    \sum_{i=1}^{N} 
\omega
\left(
  (
  \varphi^{'}
  )
  ^{-1}
  ,
  \left|
    \sum_{k=1}^{N}
  B_k(X_i)
  \cdot
  \varphi^{'}
  \left(
    \frac
    {1}
    {\pi(X_k)}
  \right)
  -
  \varphi^{'}
  \left(
    \frac
    {1}
    {\pi(X_i)}
  \right)
  \right|
  \right)
  \\
  &
  \ 
  =:
  \ 
  J_1
  \\
  &
  \qquad
  +
  \ 
  J_2
\end{align*}
\subsubsection*{Analysis of $J_1$}
By the properties of conditional expectation it holds
\begin{gather*}
  \E
  \left[ 
    \frac{T}{\pi(X)}
  \right]
  =
  \E
  \left[ 
    \frac{\E[T|X]}{\pi(X)}
  \right]
  =1
  \,.
\end{gather*}
Also
\begin{gather}
  \E
  \left[ 
    \left| 
    1
    -
    \frac{T}{\pi(X)}
    \right|
  \right]
  \le
  1
  +
  \E
  \left[ 
    \frac{T}{\pi(X)}
  \right]
  =
  2
  \,.
\end{gather}
Thus Etemadi's ($\mathcal{L}_1$ version) strong law of large numbers (cf.\cite[Theorem~5.17]{Klenke2020}) applies
to $J_1$, that is,
$J_1\overset{\P}{\to}0$.
\subsubsection*{Analysis of $J_2$}
By Lemma~\ref{lem:basis_2}.\textit{(i)}
and the uniform continuity of 
$
  (
  \varphi^{'}
  )
  ^{-1}
$
it holds
\begin{align*}
\omega
\left(
  (
  \varphi^{'}
  )
  ^{-1}
  ,
  \left|
    \sum_{k=1}^{N}
  B_k(X_i)
  \cdot
  \varphi^{'}
  \left(
    \frac
    {1}
    {\pi(X_k)}
  \right)
  -
  \varphi^{'}
  \left(
    \frac
    {1}
    {\pi(X_i)}
  \right)
  \right|
  \right)
  &
  \ 
  \le
  \ 
\omega
\left(
  (
  \varphi^{'}
  )
  ^{-1}
  ,
\omega
\left(
  \varphi^{'}
  ,
  h_N^d
  \right)
  \right)
  \\
  &
  \ 
  \to
  \ 
  0
  \,.
\end{align*}
Thus $J_2\to 0$.
\subsubsection*{Conclusion $I_1$}
It follows from the analysis of $J_1$ and $J_2$
\begin{align*}
  \P
  \left[
  I_1 \le \tilde{\varepsilon}
  \right]
  \ 
  \to
  \ 
  1
  \qquad
  \text{for all}\ 
  \tilde{\varepsilon}>0
  \,.
\end{align*}
Note, that this holds independently of the specific choice of $\varepsilon>0$ in the beginning of the proof.
\subsection*{Analysis of $I_2$}
Since
$\delta>0$,
we get
\begin{align*}
     \inner{\delta}
     {|\Delta|}
     \ 
     \le
     \ 
     \norm{\delta}_1
     \norm{\Delta}_\infty
     \ 
     \le
     \ 
     \norm{\delta}_1
     \varepsilon
     \,,
\end{align*}
Since $\norm{\delta}_1$ converges to $0$ in probability, we get
\begin{align*}
  \P
  \left[
  I_2 \le \tilde{\varepsilon}
  \right]
  \ 
  \to
  \ 
  1
  \qquad
  \text{for all}\ 
  \tilde{\varepsilon}>0
  \,.
\end{align*}
\subsection*{Conclusion}
By \eqref{99909}, and the analysis of $I_1$ and $I_2$, we get
\begin{align*}
  \P
  \left[ 
   G
   \left( 
\Delta_\rho,\Delta_0,\lambda^*+\Delta
   \right)
   \ 
   -
   \ 
   G
   \left(
0_N,0,\lambda^*
   \right)
   \ge - 
  \tilde{\varepsilon}
  \right]
  \ 
  \to
  \ 
  1
  \qquad
  \text{for all}\ 
  \tilde{\varepsilon}>0
  \,.
\end{align*}
This holds uniformly for all 
$
\norm{
\Delta_\rho,\Delta_0,\Delta
}_2=\varepsilon
$.
Thus
\begin{align*}
  \P
  \left[ 
  \underline{\Delta G^*_\varepsilon}
   \ge - 
  \tilde{\varepsilon}
  \right]
  \ 
  \to
  \ 
  1
  \qquad
  \text{for all}\ 
  \tilde{\varepsilon}>0
  \,.
\end{align*}
From the measurability of 
$
  \underline{\Delta G^*_\varepsilon}
$ 
(see Lemma~\ref{lem:meas_inf_G})
it follows
\begin{align*}
  \P
  \left[ 
  \underline{\Delta G^*_\varepsilon}
   \ge 0
  \right]
  \ 
  \to
  \ 
  1
  \,.
\end{align*}
But this holds independently of the choice $\varepsilon>0$.
 \end{proof}
\begin{remark}
  The last proof is a simplification of the similar \cite[Proof of Lemma~2]{Wang2019}. There, the authors claim to derive concrete learning rates. 
  But their proof seems to be missing some assumptions. To bound the quadratic term (first display of page 23) away from 0 they need an assumption on a Hessian matrix that seems to be missing.
  Note that the order of the terms in the conclusion (see \cite[page 25]{Wang2019}) is not quite right.
  After carefully reading the proof of Wang and Zubizarreta, I decided to aim for less, that is, only consistency instead of concrete learning rates.
  This is possible, because of the choice of partitioning estimates as basis functions that allows for a concrete oracle parameter.
  With this choice, a linear bound like \eqref{cv:primer:mvthe} suffices. Then I get rid of the quadratic Taylor expansion, assumptions on eigenvalues of a Hessian matrix and an application of matrix concentration inequalities. This greatly simplifies the proof.
  Later, I show that the lack of concrete learning rates is compensated by good approximation properties of the basis functions (see the remark of the section \textit{Analysis of $R_2$}).
\end{remark}
