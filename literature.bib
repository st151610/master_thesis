@book{Andersen1993,
  title = {Statistical {{Models Based}} on {{Counting Processes}}},
  author = {Andersen, Per Kragh and Borgan, {\O}rnulf and Gill, Richard D. and Keiding, Niels},
  year = {1993},
  series = {Springer {{Series}} in {{Statistics}}},
  publisher = {{Springer US}},
  address = {{New York, NY}},
  doi = {10.1007/978-1-4612-4348-9},
  isbn = {978-0-387-94519-4 978-1-4612-4348-9},
  keywords = {censoring,estimator,likelihood,survival analysis},
  file = {/home/ioan/Zotero/storage/AU2XP7CP/Andersen, P.K. - Statistical Models Based on Counting Processes.pdf;/home/ioan/Zotero/storage/V7HUR8HL/Andersen et al. - 1993 - Statistical Models Based on Counting Processes.pdf}
}

@article{Blumberg2016,
  title = {Causal {{Inference}} for {{Statistics}}, {{Social}}, and {{Biomedical Sciences}}: {{An Introduction}}: {{Book Reviews}}},
  shorttitle = {Causal {{Inference}} for {{Statistics}}, {{Social}}, and {{Biomedical Sciences}}},
  author = {Blumberg, Carol Joyce},
  year = {2016},
  month = apr,
  journal = {International Statistical Review},
  volume = {84},
  number = {1},
  pages = {159--159},
  issn = {03067734},
  doi = {10.1111/insr.12170},
  langid = {english},
  file = {/home/ioan/Zotero/storage/WHVECFKD/Blumberg - 2016 - Causal Inference for Statistics, Social, and Biome.pdf}
}

@misc{Chen2012,
  title = {The {{Masked Sample Covariance Estimator}}: {{An Analysis}} via {{Matrix Concentration Inequalities}}},
  shorttitle = {The {{Masked Sample Covariance Estimator}}},
  author = {Chen, Richard Y. and Gittens, Alex and Tropp, Joel A.},
  year = {2012},
  month = jun,
  number = {arXiv:1109.1637},
  eprint = {1109.1637},
  eprinttype = {arxiv},
  primaryclass = {math, stat},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.1109.1637},
  abstract = {Covariance estimation becomes challenging in the regime where the number p of variables outstrips the number n of samples available to construct the estimate. One way to circumvent this problem is to assume that the covariance matrix is nearly sparse and to focus on estimating only the significant entries. To analyze this approach, Levina and Vershynin (2011) introduce a formalism called masked covariance estimation, where each entry of the sample covariance estimator is reweighted to reflect an a priori assessment of its importance. This paper provides a short analysis of the masked sample covariance estimator by means of a matrix concentration inequality. The main result applies to general distributions with at least four moments. Specialized to the case of a Gaussian distribution, the theory offers qualitative improvements over earlier work. For example, the new results show that n = O(B log\^2 p) samples suffice to estimate a banded covariance matrix with bandwidth B up to a relative spectral-norm error, in contrast to the sample complexity n = O(B log\^5 p) obtained by Levina and Vershynin.},
  archiveprefix = {arXiv},
  keywords = {Mathematics - Probability,Mathematics - Statistics Theory,Primary: 60B20},
  file = {/home/ioan/Zotero/storage/L2AN2KQK/Chen et al. - 2012 - The Masked Sample Covariance Estimator An Analysi.pdf;/home/ioan/Zotero/storage/YZIKI8F3/1109.html}
}

@article{Fong2018,
  title = {Covariate Balancing Propensity Score for a Continuous Treatment: {{Application}} to the Efficacy of Political Advertisements},
  shorttitle = {Covariate Balancing Propensity Score for a Continuous Treatment},
  author = {Fong, Christian and Hazlett, Chad and Imai, Kosuke},
  year = {2018},
  month = mar,
  journal = {The Annals of Applied Statistics},
  volume = {12},
  number = {1},
  pages = {156--177},
  publisher = {{Institute of Mathematical Statistics}},
  issn = {1932-6157, 1941-7330},
  doi = {10.1214/17-AOAS1101},
  abstract = {Propensity score matching and weighting are popular methods when estimating causal effects in observational studies. Beyond the assumption of unconfoundedness, however, these methods also require the model for the propensity score to be correctly specified. The recently proposed covariate balancing propensity score (CBPS) methodology increases the robustness to model misspecification by directly optimizing sample covariate balance between the treatment and control groups. In this paper, we extend the CBPS to a continuous treatment. We propose the covariate balancing generalized propensity score (CBGPS) methodology, which minimizes the association between covariates and the treatment. We develop both parametric and nonparametric approaches and show their superior performance over the standard maximum likelihood estimation in a simulation study. The CBGPS methodology is applied to an observational study, whose goal is to estimate the causal effects of political advertisements on campaign contributions. We also provide open-source software that implements the proposed methods.},
  keywords = {Causal inference,covariate balance,generalized propensity score,inverse-probability weighting,treatment effect},
  file = {/home/ioan/Zotero/storage/ETHSLHAR/Fong et al. - 2018 - Covariate balancing propensity score for a continu.pdf;/home/ioan/Zotero/storage/R82C7W48/17-AOAS1101.html}
}

@article{Hainmueller2012,
  title = {Entropy {{Balancing}} for {{Causal Effects}}: {{A Multivariate Reweighting Method}} to {{Produce Balanced Samples}} in {{Observational Studies}}},
  shorttitle = {Entropy {{Balancing}} for {{Causal Effects}}},
  author = {Hainmueller, Jens},
  year = {2012},
  journal = {Political Analysis},
  volume = {20},
  number = {1},
  pages = {25--46},
  issn = {1047-1987, 1476-4989},
  doi = {10.1093/pan/mpr025},
  abstract = {This paper proposes entropy balancing, a data preprocessing method to achieve covariate balance in observational studies with binary treatments. Entropy balancing relies on a maximum entropy reweighting scheme that calibrates unit weights so that the reweighted treatment and control group satisfy a potentially large set of prespecified balance conditions that incorporate information about known sample moments. Entropy balancing thereby exactly adjusts inequalities in representation with respect to the first, second, and possibly higher moments of the covariate distributions. These balance improvements can reduce model dependence for the subsequent estimation of treatment effects. The method assures that balance improves on all covariate moments included in the reweighting. It also obviates the need for continual balance checking and iterative searching over propensity score models that may stochastically balance the covariate moments. We demonstrate the use of entropy balancing with Monte Carlo simulations and empirical applications.},
  langid = {english},
  file = {/home/ioan/Zotero/storage/Z5ENHQYD/Hainmueller - 2012 - Entropy Balancing for Causal Effects A Multivaria.pdf}
}

@incollection{Hirano2005,
  title = {The {{Propensity Score}} with {{Continuous Treatments}}},
  booktitle = {Wiley {{Series}} in {{Probability}} and {{Statistics}}},
  author = {Hirano, Keisuke and Imbens, Guido W.},
  editor = {Gelman, Andrew and Meng, Xiao-Li},
  year = {2005},
  month = jul,
  pages = {73--84},
  publisher = {{John Wiley \& Sons, Ltd}},
  address = {{Chichester, UK}},
  doi = {10.1002/0470090456.ch7},
  isbn = {978-0-470-09045-9 978-0-470-09043-5},
  langid = {english},
  file = {/home/ioan/Zotero/storage/ARALBRYZ/Hirano and Imbens - 2005 - The Propensity Score with Continuous Treatments.pdf}
}

@misc{Huling2021,
  title = {Independence Weights for Causal Inference with Continuous Exposures},
  author = {Huling, Jared D. and Greifer, Noah and Chen, Guanhua},
  year = {2021},
  month = jul,
  number = {arXiv:2107.07086},
  eprint = {2107.07086},
  eprinttype = {arxiv},
  primaryclass = {stat},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.2107.07086},
  abstract = {Studying causal effects of continuous exposures is important for gaining a deeper understanding of many interventions, policies, or medications, yet researchers are often left with observational studies for doing so. In the observational setting, confounding is a barrier to estimation of causal effects. Weighting approaches seek to control for confounding by reweighting samples so that confounders are comparable across different values of the exposure, yet for continuous exposures, weighting methods are highly sensitive to model misspecification. In this paper we elucidate the key property that makes weights effective in estimating causal quantities involving continuous exposures. We show that to eliminate confounding, weights should make exposure and confounders independent on the weighted scale. We develop a measure that characterizes the degree to which a set of weights induces such independence. Further, we propose a new model-free method for weight estimation by optimizing our measure. We study the theoretical properties of our measure and our weights, and prove that our weights can explicitly mitigate exposure-confounder dependence. The empirical effectiveness of our approach is demonstrated in a suite of challenging numerical experiments, where we find that our weights are quite robust and work well under a broad range of settings.},
  archiveprefix = {arXiv},
  keywords = {Statistics - Methodology},
  file = {/home/ioan/Zotero/storage/4JV8R6YP/Huling et al. - 2021 - Independence weights for causal inference with con.pdf;/home/ioan/Zotero/storage/JI9P3D5B/2107.html}
}

@misc{Kallus2019,
  title = {Kernel {{Optimal Orthogonality Weighting}}: {{A Balancing Approach}} to {{Estimating Effects}} of {{Continuous Treatments}}},
  shorttitle = {Kernel {{Optimal Orthogonality Weighting}}},
  author = {Kallus, Nathan and Santacatterina, Michele},
  year = {2019},
  month = oct,
  number = {arXiv:1910.11972},
  eprint = {1910.11972},
  eprinttype = {arxiv},
  primaryclass = {stat},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.1910.11972},
  abstract = {Many scientific questions require estimating the effects of continuous treatments. Outcome modeling and weighted regression based on the generalized propensity score are the most commonly used methods to evaluate continuous effects. However, these techniques may be sensitive to model misspecification, extreme weights or both. In this paper, we propose Kernel Optimal Orthogonality Weighting (KOOW), a convex optimization-based method, for estimating the effects of continuous treatments. KOOW finds weights that minimize the worst-case penalized functional covariance between the continuous treatment and the confounders. By minimizing this quantity, KOOW successfully provides weights that orthogonalize confounders and the continuous treatment, thus providing optimal covariate balance, while controlling for extreme weights. We valuate its comparative performance in a simulation study. Using data from the Women's Health Initiative observational study, we apply KOOW to evaluate the effect of red meat consumption on blood pressure.},
  archiveprefix = {arXiv},
  keywords = {Statistics - Applications,Statistics - Machine Learning,Statistics - Methodology},
  file = {/home/ioan/Zotero/storage/4D3WWK47/Kallus and Santacatterina - 2019 - Kernel Optimal Orthogonality Weighting A Balancin.pdf;/home/ioan/Zotero/storage/CSGBLEK6/1910.html}
}

@article{Kang2007,
  title = {Demystifying {{Double Robustness}}: {{A Comparison}} of {{Alternative Strategies}} for {{Estimating}} a {{Population Mean}} from {{Incomplete Data}}},
  shorttitle = {Demystifying {{Double Robustness}}},
  author = {Kang, Joseph D. Y. and Schafer, Joseph L.},
  year = {2007},
  month = nov,
  journal = {Statistical Science},
  volume = {22},
  number = {4},
  pages = {523--539},
  publisher = {{Institute of Mathematical Statistics}},
  issn = {0883-4237, 2168-8745},
  doi = {10.1214/07-STS227},
  abstract = {When outcomes are missing for reasons beyond an investigator's control, there are two different ways to adjust a parameter estimate for covariates that may be related both to the outcome and to missingness. One approach is to model the relationships between the covariates and the outcome and use those relationships to predict the missing values. Another is to model the probabilities of missingness given the covariates and incorporate them into a weighted or stratified estimate. Doubly robust (DR) procedures apply both types of model simultaneously and produce a consistent estimate of the parameter if either of the two models has been correctly specified. In this article, we show that DR estimates can be constructed in many ways. We compare the performance of various DR and non-DR estimates of a population mean in a simulated example where both models are incorrect but neither is grossly misspecified. Methods that use inverse-probabilities as weights, whether they are DR or not, are sensitive to misspecification of the propensity model when some estimated propensities are small. Many DR methods perform better than simple inverse-probability weighting. None of the DR methods we tried, however, improved upon the performance of simple regression-based prediction of the missing values. This study does not represent every missing-data problem that will arise in practice. But it does demonstrate that, in at least some settings, two wrong models are not better than one.},
  keywords = {Causal inference,missing data,model-assisted survey estimation,propensity score,weighted estimating equations},
  file = {/home/ioan/Zotero/storage/E6G6EG89/Kang and Schafer - 2007 - Demystifying Double Robustness A Comparison of Al.pdf;/home/ioan/Zotero/storage/7A9R2XDH/07-STS227.html}
}

@misc{Martinet2020,
  title = {A {{Balancing Weight Framework}} for {{Estimating}} the {{Causal Effect}} of {{General Treatments}}},
  author = {Martinet, Guillaume},
  year = {2020},
  month = feb,
  journal = {arXiv e-prints},
  abstract = {In observational studies, weighting methods that directly optimize the balance between treatment and covariates have received much attention lately; however these have mainly focused on binary treatments. Inspired by domain adaptation, we show that such methods can be actually reformulated as specific implementations of a discrepancy minimization problem aimed at tackling a shift of distribution from observational to interventional data. More precisely, we introduce a new framework, Covariate Balance via Discrepancy Minimization (CBDM), that provably encompasses most of the existing balancing weight methods and formally extends them to treatments of arbitrary types (e.g., continuous or multivariate). We establish theoretical guarantees for our framework that both offer generalizations of properties known when the treatment is binary, and give a better grasp on what hyperparameters to choose in non-binary settings. Based on such insights, we propose a particular implementation of CBDM for estimating dose-response curves and demonstrate through experiments its competitive performance relative to other existing approaches for continuous treatments.},
  keywords = {Mathematics - Statistics Theory,Statistics - Methodology},
  annotation = {ADS Bibcode: 2020arXiv200211276M},
  file = {/home/ioan/Zotero/storage/MTEWCNS6/Martinet - 2020 - A Balancing Weight Framework for Estimating the Ca.pdf}
}

@incollection{Mordukhovich2022,
  title = {{{ENHANCED CALCULUS AND FENCHEL DUALITY}}},
  booktitle = {Convex {{Analysis}} and {{Beyond}}: {{Volume I}}: {{Basic Theory}}},
  author = {Mordukhovich, Boris S. and Mau Nam, Nguyen},
  editor = {Mordukhovich, Boris S. and Mau Nam, Nguyen},
  year = {2022},
  series = {Springer {{Series}} in {{Operations Research}} and {{Financial Engineering}}},
  pages = {255--310},
  publisher = {{Springer International Publishing}},
  address = {{Cham}},
  doi = {10.1007/978-3-030-94785-9_4},
  abstract = {A large part of this chapter continues developing generalized differential calculus started in the previous chapter, but from different perspectives. Namely, we consider Fenchel conjugates and duality relationships, which are specifically related to convexity and play a fundamental role in convex analysis.},
  isbn = {978-3-030-94785-9},
  langid = {english},
  file = {/home/ioan/Zotero/storage/J2CU8HGQ/Mordukhovich and Mau Nam - 2022 - ENHANCED CALCULUS AND FENCHEL DUALITY.pdf}
}

@incollection{Mordukhovich2022a,
  title = {{{MISCELLANEOUS TOPICS ON CONVEXITY}}},
  booktitle = {Convex {{Analysis}} and {{Beyond}}: {{Volume I}}: {{Basic Theory}}},
  author = {Mordukhovich, Boris S. and Mau Nam, Nguyen},
  editor = {Mordukhovich, Boris S. and Mau Nam, Nguyen},
  year = {2022},
  series = {Springer {{Series}} in {{Operations Research}} and {{Financial Engineering}}},
  pages = {381--443},
  publisher = {{Springer International Publishing}},
  address = {{Cham}},
  doi = {10.1007/978-3-030-94785-9_6},
  abstract = {This chapter deals with certain miscellaneous topics of convex analysis in infinite-dimensional and finite-dimensional spaces. Most of the presented developments are not directly related to generalized differentiation, although some results employ subgradients. We mainly concentrate here on strong convexity and related monotonicity of subgradient mappings, which are particularly important for Nesterov's smoothing techniques in numerical optimization; on the study of asymptotic behavior of convex sets and functions at infinity; on the considerations of the remarkable classes of signed distance and minimal time functions; and on classical finite-dimensional results related to the Carath\'eodory and Helly theorems, Farkas lemma, etc.},
  isbn = {978-3-030-94785-9},
  langid = {english},
  file = {/home/ioan/Zotero/storage/2FBLTN4N/Mordukhovich and Mau Nam - 2022 - MISCELLANEOUS TOPICS ON CONVEXITY.pdf}
}

@incollection{Mordukhovich2022b,
  title = {{{FUNDAMENTALS}}},
  booktitle = {Convex {{Analysis}} and {{Beyond}}: {{Volume I}}: {{Basic Theory}}},
  author = {Mordukhovich, Boris S. and Mau Nam, Nguyen},
  editor = {Mordukhovich, Boris S. and Mau Nam, Nguyen},
  year = {2022},
  series = {Springer {{Series}} in {{Operations Research}} and {{Financial Engineering}}},
  pages = {1--64},
  publisher = {{Springer International Publishing}},
  address = {{Cham}},
  doi = {10.1007/978-3-030-94785-9_1},
  abstract = {This chapter collects fundamental notions and results in vector spaces, topological spaces, topological vector spaces, and their specifications that are widely used in the subsequent chapters of the book to build the basic theory of convexity and its applications.},
  isbn = {978-3-030-94785-9},
  langid = {english},
  file = {/home/ioan/Zotero/storage/HR4DEAM2/Mordukhovich and Mau Nam - 2022 - FUNDAMENTALS.pdf}
}

@incollection{Mordukhovich2022c,
  title = {{{VARIATIONAL TECHNIQUES AND FURTHER SUBGRADIENT STUDY}}},
  booktitle = {Convex {{Analysis}} and {{Beyond}}: {{Volume I}}: {{Basic Theory}}},
  author = {Mordukhovich, Boris S. and Mau Nam, Nguyen},
  editor = {Mordukhovich, Boris S. and Mau Nam, Nguyen},
  year = {2022},
  series = {Springer {{Series}} in {{Operations Research}} and {{Financial Engineering}}},
  pages = {311--379},
  publisher = {{Springer International Publishing}},
  address = {{Cham}},
  doi = {10.1007/978-3-030-94785-9_5},
  abstract = {We start this chapter with the study of variational structures for functions and sets in complete metric and normed spaces. The major variational and extremal principles, being held even in nonconvex frameworks, are largely related to and motivated by the developments on convexity. Variational/extremal principles and variational techniques elaborated in this chapter in complete spaces are then applied to establishing density results for \$\$\textbackslash varepsilon \$\${$\epsilon$}-subgradients of convex functions and to developing \$\$\textbackslash varepsilon \$\${$\epsilon$}-subdifferential calculus with the further applications to convex mean value theorems, subdifferential monotonicity, characterizations of the Fr\'echet and G\^ateauxG\^ateaux differentiability~differentiability together with their generic properties, and finally to deriving subgradient formulas for spectral and singular functions in convex analysis. Our major results hold in Banach spaces, but some results and proofs are valid in general settings of complete metric and topological vector spaces, while those for spectral and singular functions are primarily finite-dimensional.},
  isbn = {978-3-030-94785-9},
  langid = {english},
  file = {/home/ioan/Zotero/storage/248D8V7E/Mordukhovich and Mau Nam - 2022 - VARIATIONAL TECHNIQUES AND FURTHER SUBGRADIENT STU.pdf}
}

@incollection{Mordukhovich2022d,
  title = {{{CONVEX GENERALIZED DIFFERENTIATION}}},
  booktitle = {Convex {{Analysis}} and {{Beyond}}: {{Volume I}}: {{Basic Theory}}},
  author = {Mordukhovich, Boris S. and Mau Nam, Nguyen},
  editor = {Mordukhovich, Boris S. and Mau Nam, Nguyen},
  year = {2022},
  series = {Springer {{Series}} in {{Operations Research}} and {{Financial Engineering}}},
  pages = {179--253},
  publisher = {{Springer International Publishing}},
  address = {{Cham}},
  doi = {10.1007/978-3-030-94785-9_3},
  abstract = {Generalized differentiation lies at the very heart of convex analysis and its applications. Since the most useful and even most simple convex functions are nondifferentiable at the points of interest, the now flourishing generalized differentiation theory oriented toward optimization-related problems has started from convex analysis and then has been extended to more general variational frameworks. It concerns not only nondifferentiable functions but also sets with nonsmooth boundaries as well as set-valued mappings. Calculus rules of generalized differentiation have been the central issue of the theory and applications from the very beginning of convex analysis.},
  isbn = {978-3-030-94785-9},
  langid = {english},
  file = {/home/ioan/Zotero/storage/SWYHRVJ6/Mordukhovich and Mau Nam - 2022 - CONVEX GENERALIZED DIFFERENTIATION.pdf}
}

@incollection{Mordukhovich2022e,
  title = {{{BASIC THEORY OF CONVEXITY}}},
  booktitle = {Convex {{Analysis}} and {{Beyond}}: {{Volume I}}: {{Basic Theory}}},
  author = {Mordukhovich, Boris S. and Mau Nam, Nguyen},
  editor = {Mordukhovich, Boris S. and Mau Nam, Nguyen},
  year = {2022},
  series = {Springer {{Series}} in {{Operations Research}} and {{Financial Engineering}}},
  pages = {65--177},
  publisher = {{Springer International Publishing}},
  address = {{Cham}},
  doi = {10.1007/978-3-030-94785-9_2},
  abstract = {This chapter is devoted to basic convexity theory dealing with sets and functions defined in various space frameworks consisting of linear/vector spaces, topological vector spaces, locally convex topological spaces and their subclasses, and also specific results in finite dimensions. Developing the geometric approach to convex analysis, we start with convex sets, establish fundamental separation theorems for them, and then proceed with the study of convex functions. Further topics on convexity, including duality and generalized differentiation theories, are considered in the subsequent chapters. Unless otherwise stated, we consider real vector spaces in this chapter and the subsequent ones.},
  isbn = {978-3-030-94785-9},
  langid = {english},
  file = {/home/ioan/Zotero/storage/UF63838N/Mordukhovich and Mau Nam - 2022 - BASIC THEORY OF CONVEXITY.pdf}
}

@book{Mordukhovich2022f,
  title = {Convex {{Analysis}} and {{Beyond}}: {{Volume I}}: {{Basic Theory}}},
  shorttitle = {Convex {{Analysis}} and {{Beyond}}},
  author = {Mordukhovich, Boris S. and Mau Nam, Nguyen},
  year = {2022},
  series = {Springer {{Series}} in {{Operations Research}} and {{Financial Engineering}}},
  publisher = {{Springer International Publishing}},
  address = {{Cham}},
  doi = {10.1007/978-3-030-94785-9},
  isbn = {978-3-030-94784-2 978-3-030-94785-9},
  langid = {english},
  keywords = {Convex Analysis,Convex Functions,Convex Sets,Generalized Differentiation,Locally Convex Topological Vector Spaces,Topological Vector Spaces,Variational Analysis,Variational Methods of Nonlinear Analysis},
  file = {/home/ioan/Zotero/storage/EA8EVDIW/Mordukhovich and Mau Nam - 2022 - Convex Analysis and Beyond Volume I Basic Theory.pdf}
}

@incollection{Mordukhovich2022g,
  title = {{{CONVEXIFIED LIPSCHITZIAN ANALYSIS}}},
  booktitle = {Convex {{Analysis}} and {{Beyond}}: {{Volume I}}: {{Basic Theory}}},
  author = {Mordukhovich, Boris S. and Mau Nam, Nguyen},
  editor = {Mordukhovich, Boris S. and Mau Nam, Nguyen},
  year = {2022},
  series = {Springer {{Series}} in {{Operations Research}} and {{Financial Engineering}}},
  pages = {445--551},
  publisher = {{Springer International Publishing}},
  address = {{Cham}},
  doi = {10.1007/978-3-030-94785-9_7},
  abstract = {This chapter deals with nonconvex nondifferentiable functions. However, the machinery and results of convex analysis presented in the previous chapters provide crucial tools for the study of generalized differentiation beyond the function convexity. A conventional way to do it is developing a scheme using first a suitable extension of the directional derivative and then defining the corresponding directional subdifferential via a duality correspondence with such an extended directional derivative. The subgradient mappings obtained in this way are always convex-valued, but the most important properties of them are exhibited when the directional derivatives are assumed (or happen to be) convex with respect to directions. All of this constitutes the realm of convexified analysis in this chapter.},
  isbn = {978-3-030-94785-9},
  langid = {english},
  file = {/home/ioan/Zotero/storage/V6ZZGDI4/Mordukhovich and Mau Nam - 2022 - CONVEXIFIED LIPSCHITZIAN ANALYSIS.pdf}
}

@article{Newey1997a,
  title = {Convergence Rates and Asymptotic Normality for Series Estimators},
  author = {Newey, Whitney K.},
  year = {1997},
  month = jul,
  journal = {Journal of Econometrics},
  volume = {79},
  number = {1},
  pages = {147--168},
  issn = {0304-4076},
  doi = {10.1016/S0304-4076(97)00011-0},
  abstract = {This paper gives general conditions for convergence rates and asymptotic normality of series estimators of conditional expectations, and specializes these conditions to polynomial regression and regression splines. Both mean-square and uniform convergence rates are derived. Asymptotic normality is shown for nonlinear functionals of series estimators, covering many cases not previously treated. Also, a simple condition for n-consitency of a functional of a series estimator is given. The regularity conditions are straightforward to understand, and several examples are given to illustrate their application.},
  langid = {english},
  keywords = {Asymptotic normality,Convergence rates,Nonparametric estimation,Series estimation},
  file = {/home/ioan/Zotero/storage/D3R4MX2M/Newey - 1997 - Convergence rates and asymptotic normality for ser.pdf;/home/ioan/Zotero/storage/9UBL3IIE/S0304407697000110.html}
}

@article{Rosenbaum1983,
  title = {The {{Central Role}} of the {{Propensity Score}} in {{Observational Studies}} for {{Causal Effects}}},
  author = {Rosenbaum, Paul R. and Rubin, Donald B.},
  year = {1983},
  journal = {Biometrika},
  volume = {70},
  number = {1},
  pages = {41--55},
  publisher = {{[Oxford University Press, Biometrika Trust]}},
  issn = {0006-3444},
  doi = {10.2307/2335942},
  abstract = {The propensity score is the conditional probability of assignment to a particular treatment given a vector of observed covariates. Both large and small sample theory show that adjustment for the scalar propensity score is sufficient to remove bias due to all observed covariates. Applications include: (i) matched sampling on the univariate propensity score, which is a generalization of discriminant matching, (ii) multivariate adjustment by subclassification on the propensity score where the same subclasses are used to estimate treatment effects for all outcome variables and in all subpopulations, and (iii) visual representation of multivariate covariance adjustment by a two-dimensional plot.},
  file = {/home/ioan/Zotero/storage/4CHPRN4I/Rosenbaum and Rubin - 1983 - The Central Role of the Propensity Score in Observ.pdf;/home/ioan/Zotero/storage/H6FYUCVL/rosenbaum_1983.pdf}
}

@misc{Tropp2015,
  title = {An {{Introduction}} to {{Matrix Concentration Inequalities}}},
  author = {Tropp, Joel A.},
  year = {2015},
  month = jan,
  number = {arXiv:1501.01571},
  eprint = {1501.01571},
  eprinttype = {arxiv},
  primaryclass = {cs, math, stat},
  publisher = {{arXiv}},
  abstract = {In recent years, random matrices have come to play a major role in computational mathematics, but most of the classical areas of random matrix theory remain the province of experts. Over the last decade, with the advent of matrix concentration inequalities, research has advanced to the point where we can conquer many (formerly) challenging problems with a page or two of arithmetic. The aim of this monograph is to describe the most successful methods from this area along with some interesting examples that these techniques can illuminate.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Data Structures and Algorithms,Computer Science - Information Theory,Mathematics - Numerical Analysis,Mathematics - Probability,Primary: 60B20. Secondary: 60F10; 60G50; 60G42,Statistics - Machine Learning},
  file = {/home/ioan/Zotero/storage/WRB3MWEN/Tropp - 2015 - An Introduction to Matrix Concentration Inequaliti.pdf}
}

@misc{Tubbicke2020,
  title = {Entropy {{Balancing}} for {{Continuous Treatments}}},
  author = {T{\"u}bbicke, Stefan},
  year = {2020},
  month = may,
  number = {arXiv:2001.06281},
  eprint = {2001.06281},
  eprinttype = {arxiv},
  primaryclass = {econ, stat},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.2001.06281},
  abstract = {This paper introduces entropy balancing for continuous treatments (EBCT) by extending the original entropy balancing methodology of Hainm\textbackslash "uller (2012). In order to estimate balancing weights, the proposed approach solves a globally convex constrained optimization problem. EBCT weights reliably eradicate Pearson correlations between covariates and the continuous treatment variable. This is the case even when other methods based on the generalized propensity score tend to yield insufficient balance due to strong selection into different treatment intensities. Moreover, the optimization procedure is more successful in avoiding extreme weights attached to a single unit. Extensive Monte-Carlo simulations show that treatment effect estimates using EBCT display similar or lower bias and uniformly lower root mean squared error. These properties make EBCT an attractive method for the evaluation of continuous treatments.},
  archiveprefix = {arXiv},
  keywords = {Economics - Econometrics,Statistics - Methodology},
  file = {/home/ioan/Zotero/storage/Q4B3WC53/Tübbicke - 2020 - Entropy Balancing for Continuous Treatments.pdf;/home/ioan/Zotero/storage/FBSLNH3J/2001.html}
}

@book{Vaart1998,
  title = {Asymptotic {{Statistics}}},
  author = {van der Vaart, A. W.},
  year = {1998},
  series = {Cambridge {{Series}} in {{Statistical}} and {{Probabilistic Mathematics}}},
  publisher = {{Cambridge University Press}},
  address = {{Cambridge}},
  doi = {10.1017/CBO9780511802256},
  abstract = {This book is an introduction to the field of asymptotic statistics. The treatment is both practical and mathematically rigorous. In addition to most of the standard topics of an asymptotics course, including likelihood inference, M-estimation, the theory of asymptotic efficiency, U-statistics, and rank procedures, the book also presents recent research topics such as semiparametric models, the bootstrap, and empirical processes and their applications. The topics are organized from the central idea of approximation by limit experiments, which gives the book one of its unifying themes. This entails mainly the local approximation of the classical i.i.d. set up with smooth parameters by location experiments involving a single, normally distributed observation. Thus, even the standard subjects of asymptotic statistics are presented in a novel way. Suitable as a graduate or Master's level statistics text, this book will also give researchers an overview of research in asymptotic statistics.},
  isbn = {978-0-521-78450-4},
  file = {/home/ioan/Zotero/storage/I2LRWQVH/TapScanner-31.10.pdf;/home/ioan/Zotero/storage/Z2Q5RWU3/A3C7DAD3F7E66A1FA60E9C8FE132EE1D.html}
}

@misc{Vegetabile2020,
  title = {Nonparametric {{Estimation}} of {{Population Average Dose-Response Curves}} Using {{Entropy Balancing Weights}} for {{Continuous Exposures}}},
  author = {Vegetabile, Brian G. and Griffin, Beth Ann and Coffman, Donna L. and Cefalu, Matthew and McCaffrey, Daniel F.},
  year = {2020},
  month = mar,
  number = {arXiv:2003.02938},
  eprint = {2003.02938},
  eprinttype = {arxiv},
  primaryclass = {stat},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.2003.02938},
  abstract = {Weighted estimators are commonly used for estimating exposure effects in observational settings to establish causal relations. These estimators have a long history of development when the exposure of interest is binary and where the weights are typically functions of an estimated propensity score. Recent developments in optimization-based estimators for constructing weights in binary exposure settings, such as those based on entropy balancing, have shown more promise in estimating treatment effects than those methods that focus on the direct estimation of the propensity score using likelihood-based methods. This paper explores recent developments of entropy balancing methods to continuous exposure settings and the estimation of population dose-response curves using nonparametric estimation combined with entropy balancing weights, focusing on factors that would be important to applied researchers in medical or health services research. The methods developed here are applied to data from a study assessing the effect of non-randomized components of an evidence-based substance use treatment program on emotional and substance use clinical outcomes.},
  archiveprefix = {arXiv},
  keywords = {Statistics - Methodology},
  file = {/home/ioan/Zotero/storage/B2BR8INK/Vegetabile et al. - 2020 - Nonparametric Estimation of Population Average Dos.pdf;/home/ioan/Zotero/storage/57SI6MRJ/2003.html}
}

@article{Wang2019,
  title = {Minimal {{Dispersion Approximately Balancing Weights}}: {{Asymptotic Properties}} and {{Practical Considerations}}},
  shorttitle = {Minimal {{Dispersion Approximately Balancing Weights}}},
  author = {Wang, Yixin and Zubizarreta, Jos{\'e} R.},
  year = {2019},
  month = oct,
  journal = {Biometrika},
  eprint = {1705.00998},
  eprinttype = {arxiv},
  primaryclass = {math, stat},
  pages = {asz050},
  issn = {0006-3444, 1464-3510},
  doi = {10.1093/biomet/asz050},
  abstract = {Weighting methods are widely used to adjust for covariates in observational studies, sample surveys, and regression settings. In this paper, we study a class of recently proposed weighting methods which find the weights of minimum dispersion that approximately balance the covariates. We call these weights "minimal weights" and study them under a common optimization framework. The key observation is the connection between approximate covariate balance and shrinkage estimation of the propensity score. This connection leads to both theoretical and practical developments. From a theoretical standpoint, we characterize the asymptotic properties of minimal weights and show that, under standard smoothness conditions on the propensity score function, minimal weights are consistent estimates of the true inverse probability weights. Also, we show that the resulting weighting estimator is consistent, asymptotically normal, and semiparametrically efficient. From a practical standpoint, we present a finite sample oracle inequality that bounds the loss incurred by balancing more functions of the covariates than strictly needed. This inequality shows that minimal weights implicitly bound the number of active covariate balance constraints. We finally provide a tuning algorithm for choosing the degree of approximate balance in minimal weights. We conclude the paper with four empirical studies that suggest approximate balance is preferable to exact balance, especially when there is limited overlap in covariate distributions. In these studies, we show that the root mean squared error of the weighting estimator can be reduced by as much as a half with approximate balance.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Mathematics - Statistics Theory,Statistics - Applications,Statistics - Methodology},
  file = {/home/ioan/Zotero/storage/Z95A68CS/Wang and Zubizarreta - 2019 - Minimal Dispersion Approximately Balancing Weights.pdf}
}

@article{Wang2023,
  title = {Large {{Sample Properties}} of {{Matching}} for {{Balance}}},
  author = {Wang, Yixin and Zubizarreta, Jos{\'e} R.},
  year = {2023},
  journal = {Statistica Sinica},
  eprint = {1905.11386},
  eprinttype = {arxiv},
  primaryclass = {math, stat},
  issn = {10170405},
  doi = {10.5705/ss.202020.0343},
  abstract = {Matching methods are widely used for causal inference in observational studies. Among them, nearest neighbor matching is arguably the most popular. However, nearest neighbor matching does not generally yield an average treatment effect estimator that is \$\textbackslash sqrt\{n\}\$-consistent (Abadie and Imbens, 2006). Are matching methods not \$\textbackslash sqrt\{n\}\$-consistent in general? In this paper, we study a recent class of matching methods that use integer programming to directly target aggregate covariate balance as opposed to finding close neighbor matches. We show that under suitable conditions these methods can yield simple estimators that are \$\textbackslash sqrt\{n\}\$-consistent and asymptotically optimal.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Mathematics - Statistics Theory,Statistics - Methodology},
  file = {/home/ioan/Zotero/storage/NBRATHRB/Wang and Zubizarreta - 2023 - Large Sample Properties of Matching for Balance.pdf}
}

@misc{zotero-79,
  title = {{Estimating Balancing Weights for Continuous Treatments Using Constrained Optimization}},
  langid = {http://id.loc.gov/vocabulary/iso639-2/eng},
  annotation = {Context Object: url\_ver=Z39.88-2004\&ctx\_ver=Z39.88-2004\&rft\_val\_fmt=info\%3Aofi\%2Ffmt\%3Akev\%3Amtx\%3Adc\&rfr\_id=info\%3Asid\%2Fblacklight.rubyforge.org\%3Agenerator\&rft.title=Estimating+Balancing+Weights+for+Continuous+Treatments+Using+Constrained+Optimization\&rft.format=Dissertation\&rft.language=http\%3A\%2F\%2Fid.loc.gov\%2Fvocabulary\%2Fiso639-2\%2Feng\&rft.relation=Regression\&rft.relation=Quantitative+psychology\&rft.relation=Casual+inference\&rft.relation=Optimization\&rft.relation=Biostatistics\&rft.relation=Epidemiology\&rft.relation=Propensity+score},
  file = {/home/ioan/Zotero/storage/PDRHTLSE/Estimating Balancing Weights for Continuous Treatm.pdf;/home/ioan/Zotero/storage/2QESEEBR/n583z159z.html}
}

@article{Zubizarreta2015,
  title = {Stable {{Weights}} That {{Balance Covariates}} for {{Estimation With Incomplete Outcome Data}}},
  author = {Zubizarreta, Jos{\'e} R.},
  year = {2015},
  month = jul,
  journal = {Journal of the American Statistical Association},
  volume = {110},
  number = {511},
  pages = {910--922},
  issn = {0162-1459, 1537-274X},
  doi = {10.1080/01621459.2015.1023805},
  abstract = {Weighting methods that adjust for observed covariates, such as inverse probability weighting, are widely used for causal inference and estimation with incomplete outcome data. Part of the appeal of such methods is that one set of weights can be used to estimate a range of treatment effects based on different outcomes, or a variety of population means for several variables. However, this appeal can be diminished in practice by the instability of the estimated weights and by the difficulty of adequately adjusting for observed covariates in some settings. To address these limitations, this paper presents a new weighting method that finds the weights of minimum variance that adjust or balance the empirical distribution of the observed covariates up to levels prespecified by the researcher. This method allows the researcher to balance very precisely the means of the observed covariates and other features of their marginal and joint distributions, such as variances and correlations and also, for example, the quantiles of interactions of pairs and triples of observed covariates, thus balancing entire two- and three-way marginals. Since the weighting method is based on a well-defined convex optimization problem, duality theory provides insight into the behavior of the variance of the optimal weights in relation to the level of covariate balance adjustment, answering the question, how much does tightening a balance constraint increases the variance of the weights? Also, the weighting method runs in polynomial time so relatively large data sets can be handled quickly. An implementation of the method is provided in the new package sbw for R. This paper shows some theoretical properties of the resulting weights and illustrates their use by analyzing both a data set from the 2010 Chilean earthquake and a simulated example.},
  langid = {english},
  file = {/home/ioan/Zotero/storage/EI2GZH6U/Zubizarreta - 2015 - Stable Weights that Balance Covariates for Estimat.pdf}
}
