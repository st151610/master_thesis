\subsection*{My Contribution}
I found out, that consistency for the dual variable is enough to prove later results. This simplifies the proof. 
In \cite{Wang2019} they use a quadratic Taylor expansion to obtain learning rates. I found out, that a simpler mean value result for differentiable convex functions is sufficient to proof consistency. 
Since I work with partitioning estimates, I found a suitable oracle parameter.
I prove an (extended) lemma which is central but the details were omited.

Throughout this section we assume for all $N\in\mathbb{N}$ the existence of an 
optimal solution 
$(\lambda_0^\dagger,\lambda^\dagger)$
to Problem?
We define the oracle parameter $\lambda^*\in\R^N$ to be the vector with coordinates
\begin{gather}
  \label{oracle_1}
  \lambda^*_k
  \ 
  :=
  \ 
  f^{'}
  \left( 
    \frac{1}{\pi(X_k)}
  \right)
  \ 
  -
  \ 
  \lambda^\dagger_0
  \qquad
  \text{for all}\ 
  k\in \left\{ 1,\ldots,N \right\}
  \,,
\end{gather}
where $\pi(x)=\P[T=1|X=x]$ is the \textbf{propensity score} at $x\in\mathcal{X}$. Why this choice? First, the $\lambda_0^\dagger$ part is unimportant. We need it to eliminate the same factor in
\begin{gather}
  w(x):=
  (
  f^{'}
  )^{-1}
  \left( \inner{B(x)}{\lambda^\dagger}+\lambda_0^\dagger \right)
\end{gather}
that is 
\begin{gather}
  \inner{B(x)}{\lambda^*}+\lambda_0^\dagger 
  =
\sum_{k=1}^{N} 
B_k(x)
  f^{'}
  \left( 
    \frac{1}{\pi(X_k)}
  \right)
  \,.
\end{gather}
The other part is the foundation of why everything works.
We will show
\begin{gather}
\left| 
\sum_{k=1}^{N} 
B_k(X_i)
  f^{'}
  \left( 
    \frac{1}{\pi(X_k)}
  \right)
  -
  f^{'}
  \left( 
    \frac{1}{\pi(X_k)}
  \right)
\right|
\ 
\le
\ 
      \omega
      \left( f^{'}\circ (x\mapsto1/x)\circ \pi,h_N \right)
      \,.
\end{gather}
Consequently, if $\pi$ is continuous and positive (not 0) on $\mathcal{X}$ and the width of the partition $h_N$ converges to 0, we get 
\begin{gather}
 \left| 
  \inner{B(X_i)}{\lambda^*}+\lambda_0^\dagger 
  -
  f^{'}
  \left( 
    \frac{1}{\pi(X_k)}
  \right)
 \right| 
 \
 \to
 \ 
 0
 \qquad
 \text{almost surely.}
\end{gather}
This helps proving 
\begin{ftheorem}
  \label{bw:cd:th}
  Let 
$(\lambda_0^\dagger,\lambda^\dagger)$
be an optimal solution 
to Problem?
and define the oracle parameter $\lambda^*$ as in \eqref{oracle_1}.
Furthermore, assume that the propensity score function is continuous and positive on $\mathcal{X}$.
Then 
$
    \norm{
      \lambda^\dagger
      -
      \lambda^*
    }_2
    \ 
    \overset{\P}{\to}
    \ 
    0
$
for $N\to\infty$.
\end{ftheorem}
\begin{proof}
We use a hint from the last display of~\cite[p.22]{Wang2019}.
The high-level idea is that the connection of
optimality of  
$(\lambda_0^\dagger,\lambda^\dagger)$
to proximity to (any) oracle parameter $\lambda^*$
is due to
convexity and differentiability of (parts of) the 
the objective function of Problem.
We deliver the omitted technical details.
I proved the following lemma by myself.
\begin{lemma}
  \label{bw:cd:lem}
  Let $m\in\mathbb{N}$ and
  $g \,:\, \R^m \to \overline{\R}$ 
  be convex.
  Then 
  for all $y \in \R^m$ and $\varepsilon>0$ 
    \begin{gather}
      \label{7060_0}
      \inf_{\norm{\Delta}=\varepsilon} g(y+\Delta) - g(y) \ge 0 \quad
    \end{gather}
    implies
    the existence of  
    a global minimum
    $
    y^* \in \,\R^m
    $
    of $g$
    satisfying
    $
      \norm{y^* - y}_2 \le \varepsilon
    $.
\end{lemma}
\begin{proof}
  Let $B$ be the euclidian ball in $\R^m$.
  Since 
  $
  y
  \,
  +
  \,
  \varepsilon
  B
  $
  is convex, it contains a 
  local minimum  
  of $g$.
  Suppose towards a contradiction that
  $
    y^* 
    \ 
    \in 
    \ 
  y
  \,
  +
  \,
  \varepsilon
  B
  $
  is a local minimum, but not a global one, and
  \eqref{7060_0} is true.
  Then it holds
  \begin{gather}
    \label{7060_3}
    g(x) < g(y^*)
    \quad
    \text{for some}\ 
    x 
    \in 
    \R^m 
    \setminus 
    \left( 
  y
  \,
  +
  \,
  \varepsilon
  B
    \right)
  \,.
  \end{gather}
  Furthermore, since 
  $
  y
  \,
  +
  \,
  \varepsilon
  B
  $ is compact and contains $y^*$,
  the line segment connecting 
  $y^*$ and $x$
  intersects the boundary of 
  $y + \mathcal{C}$, that is,
  there exist
  $
    \theta \in (0,1)
  $
  and 
  $
    \Delta_x
  $
  with 
  $
    \norm{\Delta_x}_2=\varepsilon
  $
  such that
  \begin{gather}
    \label{7060_4}
    \theta x + (1 - \theta) y^* = y + \Delta_x
    \,.
  \end{gather}
    It follows
    \begin{align}
      \label{7060_5}
      \begin{split}
      g(y^*)
      \le
      g(y)
      \le
      g(y + \Delta_x)
      &=
      g(
        \theta x + (1 - \theta) y^*
      )
      \\
      &\le
      \theta g(x)
      + 
      (1 - \theta)
      g(y^*)
      <
      g(y^*)
      ,
      \end{split}
    \end{align}
    which is a contradiction.
    The first inequality is due to
    $y^*$ being a local minimum of $g$ in
    $
  y
  \,
  +
  \,
  \varepsilon
  B
    $,
    the second inequality is due to  
    \eqref{7060_0} being true,
    the equality is due to \eqref{7060_4},
    the third inequality is due to the convexity of $g$
    and the strict inequality is due to \eqref{7060_3}.
    Thus every local minimum of $g$ in
    $
  y
  \,
  +
  \,
  \varepsilon
  B
    $
    is also a global minimum.
    %It follows the right-hand side of \eqref{7060_0}.
\end{proof}
Since 
$
(\lambda_0^\dagger,\lambda^\dagger)
$
is a global minimum (in $\R^{N+1}$) of the 
objective function $G$ of Problem?, that is,
\begin{gather*}
  G(\lambda,\lambda_0)
  \ 
  :=
  \ 
    \frac{1}{N}
\sum_{i=1} 
  ^N
  \left[ 
    \,
  T_i
  \cdot
  f^*
  \!
  \left( 
\lambda_0
+
\inner
{B(X_i)}
{\lambda}
  \right)
  \ 
-
\ 
  \left( 
\lambda_0
+
\inner
{B(X_i)}
{\lambda}
  \right)
  \,
  \right]
  \ 
+
\ 
\inner
{\delta}
{|\lambda|}
\,.
\end{gather*}
an immediate consequence of Lemma~\ref{bw:cd:lem} is
\begin{gather*}
   \P
   \left[ 
     \norm
     {
      \lambda ^ \dagger
      -
      \lambda^*
     }_2
     \le
     \varepsilon
   \right]
   \
   \ge
   \ 
   \P
   \left[ 
     \inf _ { 
       \norm{
         (
     \Delta
     ,
     \Delta_0
         )
 } 
= \varepsilon }
     G
     (
     \lambda^*
      +
      \Delta
      ,
      \lambda^\dagger_0
      +
     \Delta_0
     )
     -
     G
     (
     \lambda^*,
      \lambda^\dagger_0
     )
     \ge 
     0
   \right]
   \,.
 \end{gather*}

 To prove Theorem~\ref{bw:cd:th}
 it thus suffices to prove
 \begin{lemma}
   Under the conditions of Theorem~\ref{bw:cd:th} it holds
   for all $\varepsilon>0$
\begin{gather}
   \P
   \left[ 
     \inf _ { 
       \norm{
         (
     \Delta
     ,
     \Delta_0
         )
 } 
= \varepsilon }
     G
     (
     \lambda^*
      +
      \Delta
      ,
      \lambda^\dagger_0
      +
     \Delta_0
     )
     -
     G
     (
     \lambda^*,
      \lambda^\dagger_0
     )
     \ge 
     0
   \right]
   \ 
   \to
   \ 
   1
   \qquad
   \text{for}
   \ 
   N\to\infty
   \,.
\end{gather}
 \end{lemma}
 \begin{proof}
  Recall the objective function $G$ of Problem? 
\begin{gather*}
  G(\lambda,\lambda_0)
  \ 
  :=
  \ 
    \frac{1}{N}
\sum_{i=1} 
  ^N
  \left[ 
    \,
  T_i
  \cdot
  f^*
  \!
  \left( 
\lambda_0
+
\inner
{B(X_i)}
{\lambda}
  \right)
  \ 
-
\ 
  \left( 
\lambda_0
+
\inner
{B(X_i)}
{\lambda}
  \right)
  \,
  \right]
  \ 
+
\ 
\inner
{\delta}
{|\lambda|}
\,.
\end{gather*}
   Since we assume the convex conjugate $f^*$ to be differentiable
   (it always convex),
without the last term, $G$ would be a differentiable convex function.

It is well know that a differentiable convex functions $g$ satisfies
  \begin{gather}
    \label{cv:ts:concD}
    g(x)-g(y)
    \ge
    \nabla
    g(y)^\top
    (x-y)
    \qquad 
    \text{for all}\ 
    x,y\,.
  \end{gather}
  The gradient of
  \begin{gather}
    g := 
    (\lambda,\lambda_0)
    \mapsto
    \frac{1}{N}
\sum_{i=1} 
  ^N
  \left[ 
    \,
  T_i
  \cdot
  f^*
  \!
  \left( 
\lambda_0
+
\inner
{B(X_i)}
{\lambda}
  \right)
  \ 
-
\ 
  \left( 
\lambda_0
+
\inner
{B(X_i)}
{\lambda}
  \right)
  \,
  \right]
  \end{gather}
  is 

  \begin{gather}
    \nabla
    g
    =
    (\lambda,\lambda_0)
    \mapsto
\frac{1}{N}
\sum_{i=1} 
  ^N
  \left[ 
    \,
  T_i
  \cdot
  (f^{'})^{-1}
  \!
  \left( 
\lambda_0
+
\inner
{B(X_i)}
{\lambda}
  \right)
  \ 
-
\ 
1
  \right]
  [
  B(X_i)^\top
  ,
  1
  ]^\top
  \end{gather}
  Thus
\begin{align}
  \label{c:1}
  \begin{split}
     &
   G
     (
     \lambda^*
      +
      \Delta
      ,
      \lambda^\dagger_0
      +
     \Delta_0
     )
     \ 
     -
     \ 
     G
     (
     \lambda^*,
      \lambda^\dagger_0
     )
         %%%%%%%%%%%%% 1 %%%%%%%%%%%%%%
     \\
     &
     \quad
     \ge
     -
     \frac{1}{N}
     \sum_{i=1}^{N} 
     \left[ 
       B(X_i)^\top,
       1
     \right]
     \cdot
     \begin{bmatrix}
       \Delta\\
       \Delta_0
     \end{bmatrix}
     \left( 
       1
       \ 
       -
       \ 
     T_i
     \cdot
     (f^{'})^{-1}
     \left( 
       \inner
       {B(X_i)}
       {\lambda^*}
       +
      \lambda^\dagger_0
     \right)
     \right)
     \\
     &
     \qquad
     +
     \ 
     \inner
     {\delta}
     {
       |\lambda^*+\Delta|
       -
       |\lambda^*|
     }
     \,.
   \end{split}
\end{align}
Next, we fix $
\tilde{\varepsilon}
>0
$
and establish in \eqref{c:1} the lower bound
$
-
\tilde{\varepsilon}
$
with probability going to $1$ as $N\to\infty$.
Then we conclude that this holds for all $\tilde{\varepsilon}>0$.
The measurability of
$
G
     (
     \lambda^*
      +
      \Delta
      ,
      \lambda^\dagger_0
      +
     \Delta_0
     )
     \ 
     -
     \ 
     G
     (
     \lambda^*,
      \lambda^\dagger_0
     )
$
will give us the lower bound 0 in \eqref{c:1} with probability going to $1$.

In \eqref{c:1} we control the \textbf{first term} by (what?) 
and the \textbf{second term} by $\norm{\delta}_1$.
\subsection*{First Term}
We note, that by $\norm{B(x)}_2\le 1$ for all $x\in\mathcal{X}$ and the Cauchy-Schwarz inequality
it holds
\begin{gather}
  \label{c:first:0}
      \left[ 
       B(X_i)^\top,
       1
     \right]
     \cdot
     \begin{bmatrix}
       \Delta\\
       \Delta_0
     \end{bmatrix}
     \ 
     \lesssim
     \ 
     \norm{(\Delta,\Delta_0)}
     =\varepsilon
     \,.
\end{gather}
Next, we see that
\begin{align}
  \label{c:first:1}
  \begin{split}
  &
     \frac{1}{N}
     \sum_{i=1}^{N} 
     \left( 
       1
       \ 
       -
       \ 
     T_i
     \cdot
     (f^{'})^{-1}
     \left( 
       \inner
       {B(X_i)}
       {\lambda^*}
       +
      \lambda^\dagger_0
     \right)
     \right)
     \\
     &
     \ 
     \lesssim
     \ 
     \frac{1}{N}
     \sum_{i=1}^{N} 
     \left|
     1
     -
     \frac{T_i}{\pi(X_i)}
     \right|
      \ 
     +
      \ 
     \frac{1}{N}
     \sum_{i=1}^{N} 
     \left| 
        \inner
       {B(X_i)}
       {\lambda^*}
       +
      \lambda^\dagger_0
      \ 
        -
        \ 
        f^{'}
        \left( 
          \frac{1}{\pi(X_i)}
        \right)
     \right|
     \\
     &
     \ 
     =:
     \ 
     S_N
     \
     +
     \ 
     M_N
     \,.
\end{split}
\end{align}
With $
\tilde{\varepsilon}
>0
$
fixed previously,
we want to
establish the upper bound
$
\tilde{\varepsilon}
/
(2\varepsilon)
$
with probability going to $1$ as $N\to\infty$.
First, we bound $S_N$.
By the properties of conditional expectation it holds
\begin{gather*}
  \E
  \left[ 
    \frac{T}{\pi(X)}
  \right]
  =
  \E
  \left[ 
    \frac{\E[T|X]}{\pi(X)}
  \right]
  =1
  \,.
\end{gather*}
Also
\begin{gather}
  \E
  \left[ 
    \left| 
    1
    -
    \frac{T}{\pi(X)}
    \right|
  \right]
  \le
  1
  +
  \E
  \left[ 
    \frac{T}{\pi(X)}
  \right]
  =
  2
  \,.
\end{gather}
Thus Etemadi's ($\mathcal{L}_1$ version) strong law of large numbers (cf.\cite[Theorem~5.17]{Klenke2020}) applies
to $S_N$, that is,
$S_N \le 
\tilde{\varepsilon}
/
(4\varepsilon)
$
with probability going to 1.
Next, we bound $M_N$.
Recall that $\sum_{k=1}^{N}B_k(x)=1$ for all $x\in\mathcal{X}$. Thus
\begin{align*}
  &
\left| 
        \inner
       {B(X_i)}
       {\lambda^*}
      \ 
       +
      \ 
      \lambda^\dagger_0
      \ 
        -
        \ 
        f^{'}
        \left( 
          \frac{1}{\pi(X_i)}
     \right)
\right|
      \\
      &
      \ 
      =
      \ 
      \left| 
      \sum_{k=1}^{N} 
      B_k(X_i)
      \left( 
        f^{'}
        \left( 
          \frac{1}{\pi(X_k)}
     \right)
  -
  \ 
  \lambda_0^\dagger
      \right)
      \ 
      +
      \ 
      \lambda_0^\dagger
      -
        f^{'}
        \left( 
          \frac{1}{\pi(X_i)}
     \right)
      \right|
      \\
      &
      \ 
      =
      \ 
      \left| 
      \sum_{k=1}^{N} 
      B_k(X_i)
      \left( 
        f^{'}
        \left( 
          \frac{1}{\pi(X_k)}
     \right)
     -
        f^{'}
        \left( 
          \frac{1}{\pi(X_i)}
     \right)
      \right)
      \right|
      \\
      &
      \ 
      \le
      \ 
      \sum_{k=1}^{N} 
      \frac{1_{\left\{ X_k\in A_N(X_i) \right\}}}
      {
        \sum_{j=1}^{N} 
1_{\left\{ X_j\in A_N(X_i) \right\}}
      }
      \left| 
      \left( 
        f^{'}
        \left( 
          \frac{1}{\pi(X_k)}
     \right)
     -
        f^{'}
        \left( 
          \frac{1}{\pi(X_i)}
     \right)
      \right)
      \right|
      \\
      &
      \ 
      \le
      \ 
      \omega
      \left( f^{'}\circ (x\mapsto1/x)\circ \pi,h_N \right)
      \to
      0
      \qquad
      \text{almost surely}
      \,,
\end{align*}
where $\omega$ is the modulus of continuity.
The convergence to 0 is due to $f^{'}$ being continuous, $\pi(x)\in(0,1)$ for all $x\in\mathcal{X}$ and the (assumed) continuity of $\pi$.
Indeed, by $h_N\to 0$ for $N\to\infty$ it follows
$
      \omega
      \left( f^{'}\circ (x\mapsto1/x)\circ \pi,h_N \right)
      \to
      0
$
.
Note, that this works because of the partitioning.
We conclude, that $
M_N\le
\tilde{\varepsilon}
/
(4\varepsilon)
$
with probability going to 1.

This establishes the desired bound of 
$
\tilde{\varepsilon}/(2\varepsilon)
$
in \eqref{c:first:1}.
Together with \eqref{c:first:0}
we conclude that the \textbf{first term} 
in
\eqref{c:1}
is bounded below by
$
-
\tilde{\varepsilon}/2
$
with probability going to $1$ as $N\to\infty$.
\subsection*{Second Term}
It holds
\begin{gather*}
  |x+y|-|x|\ge
  -|y|
  \qquad
  \text{for all}\ 
  x,y
  \,.
\end{gather*}
Since
$\delta\ge 0$
we get
\begin{align*}
  &
     \inner
     {\delta}
     {
       |\lambda^*+\Delta|
       -
       |\lambda^*|
     }
     \\
     &
     \ 
     \ge
     \ 
     -
     \inner{\delta}
     {|\Delta|}
     \ 
     \ge
     \ 
     -
     \norm{\delta}_1
     \norm{\Delta}_\infty
     \ 
     \ge
     \ 
     -
     \norm{\delta}_1
     \norm{(\Delta,\Delta_0)}_2
     \ 
     \ge
     \ 
     -
     \norm{\delta}_1
     \varepsilon
     \ 
     \ge
     \ 
     -
     \tilde{\varepsilon}/2
     \,,
\end{align*}
with probability going to $1$ as $N\to \infty$.
The convergence is due to $\norm{\delta}_1$ converging to $0$ in probability.
\subsection*{Conclusion}
With the analysis of the \textbf{first} and \textbf{second term} in
\eqref{c:1} we conclude
\begin{gather}
  G
     (
     \lambda^*
      +
      \Delta
      ,
      \lambda^\dagger_0
      +
     \Delta_0
     )
     \ 
     -
     \ 
     G
     (
     \lambda^*,
      \lambda^\dagger_0
     )
     \ge
     -
     \tilde{\varepsilon}
\end{gather}
with probability going to $1$ as $N\to \infty$.
A closer look reveils the measurability of 
$
  G
     (
     \lambda^*
      +
      \Delta
      ,
      \lambda^\dagger_0
      +
     \Delta_0
     )
     \ 
     -
     \ 
     G
     (
     \lambda^*,
      \lambda^\dagger_0
     )
$.
Since this holds true for all $\tilde{\varepsilon}>0$ we get
\begin{gather}
  G
     (
     \lambda^*
      +
      \Delta
      ,
      \lambda^\dagger_0
      +
     \Delta_0
     )
     \ 
     -
     \ 
     G
     (
     \lambda^*,
      \lambda^\dagger_0
     )
     \ge
     0
\end{gather}
with probability going to $1$ as $N\to \infty$.
But this holds for all 
$
(\Delta,\Delta_0)
$
with 
$
\norm{
(\Delta,\Delta_0)
}
=\varepsilon
$. Thus
\begin{gather}
   \inf _ { 
       \norm{
         (
     \Delta
     ,
     \Delta_0
         )
 } 
= \varepsilon }
     G
     (
     \lambda^*
      +
      \Delta
      ,
      \lambda^\dagger_0
      +
     \Delta_0
     )
     -
     G
     (
     \lambda^*,
      \lambda^\dagger_0
     )
     \ge 
     0
\end{gather}
with probability going to $1$ as $N\to \infty$.
We see, that this holds for all $\varepsilon>0$. This finish the proof.
 \end{proof}

\end{proof}

