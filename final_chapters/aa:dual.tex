\begin{assumption}
  \label{aa:assumption:overlap}
  The propensity score function satisfies
  $\pi(x)\in(0,1)$ for all $x\in\mathcal{X}$.
\end{assumption}
\begin{assumption}
  \label{aa:assumption:ps_continuous}
  The propensity score function is continuous in $\mathcal{X}$.
\end{assumption}
\subsection*{My Contribution}
I found out, that consistency for the dual variable is enough to prove later results. This simplifies the proof. 
In \cite{Wang2019} they use a quadratic Taylor expansion to obtain learning rates. I found out, that a simpler mean value result for differentiable convex functions is sufficient to proof consistency. 
Since I work with partitioning estimates, I found a suitable oracle parameter.
I prove an (extended) lemma which is central but the details were omited.

Throughout this section we assume for all $N\in\mathbb{N}$ the existence of an 
optimal solution 
$(\lambda_0^\dagger,\lambda^\dagger)$
to Problem?
We define the oracle parameter $\lambda^*\in\R^N$ to be the vector with coordinates
\begin{gather}
  \label{oracle_1}
  \lambda^*_k
  \ 
  :=
  \ 
  f^{'}
  \left( 
    \frac{1}{\pi(X_k)}
  \right)
  \qquad
  \text{for all}\ 
  k\in \left\{ 1,\ldots,N \right\}
  \,,
\end{gather}
where $\pi(x)=\P[T=1|X=x]$ is the \textbf{propensity score} at $x\in\mathcal{X}$. Why this choice? 
%First, the $\lambda_0^\dagger$ part is unimportant. We need it to eliminate the same factor in
%\begin{gather}
%  w(x):=
%  (
%  f^{'}
%  )^{-1}
%  \left( \inner{B(x)}{\lambda^\dagger}+\lambda_0^\dagger \right)
%\end{gather}
%that is 
%\begin{gather}
%  \inner{B(x)}{\lambda^*}+\lambda_0^\dagger 
%  =
%\sum_{k=1}^{N} 
%B_k(x)
%  f^{'}
%  \left( 
%    \frac{1}{\pi(X_k)}
%  \right)
%  \,.
%\end{gather}
With hindsight, we need
for all $i\in \left\{ 1,\ldots,N \right\}$
\begin{gather}
\left| 
\inner{B(X_i)}
{\lambda^*}
  -
  f^{'}
  \left( 
    \frac{1}{\pi(X_i)}
  \right)
\right|
\ 
\le
\ 
      \omega
      \left( f^{'}\circ (x\mapsto1/x)\circ \pi,h_N \right)
      \,.
\end{gather}
Consequently, if $\pi$ is continuous and positive (not 0) on $\mathcal{X}$ and the width of the partition $h_N$ converges to 0, we get 
\begin{gather}
 \left| 
  \inner{B(X_i)}{\lambda^*}
  -
  f^{'}
  \left( 
    \frac{1}{\pi(X_k)}
  \right)
 \right| 
 \
 \to
 \ 
 0
 \qquad
 \text{almost surely.}
\end{gather}
We need this to bound the term $M_N$ in \eqref{c:first:1}.
The first big goal is to prove the following proposition.
\begin{proposition}
  \label{bw:cd:th}
Define the oracle parameter $\lambda^*$ as in \eqref{oracle_1}.
Furthermore, assume that the propensity score function is continuous and positive on $\mathcal{X}$.
Then for all $\varepsilon>0$ with probability going to 1 there exists an optimal solution
     $(\lambda^\dagger,\lambda_0^\dagger)$
     to Problem ? with
     $
     \norm
     {
       (\lambda^\dagger,\lambda_0^\dagger)
      -
      (\lambda^*,0)
     }_2
     \le
     \varepsilon
     $.
\end{proposition}
\begin{remark}
  In the analysis of the next sections will assume existence of an optimal solution to Problem?
  The purpose of Proposition~\ref{bw:cd:th} is therefore twofold. 
  First, it ensures the existence of an optimal solution with probability going to 1 as $N\to\infty$.
  This shows that the assumption of existence is (at least for large $N$) likely met.
  Nevertheless, it is beyond the scope of my thesis to investigate feasibility of Problem ? in more detail. 
  Second, if a solution exists it converges to the oracle parameter $\lambda^*$ in probability. This ensures the consistency of the weights for the inverse propensity score.
\end{remark}

We use a hint from the last display of~\cite[p.22]{Wang2019}.
The high-level idea is that the existence of the optimal solution and its proximity to the oracle parameter can be analysed by the objective function.
\begin{lemma}
  \label{bw:cd:lem}
  Let $m\in\mathbb{N}$ and
  $g \,:\, \R^m \to \overline{\R}$ 
  be convex.
  Then 
  for all $y \in \R^m$ and $\varepsilon>0$ 
    \begin{gather}
      \label{7060_0}
      \inf_{\norm{\Delta}=\varepsilon} g(y+\Delta) - g(y) \ge 0 \quad
    \end{gather}
    implies
    the existence of  
    a global minimum
    $
    y^* \in \,\R^m
    $
    of $g$
    satisfying
    $
      \norm{y^* - y}_2 \le \varepsilon
    $.
\end{lemma}
\begin{proof}
  Let $B$ be the euclidian ball in $\R^m$.
  Since 
  $
  y
  \,
  +
  \,
  \varepsilon
  B
  $
  is convex, it contains a 
  local minimum  
  of $g$.
  Suppose towards a contradiction that
  $
    y^* 
    \ 
    \in 
    \ 
  y
  \,
  +
  \,
  \varepsilon
  B
  $
  is a local minimum, but not a global one, and
  \eqref{7060_0} is true.
  Then it holds
  \begin{gather}
    \label{7060_3}
    g(x) < g(y^*)
    \quad
    \text{for some}\ 
    x 
    \in 
    \R^m 
    \setminus 
    \left( 
  y
  \,
  +
  \,
  \varepsilon
  B
    \right)
  \,.
  \end{gather}
  Furthermore, since 
  $
  y
  \,
  +
  \,
  \varepsilon
  B
  $ is compact and contains $y^*$,
  the line segment connecting 
  $y^*$ and $x$
  intersects the boundary of 
  $y + \mathcal{C}$, that is,
  there exist
  $
    \theta \in (0,1)
  $
  and 
  $
    \Delta_x
  $
  with 
  $
    \norm{\Delta_x}_2=\varepsilon
  $
  such that
  \begin{gather}
    \label{7060_4}
    \theta x + (1 - \theta) y^* = y + \Delta_x
    \,.
  \end{gather}
    It follows
    \begin{align}
      \label{7060_5}
      \begin{split}
      g(y^*)
      \le
      g(y)
      \le
      g(y + \Delta_x)
      &=
      g(
        \theta x + (1 - \theta) y^*
      )
      \\
      &\le
      \theta g(x)
      + 
      (1 - \theta)
      g(y^*)
      <
      g(y^*)
      ,
      \end{split}
    \end{align}
    which is a contradiction.
    The first inequality is due to
    $y^*$ being a local minimum of $g$ in
    $
  y
  \,
  +
  \,
  \varepsilon
  B
    $,
    the second inequality is due to  
    \eqref{7060_0} being true,
    the equality is due to \eqref{7060_4},
    the third inequality is due to the convexity of $g$
    and the strict inequality is due to \eqref{7060_3}.
    Thus every local minimum of $g$ in
    $
  y
  \,
  +
  \,
  \varepsilon
  B
    $
    is also a global minimum.
    %It follows the right-hand side of \eqref{7060_0}.
\end{proof}
\begin{remark}
  The hint from \cite[page 22]{Wang2019}
  states
  \eqref{7060_0}
  with strict inequality.
  They base their subsequent analysis on a quadratic Taylor expansion and aim to prove \eqref{7060_0} with strict inequality.
  I show, that this approach is inefficient.  
  To do that, I need Lemma~\ref{bw:cd:lem} exactly as stated - with $\ge$ in \eqref{7060_0}.
\end{remark}
Lemma~\ref{bw:cd:lem} motivates the following lemma.
 \begin{lemma}
   \label{bw:cd:lem2}
   Under the conditions of Theorem~\ref{bw:cd:th} it holds
   for all $\varepsilon>0$
\begin{gather}
   \P
   \left[ 
     \inf _ { 
       \norm{
         (
     \Delta
     ,
     \Delta_0
         )
 } 
= \varepsilon }
     G
     (
     \lambda^*
      +
      \Delta
      ,
     \Delta_0
     )
     -
     G
     (
     \lambda^*,
     0
     )
     \ge 
     0
   \right]
   \ 
   \to
   \ 
   1
   \qquad
   \text{for}
   \ 
   N\to\infty
   \,.
\end{gather}
 \end{lemma}
 \begin{proof}
  Recall the objective function $G$ of Problem? 
\begin{align*}
  G(\lambda,\lambda_0)
  &
  \ 
  =
  \ 
    \frac{1}{N}
\sum_{i=1} 
  ^N
  \left[ 
    \,
  T_i
  \cdot
  f^*
  \!
  \left( 
\lambda_0
+
\inner
{B(X_i)}
{\lambda}
  \right)
  \ 
-
\ 
  \left( 
\lambda_0
+
\inner
{B(X_i)}
{\lambda}
  \right)
  \,
  \right]
  \ 
+
\ 
\inner
{\delta}
{|\lambda|}
\,
\\
  &
\ 
=
\ 
g(\lambda,\lambda_0)
\ 
+
\ 
\inner
{\delta}
{|\lambda|}
\,
\end{align*}
with
  \begin{gather}
    g
    \ 
    := 
    \ 
    (\lambda,\lambda_0)
    \ 
    \mapsto
    \ 
    \frac{1}{N}
\sum_{i=1} 
  ^N
  \left[ 
    \,
  T_i
  \cdot
  f^*
  \!
  \left( 
\lambda_0
+
\inner
{B(X_i)}
{\lambda}
  \right)
  \ 
-
\ 
  \left( 
\lambda_0
+
\inner
{B(X_i)}
{\lambda}
  \right)
  \,
  \right]
  \,.
  \end{gather}
   Since we assume the convex conjugate $f^*$ of $f$ to be differentiable
   (it always convex),
   $g$ is differentiable convex function with gradient 
  \begin{gather*}
    \nabla
    g
    \ 
    =
    \ 
    (\lambda,\lambda_0)
    \ 
    \mapsto
    \ 
\frac{1}{N}
\sum_{i=1} 
  ^N
  \left[ 
    \,
  T_i
  \cdot
  (f^{'})^{-1}
  \!
  \left( 
\lambda_0
+
\inner
{B(X_i)}
{\lambda}
  \right)
  \ 
-
\ 
1
  \right]
  [
  B(X_i)^\top
  ,
  1
  ]^\top
  \,.
  \end{gather*}
It is well know that a differentiable convex functions $g$ satisfies
  \begin{gather*}
    g(x)
    \ 
    -
    \ 
    g(y)
    \ 
    \ge
    \ 
    \nabla
    g(y)^\top
    (x-y)
    \qquad 
    \text{for all}\ 
    x,y\,.
  \end{gather*}
Thus 
\begin{align}
  \label{c:1}
  \begin{split}
     &
   G
     (
     \lambda^*
      +
      \Delta
      ,
     \Delta_0
     )
     \ 
     -
     \ 
     G
     (
     \lambda^*,
     0
     )
         %%%%%%%%%%%%% 1 %%%%%%%%%%%%%%
     \\
     &
     \quad
     \ge
     -
     \frac{1}{N}
     \sum_{i=1}^{N} 
     \left[ 
       B(X_i)^\top,
       1
     \right]
     \cdot
     \begin{bmatrix}
       \Delta\\
       \Delta_0
     \end{bmatrix}
     \left( 
       1
       \ 
       -
       \ 
     T_i
     \cdot
     (f^{'})^{-1}
     \left( 
       \inner
       {B(X_i)}
       {\lambda^*}
     \right)
     \right)
     \\
     &
     \qquad
     +
     \ 
     \inner
     {\delta}
     {
       |\lambda^*+\Delta|
       -
       |\lambda^*|
     }
     \,.
   \end{split}
\end{align}

Next, we fix $
\tilde{\varepsilon}
>0
$
and establish in \eqref{c:1} the lower bound
$
-
\tilde{\varepsilon}
$
with probability going to $1$ as $N\to\infty$.
Then we conclude that this holds for all $\tilde{\varepsilon}>0$.
The measurability of
$
G
     (
     \lambda^*
      +
      \Delta
      ,
     \Delta_0
     )
     \ 
     -
     \ 
     G
     (
     \lambda^*,
     0
     )
$
will give us the lower bound 0 in \eqref{c:1} with probability going to $1$.

In \eqref{c:1} we control the \textbf{first term} by the law of large numbers 
and the \textbf{second term} by $\norm{\delta}_1\to 0$.
\subsection*{First Term}
We note, that by $\norm{B(x)}_2\le 1$ for all $x\in\mathcal{X}$ and the Cauchy-Schwarz inequality
it holds
\begin{gather}
  \label{c:first:0}
      \left[ 
       B(X_i)^\top,
       1
     \right]
     \cdot
     \begin{bmatrix}
       \Delta\\
       \Delta_0
     \end{bmatrix}
     \ 
     \lesssim
     \ 
     \norm{(\Delta,\Delta_0)}
     =\varepsilon
     \,.
\end{gather}
Next, we see that
\begin{align}
  \label{c:first:1}
  \begin{split}
  &
     \frac{1}{N}
     \sum_{i=1}^{N} 
     \left( 
       1
       \ 
       -
       \ 
     T_i
     \cdot
     (f^{'})^{-1}
     \left( 
       \inner
       {B(X_i)}
       {\lambda^*}
     \right)
     \right)
     \\
     &
     \ 
     \lesssim
     \ 
     \frac{1}{N}
     \sum_{i=1}^{N} 
     \left|
     1
     -
     \frac{T_i}{\pi(X_i)}
     \right|
      \ 
     +
      \ 
     \frac{1}{N}
     \sum_{i=1}^{N} 
     \left| 
        \inner
       {B(X_i)}
       {\lambda^*}
      \ 
        -
        \ 
        f^{'}
        \left( 
          \frac{1}{\pi(X_i)}
        \right)
     \right|
     \\
     &
     \ 
     =:
     \ 
     S_N
     \
     +
     \ 
     M_N
     \,.
\end{split}
\end{align}
With $
\tilde{\varepsilon}
>0
$
fixed previously,
we want to
establish the upper bound
$
\tilde{\varepsilon}
/
(2\varepsilon)
$
with probability going to $1$ as $N\to\infty$.
First, we bound $S_N$.
By the properties of conditional expectation it holds
\begin{gather*}
  \E
  \left[ 
    \frac{T}{\pi(X)}
  \right]
  =
  \E
  \left[ 
    \frac{\E[T|X]}{\pi(X)}
  \right]
  =1
  \,.
\end{gather*}
Also
\begin{gather}
  \E
  \left[ 
    \left| 
    1
    -
    \frac{T}{\pi(X)}
    \right|
  \right]
  \le
  1
  +
  \E
  \left[ 
    \frac{T}{\pi(X)}
  \right]
  =
  2
  \,.
\end{gather}
Thus Etemadi's ($\mathcal{L}_1$ version) strong law of large numbers (cf.\cite[Theorem~5.17]{Klenke2020}) applies
to $S_N$, that is,
$S_N \le 
\tilde{\varepsilon}
/
(4\varepsilon)
$
with probability going to 1.

Next, we bound $M_N$.
By \eqref{basis_conv_comb} it holds
\begin{align}
  \label{part_trick}
  \begin{split}
  &
\left| 
        \inner
       {B(X_i)}
       {\lambda^*}
       \ 
        -
        \ 
        f^{'}
        \left( 
          \frac{1}{\pi(X_i)}
     \right)
\right|
      \\
      &
      \ 
      =
      \ 
      \left| 
      \sum_{k=1}^{N} 
      B_k(X_i)
      \cdot
        f^{'}
        \left( 
          \frac{1}{\pi(X_k)}
          \right)
      \ 
      -
      \ 
        f^{'}
        \left( 
          \frac{1}{\pi(X_i)}
     \right)
      \right|
      \\
      &
      \ 
      =
      \ 
      \left| 
      \sum_{k=1}^{N} 
      B_k(X_i)
      \left( 
        f^{'}
        \left( 
          \frac{1}{\pi(X_k)}
     \right)
     -
        f^{'}
        \left( 
          \frac{1}{\pi(X_i)}
     \right)
      \right)
      \right|
      \\
      &
      \ 
      \le
      \ 
      \sum_{k=1}^{N} 
      \frac{1_{\left\{ X_k\in A_N(X_i) \right\}}}
      {
        \sum_{j=1}^{N} 
1_{\left\{ X_j\in A_N(X_i) \right\}}
      }
      \left| 
        f^{'}
        \left( 
          \frac{1}{\pi(X_k)}
     \right)
     -
        f^{'}
        \left( 
          \frac{1}{\pi(X_i)}
     \right)
      \right|
      \\
      &
      \ 
      \le
      \ 
      \omega
      \left( f^{'}\circ (x\mapsto1/x)\circ \pi,h_N \right)
      \ 
      \to
      \ 
      0
      \,,
\end{split}
\end{align}
where $\omega$ is the modulus of continuity.
The convergence to 0 is due to the $f^{'}$ being continuous, $\pi(x)\in(0,1)$ for all $x\in\mathcal{X}$, the (assumed) continuity of $\pi$ and $h_N\to 0$ for $N\to\infty$.
We conclude, that $
M_N\le
\tilde{\varepsilon}
/
(4\varepsilon)
$
with probability going to 1.

This establishes the desired bound of 
$
\tilde{\varepsilon}/(2\varepsilon)
$
in \eqref{c:first:1}.
Together with \eqref{c:first:0}
we conclude that the \textbf{first term} 
in
\eqref{c:1}
is bounded below by
$
-
\tilde{\varepsilon}/2
$
with probability going to $1$ as $N\to\infty$.
\subsection*{Second Term}
It holds
\begin{gather*}
  |x+y|-|x|\ge
  -|y|
  \qquad
  \text{for all}\ 
  x,y\in\R
  \,.
\end{gather*}
Since
$\delta\ge 0$
we get
\begin{align*}
  &
     \inner
     {\delta}
     {
       |\lambda^*+\Delta|
       -
       |\lambda^*|
     }
     \\
     &
     \ 
     \ge
     \ 
     -
     \inner{\delta}
     {|\Delta|}
     \ 
     \ge
     \ 
     -
     \norm{\delta}_1
     \norm{\Delta}_\infty
     \ 
     \ge
     \ 
     -
     \norm{\delta}_1
     \norm{(\Delta,\Delta_0)}_2
     \ 
     \ge
     \ 
     -
     \norm{\delta}_1
     \varepsilon
     \ 
     \ge
     \ 
     -
     \tilde{\varepsilon}/2
     \,,
\end{align*}
with probability going to $1$ as $N\to \infty$.
The convergence is due to $\norm{\delta}_1$ converging to $0$ in probability.
\subsection*{Conclusion}
With the analysis of the \textbf{first} and \textbf{second term} in
\eqref{c:1} we conclude
\begin{gather}
  \label{c:7}
  G
     (
     \lambda^*
      +
      \Delta
      ,
     \Delta_0
     )
     \ 
     -
     \ 
     G
     (
     \lambda^*,
     0
     )
     \ge
     -
     \tilde{\varepsilon}
\end{gather}
with probability going to $1$ as $N\to \infty$.
But this holds for all $\tilde{\varepsilon}>0$.
A closer look reveals that
$
  G
     (
     \lambda^*
      +
      \Delta
      ,
     \Delta_0
     )
     \ 
     -
     \ 
     G
     (
     \lambda^*,
     0
     )
$ 
is measurable.
Indeed, this holds because $X$,$T$,$B(X)$ and $\lambda^*$ are measurable and $f^*$ is continuous.
Since \eqref{c:7} holds true for all $\tilde{\varepsilon}>0$, it thus follows 
\begin{gather}
  G
     (
     \lambda^*
      +
      \Delta
      ,
     \Delta_0
     )
     \ 
     -
     \ 
     G
     (
     \lambda^*,
     0
     )
     \ge
     0
\end{gather}
with probability going to $1$ as $N\to \infty$.
But this holds for all 
$
(\Delta,\Delta_0)
$
with 
$
\norm{
(\Delta,\Delta_0)
}
=\varepsilon
$. Thus
\begin{gather}
   \inf _ { 
       \norm{
         (
     \Delta
     ,
     \Delta_0
         )
 } 
= \varepsilon }
     G
     (
     \lambda^*
      +
      \Delta
      ,
     \Delta_0
     )
     -
     G
     (
     \lambda^*,
     0
     )
     \ge 
     0
\end{gather}
with probability going to $1$ as $N\to \infty$.
Finally, we see, that this holds for all $\varepsilon>0$. This finish the proof.
 \end{proof}

\begin{proof}
  \emph{(Proposition~\ref{bw:cd:th})}
An immediate consequence of Lemma~\ref{bw:cd:lem} is
that for all $\varepsilon>0$ it holds
\begin{gather*}
   \P
   \left[ 
     \text{There exists an optimal solution $(\lambda^\dagger,\lambda_0^\dagger)$ to Problem? with}
     \ 
     \norm
     {
       (\lambda^\dagger,\lambda_0^\dagger)
      -
      (\lambda^*,0)
     }_2
     \le
     \varepsilon
   \right]
   \\
   \
   \ge
   \ 
   \P
   \left[ 
     \inf _ { 
       \norm{
         (
     \Delta
     ,
     \Delta_0
         )
 } 
= \varepsilon }
     G
     (
     \lambda^*
      +
      \Delta
      ,
     \Delta_0
     )
     -
     G
     (
     \lambda^*,
     0
     )
     \ge 
     0
   \right]
   \,.
 \end{gather*}
 Applying Lemma~\ref{bw:cd:lem2} finishes the proof.
\end{proof}

