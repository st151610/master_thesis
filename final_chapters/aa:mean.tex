\subsection*{Notation}
Throughout this section we use the following notation.
Let $F_{Y(1)}$ denote the distribution function of $Y(1)$, that is,
\begin{gather*}
  F_{Y(1)}
  \ 
  \colon
  \ 
  \R
  \ 
  \to
  \ 
  [0,1]
  \, 
  , 
  \qquad
  z
  \ 
  \mapsto
  \ 
  \P
  [
  Y(1)
  \le
  z
  ]
  \,.
\end{gather*}
Let $F_{Y(1)}(\cdot|x)$ denote the distribution function of $Y(1)$ conditional on $X=x\in\mathcal{X}$, that is,
\begin{gather*}
  F_{Y(1)}
  (z|x)
\ 
  =
\ 
  \P
  [
  Y(1)
  \le
  z
  \,
  |
  \,
  X=x
  ]
  \qquad
  \text{for all}\ 
  (z,x)\in\R\times\mathcal{X}
  \,.
\end{gather*}
We recall from Definition~\ref{def:weights_function} the weight function 
\begin{gather*}
  w
  \colon
  \mathcal{X}
  \times
  \R^{N+1}
  \to
  \R
  \,,\qquad
  \left( 
  x
  ,
  \lambda
  ,
  \lambda_0
  \right)
  \mapsto
    (
    f^{'}
    )^{-1}
    \left( 
      \inner{B(x)}{\lambda^\dagger}
      +
      \lambda_0^\dagger
    \right)
    \,,
\end{gather*}
and that, if 
  the optimal solution
  $(\lambda^\dagger,\lambda_0^\dagger)$
  to Problem ? exists, we write
  \begin{gather*}
    w^\dagger(x)
    =
    w(x,\lambda^\dagger,\lambda^\dagger_0)
    \qquad
    \text{for all}\ 
    x\in\mathcal{X}\,.
  \end{gather*}
We introduced the basis functions $(B_k)$ in ?. 
Let $\pi(\cdot)$ denote the propensity score function, that is,
\begin{gather*}
  \pi
  \colon
  \mathcal{X}
  \to
  [0,1]
  \,,
  \qquad
  x\mapsto
  \P[T=1\,|\,X=x]
  \,.
\end{gather*}
Let the symbol
$\lesssim$ denote the lesser-equal order multiplied by a generic constant $C>1$ that is independent of $N$ or any other quantitative assumption. We always take $C$ large enough.
For example
\begin{gather*}
  17\cdot f(x)\ \lesssim\  f(x)
  \,.
\end{gather*}
This helps keeping the complexity of notation at a (necessary) minimum.

\subsection*{Goal of this section}
We illustrate the flexibility of 
the weighted mean estimator by 
extending the method of \cite{Wang2019} to
estimates of 
the distribution function of $Y(1)$, that is, $F_{Y(1)}$.
For the asymptotic analysis of estimating the mean $\E[Y(1)]$ see \cite[Proof of Theorem~3]{Wang2019}.
To make this extension, the central observation is, that we can adapt the error decomposition in \cite[page 27]{Wang2019} 
to estimates of the distribution function $F_{Y(1)}$ of $Y(1)$.
We do this in Lemma~\ref{aa:mean:lemma_decomp}.
With this modification, we aim at proving
the convergence of
\begin{gather}
    \sqrt{N}
    \left( 
  \frac{1}{N}
    \sum_{i=1}^{n} 
    w^\dagger(X_i)
    \mathbf{1}\left\{ Y_i(T_i)\,\le\, z \right\}
    \ 
    -
    \ 
    F_{Y(1)}(z)
    \right)
    _{z\in\R}
    \,
  \end{gather}
  in
  $l^\infty(\R)$
  to a Gaussian process with mean 0 and covariance specified in Theorem~\ref{aa:mean:th}.
  We show, that this is possible under mild assumptions.
  We discuss this in the next section.
\subsection{Tools and Assumptions}
\begin{assumption}
  \label{aa:assumption:1}
  For any decreasing sequence
  $(\varepsilon_N)$ with $\varepsilon_N\to 0$ for $N\to\infty$,
there exists a sequence of (measurable) function classes
$(\mathcal{F}_N)$
with envelope functions
$(F_N)$,
satisfying 
for some $k<2$
\begin{gather*}
\norm{F_N}_{L^2(\P)}
\ 
\le
\ 
\varepsilon_N
\quad
\text{and}
\quad
  \log
  N_{[\,]}(\varepsilon,\mathcal{F}_N,\mathrm{L}_2(\P))
  \ 
  \lesssim
  \ 
  \left( 
  \frac{1}{\varepsilon}
  \right)^k
  \quad
  \text{for all}
  \ 
  N\in\mathbb{N}
  \,,
\end{gather*}
such that for all $(\lambda,\lambda_0)\in\R^{N+1}$ and for all $N\in\mathbb{N}$ the function
\begin{gather}
  \label{ghost_function}
  \mathcal{X}
  \ 
  \to
  \ 
  \R
  \,,
  \qquad
  x
  \ 
  \mapsto
  \ 
  \mathbf{1}{
    \left\{ 
      \sup_{y\in A_N(x)}
      \left| 
      w(y,\lambda,\lambda_0)
      -
      \frac{1}{\pi(y)}
      \right|
      \,
      \le
      \,
      \varepsilon_N
    \right\}
  }
  \left( 
      w(x,\lambda,\lambda_0)
      -
      \frac{1}{\pi(x)}
  \right)
\end{gather}
is contained in $\mathcal{F}_N$.
\end{assumption}
The next Lemma shows that mild assumptions on the regularity of the inverse propensity score implies Assumption~\ref{aa:assumption:1}.
To this end, we need notation from \cite[§2.7.1]{vaart2013}.
To this end, let for any vector $k\in\mathbb{N}_0^d \ (d\in\R)$
\begin{gather*}
  D^k
  \ :=\ 
  \frac
  {\partial^{\norm{k}_1}}
  {
    \partial^{k_1}x_1
    \cdots
    \partial^{k_d}x_d
  }
  \,,
\end{gather*}
and let $\lfloor a \rfloor$ be the greatest integer smaller than $a>0$.
For $\alpha>0$, a bounded set 
$\mathcal{X}\subset\R^d\ (d\in\mathbb{N})$
and
$M>0$, we define $C^\alpha_M(\mathcal{X})$ to be the space of all continuous functions $f\colon \mathcal{X}\to\R$ with
\begin{gather*}
  \max_{\norm{k}_1\le \alpha}\sup_{x\in\mathcal{X}}
  \left| D^k f(x) \right|
  \ 
  +
  \ 
  \max_{\norm{k}_1=\lfloor \alpha \rfloor}\sup_{x,y}
  \frac
  {
  \left|
  D^k f(x) 
  -
  D^k f(y) 
  \right|
  }
  {
    \norm{x-y}_2^{\alpha-\lfloor \alpha \rfloor}
  }
  \
  \le
  \ 
  M
  \,.
\end{gather*}
where the suprema in the second term are taken over all $x,y$ in the interior of $\mathcal{X}$ with $x\neq y$.
Furthermore, let
\begin{gather*}
  \mathcal{X}^1
  :=
  \left\{ 
    y\in\R^d
    \colon
    \norm{x-y}_2 <1
    \ 
    \text{for some}\ x\in\mathcal{X}
  \right\}
  \,.
\end{gather*}
We will employ the following result.
\begin{lemma}
  Let $\mathcal{P}=\left\{ A_1,A_2,\ldots \right\}$ be a partition of $\R^d$ into bounded, convex sets with non-empty interior, and let $\mathcal{F}$ be a class of functions $f\colon\R^d\to\R$ such that the restrictions $\mathcal{F}_{|A_j}$ belong to $C^\alpha_{M_j}(A_j)$
  for all $j\in\mathbb{N}$.
  Then there exists a constant $K$, depending only on $\alpha$, $V$, $r$ and $d$
  such that
  \begin{gather*}
    \log
    N_{[\,]}
    (
    \varepsilon
    ,
    \mathcal{F}
    ,
    L^r(\mathbf{Q})
    )
    \le
    K
    \left( \frac{1}{\varepsilon} \right)^V
    \left( 
      \sum_{j=1}^{\infty}
      \lambda(A_j^1)^{r/(V+r)}
      M_j^{Vr/(V+r)}
      \mathbf{Q}(A_j)^{V/(V+r)}
    \right)
    ^{(V+r)/r}
  \end{gather*}
  for every $\varepsilon>0$, $V\ge d/\alpha$, and probability measure $\mathbf{Q}$.
\end{lemma}
\begin{proof}
  \emph{\cite[Corollary~2.7.4]{vaart2013}}
\end{proof}
\pagebreak
\begin{lemma}
  Let $(\mathcal{P}_N)$ denote a sequence of qubic partitions
  $\mathcal{P}_N=\left\{ A_{N,1},A_{N,2},\ldots \right\}$ 
  of $\R^d$ 
  with decreasing width $(h_N)\subset(0,1]$ such that $h_N\to 0$ for $N\to\infty$.
  Furthermore, assume that there exists
  $\alpha>d/2$, where $\mathcal{X}\subseteq \R^d$, such
  that for 
  $V:=d/\alpha$
 and for all 
$
(j,N)\in\mathbb{N}^2
$
there exists 
$M_{N,j}\ge 1$ such that 
\begin{gather*}
  \frac{1}{\pi(\cdot)}
  \in C^\alpha_{M_{N,j}}(A_{N,j})
  \quad
  \text{and}
  \quad
  \sum_{j=1}^{\infty} 
  M_{N,j}^{2V/(V+2)}
  \P
  [
  X\in A_{N,j}
  ]^{V/(V+2)}
  \ 
  \lesssim
  \ 
  1
  \,.
\end{gather*}
It then holds the statement of Assumption~\ref{aa:assumption:1}.
\end{lemma}
\begin{remark}
  We can derive Assumption~\ref{aa:assumption:1} from regularity assumptions on the inverse propensity score 
  $1/\pi(\cdot)$ and the distribution of $X$.
  To this end, we want to employ \cite[Corollary~2.7.4]{vaart2013}.
  To do this, the crucial observation is, that
  \begin{gather*}
    w(\cdot,\lambda,\lambda_0)
    \quad
    \text{is constant on each cell}\ 
    A_N\in\mathcal{P}_N
    \,.
  \end{gather*}
  Thus, the regularity of the function \eqref{ghost_function}
  on each cell $A_N\in\mathcal{P}_N$ is decided by 
  $1/\pi(\cdot)$.
  Thus, we assume that there exists
  $\alpha>d/2$, where $\mathcal{X}\subseteq \R^d$, such
  that for 
  $V:=d/\alpha$.
 and for all 
$
(j,N)\in\mathbb{N}^2
$
there exists 
$M_{N,j}\ge \varepsilon_N$ such that 
\begin{gather*}
  \frac{1}{\pi(\cdot)}
  \in C^\alpha_{M_{N,j}}(A_{N,j})
  \quad
  \text{and}
  \quad
  \sum_{j=1}^{\infty} 
  M_{N,j}^{2V/(V+2)}
  \P
  [
  X\in A_{N,j}
  ]^{V/(V+2)}
  \ 
  \lesssim
  \ 
  1
  \,.
\end{gather*}
Then the function \ref{ghost_function}, restricted to $A_{N,j}$, is in 
$
  C^\alpha_{M_{N,j}}(A_{N,j})
$
for all 
$
(j,N)\in\mathbb{N}^2
$.
Then \cite[Corollary~2.7.4]{vaart2013} gives us the statement 
in
Assumption~\ref{aa:assumption:1}.
\end{remark}
\begin{ftheorem}
  \label{aa:mean:th}
  Under conditions 
the stochastic process
\begin{gather}
    \sqrt{N}
    \left( 
  \frac{1}{N}
    \sum_{i=1}^{n} 
    w_i^\dagger
    \mathbf{1}
    _{\left\{ Y_i\,\le\, z \right\}}
    \ 
    -
    \ 
    F_{Y(1)}(z)
    \right)
    _{z\in\R}
    \,
  \end{gather}
  converges in
  $l^\infty(\R)$
  to a Gaussian process with mean 0 and covariance
\begin{align}
  \label{cov:lp}
 \begin{split}
  &
  \mathbf{Cov}
  (z_1,z_2)
  \\
  &
  =\ 
  \E
  \left[ 
 \frac{
 F_{Y(1)}(z_1 \land z_2\,|\,X)
}{\pi(X)}
\ 
-
\ 
 \frac{1-\pi(X)}{\pi(X)}
 F_{Y(1)}(z_1|X)
 \cdot
 F_{Y(1)}(z_2|X)
  \right]
  \\
  &
  \qquad 
 -
 \ 
 F_{Y(1)}(z_1)
 \cdot
 F_{Y(1)}(z_2)
 \end{split}
\end{align}
\end{ftheorem}

\pagebreak
\begin{lemma}
  \label{aa:mean:lemma_decomp}
  It holds
  \begin{gather}
  \sqrt{N}
\left( 
    \frac{1}{N}
    \sum_{i=1}^{n} 
    w(X_i)
    \mathbf{1}{\left\{ Y_i \le z \right\}}
    \ 
    -
    \ 
    F_{Y(1)}(z)
    \right)
    _{z\in\R}
    \ 
    =
    \ 
    R_1
    \ 
    +
    \ 
    R_2
    \ 
    +
    \ 
    R_3
    \ 
    +
    \ 
    R_4
  \end{gather}
  with
\begin{align*}
  R_1
  &
  \ 
  :=
  \ 
  \sqrt{N}
  \sum_{k=1}^{N} 
  \left[ 
  \frac{1}{N}
  \left( 
    \sum_{i=1}^{n} 
    w(X_i)
    B_k(X_i)
    -
    \sum_{i=1}^{N} 
    B_k(X_i)
  \right)
  F_{Y(1)}(z|X_k)
  \right]
  _{z\in\R}
  \,,
  %%%% 1 %%%%
  \\
  R_2
  &
  \
  :=
  \ 
  \sqrt{N}
    \sum_{i=1}^{N} 
    \left[ 
  \frac{
    T_i\cdot w(X_i) -1 }{N}
    \left( 
  F_{Y(1)}(z|X_i)
    \ 
    -
    \ 
    \sum_{k=1}^{N} 
    B_k(X_i)
    \cdot
  F_{Y(1)}(z|X_k)
    \right)
    \right]
  _{z\in\R}
  \,,
  %%%% 2 %%%%
  \\
  R_3
  &
  \
  :=
  \ 
  \sqrt{N}
  \left( 
  \frac{1}{N}
    \sum_{i=1}^{N} 
    \left[ 
    T_i
    \left( 
    w(X_i) 
    -
    \frac{1}{\pi(X_i)}
    \right)
    \left( 
    \mathbf{1}{\left\{ Y_i \le z \right\}}
    -
  F_{Y(1)}(z|X_i)
    \right)
    \right]
  \right)
  _{z\in\R}
  \,,
  %%%% 3 %%%%
  \\
  R_4
  &
  \
  :=
  \ 
  \sqrt{N}
  \left( 
  \frac{1}{N}
    \sum_{i=1}^{N} 
    \frac{T_i}{\pi(X_i)}
    \left( 
    \mathbf{1}{\left\{ Y_i \le z \right\}}
    -
  F_{Y(1)}(z|X_i)
    \right)
    \ 
    +
    \ 
    \left( 
  F_{Y(1)}(z|X_i)
    -
  F_{Y(1)}(z)
    \right)
  \right)
  _{z\in\R}
  \,.
  \end{align*}
\end{lemma}
\nopagebreak
\begin{proof}
  We fix $z\in\R$.
  It holds
  \begin{align*}
    &
    \frac{1}{N}
    \sum_{i=1}^{N} 
    w^\dagger(X_i)
    \cdot
    T_i
    \cdot
    \mathbf{1}{\left\{ Y_i(T_i)\, \le\, z \right\}}
    \\
    &
    \ 
    =
    \ 
    \frac{1}{N}
    \sum_{i=1}^{N} 
    \left( 
    w^\dagger(X_i)
    -
    \frac{1}{\pi(X_i)}
    \right)
    T_i
    \cdot
    \mathbf{1}{\left\{ Y_i(T_i)\, \le\, z \right\}}
    \\
    &
    \quad 
    +
    \ 
    \frac{1}{N}
    \sum_{i=1}^{N} 
    \frac{T_i}{\pi(X_i)}
    \mathbf{1}{\left\{ Y_i(T_i)\, \le\, z \right\}}
    \\
    &
    \ 
    =
    \ 
    \frac{1}{N}
    \sum_{i=1}^{N} 
    \left( 
    w^\dagger(X_i)
    -
    \frac{1}{\pi(X_i)}
    \right)
    T_i
    \left( 
    \mathbf{1}{\left\{ Y_i(T_i)\, \le\, z \right\}}
    -
    F_{Y(1)}(z|X_i)
    \right)
    \\
    &
    \quad 
    +
    \ 
    \frac{1}{N}
    \sum_{i=1}^{N} 
    \frac{T_i}{\pi(X_i)}
    \left( 
    \mathbf{1}{\left\{ Y_i(T_i)\, \le\, z \right\}}
    -
    F_{Y(1)}(z|X_i)
    \right)
    \\
    &
    \qquad 
    +
    \ 
    \frac{1}{N}
    \sum_{i=1}^{N} 
    w^\dagger(X_i)\cdot T_i\cdot
    F_{Y(1)}(z|X_i)
    \\
    &
    \ 
    =
    \ 
    R_3(z)/\sqrt{N}
    \\
    &
    \quad 
    +
    \ 
    \frac{1}{N}
    \sum_{i=1}^{N} 
    \frac{T_i}{\pi(X_i)}
    \left( 
    \mathbf{1}{\left\{ Y_i(T_i)\, \le\, z \right\}}
    -
    F_{Y(1)}(z|X_i)
    \right)
    +
    \left( 
    F_{Y(1)}(z|X_i)
    -
    F_{Y(1)}(z)
    \right)
    \\
    &
    \qquad
    +
    \ 
    \frac{1}{N}
    \sum_{i=1}^{N} 
    \left( 
    w^\dagger(X_i)\cdot T_i
    \ 
    -
    \ 
    1
    \right)
    F_{Y(1)}(z|X_i)
    \\
    &
    \quad\qquad
    +
    \ 
    F_{Y(1)}(z)
    \\
    &
    \ 
    =
    \ 
    R_3(z)/\sqrt{N}
    \\
    &
    \quad
    +
    \ 
    R_4(z)/\sqrt{N}
    \\
    &
    \qquad
    +
    \ 
    \frac{1}{N}
    \sum_{i=1}^{N} 
    \left( 
    w^\dagger(X_i)\cdot T_i
    \ 
    -
    \ 
    1
    \right)
    \left( 
    F_{Y(1)}(z|X_i)
    -
    \sum_{k=1}^{N} 
    B_k(X_i)
    \cdot
  F_{Y(1)}(z|X_k)
    \right)
    \\
    &
    \quad\qquad
    +
    \ 
    \frac{1}{N}
    \sum_{i=1}^{N} 
    \left( 
    w^\dagger(X_i)\cdot T_i
    \ 
    -
    \ 
    1
    \right)
    \sum_{k=1}^{N} 
    B_k(X_i)
    \cdot
  F_{Y(1)}(z|X_k)
    \\
    &
    \qquad\qquad
    +
    \ 
    F_{Y(1)}(z)
\\
    &
    \ 
    =
    \ 
    R_3(z)/\sqrt{N}
    \\
    &
    \quad
    +
    \ 
    R_4(z)/\sqrt{N}
    \\
    &
    \qquad
    +
    \ 
    R_2(z)/\sqrt{N}
    \\
    &
    \quad\qquad
    +
    \ 
    \sum_{k=1}^{N} 
    \frac{1}{N}
    \sum_{i=1}^{N} 
    \left( 
    w^\dagger(X_i)\cdot T_i
    B_k(X_i)
    \ 
    -
    \ 
    B_k(X_i)
    \right)
    \cdot
  F_{Y(1)}(z|X_k)
    \\
    &
    \qquad\qquad
    +
    \ 
    F_{Y(1)}(z)
    \\
    &
    \ 
    =
    \ 
    \left( 
R_3(z)
    \ 
    +
    \ 
    R_4(z)
    \ 
    +
    \ 
    R_2(z)
    \ 
    +
    \ 
    R_1(z)
    \right)
    /\sqrt{N}
    \ 
    +
    \ 
    F_{Y(1)}(z)
    \,.
  \end{align*}
  This holds for all $z\in\R$.
  Multiplying with $\sqrt{N}$ yields the result.
\end{proof}
Let $F_{Y(1)}(z|x):=\P[Y(1)\le z|X=x]$ denote a conditional version of the distribution function of $Y(1)$ at $x\in\mathcal{X}$.
We also need the propensity score
$
  \pi(x):=\P[T=1|X=x]
$
and the weights function
$
  w(x)
  :=
  (
  f^{'}
  )^{-1}
  \left( 
    \inner{B(x)}{\lambda^\dagger}
    +
    \lambda_0^\dagger
  \right)
$.
\begin{lemma}
  \label{aa:mean:l:r1}
Let the weights function $w$ satisfy the box constraints in 
Problem~\ref{bw:1:primal} and 
$\sqrt{N}\norm{\delta}_1\overset{\P}{\to}0$.
Then it holds
$\sup_{z\in\R}|R_1(z)|\overset{\P}{\to}0$.
  \end{lemma}
\begin{proof}
%  By Theorem~\ref{dual_solution_th}
%  it holds $w_i^\dagger=w(X_i)$ for $i\in \left\{ 1,\ldots,n \right\}$, that is, for $i\le n$ we can identify $w(X_i)$ with the optimal
%  solution to 
%  problem~\ref{bw:1:primal}. 
%  Thus the constraints of the problem apply.
%   Let's bound $R_1$.
It holds
  \begin{align}
    \label{R_1:1}
    \begin{split}
    \sup_{z\in\R}
    \left| 
    R_1(z)
    \right|
    &
    \ 
    =
    \ 
    \sup_{z\in\R}
    \left| 
  \sqrt{N}
  \sum_{k=1}^{N} 
  \left[ 
  \frac{1}{N}
  \left( 
    \sum_{i=1}^{n} 
    w(X_i)
    B_k(X_i)
    -
    \sum_{i=1}^{N} 
    B_k(X_i)
  \right)
  F_{Y(1)}(z|X_k)
  \right]
    \right|
    \\
    &
    \ 
    \le
    \ 
  \sqrt{N}
  \sum_{k=1}^{N} 
  \left| 
  \frac{1}{N}
  \left( 
    \sum_{i=1}^{n} 
    w(X_i)
    B_k(X_i)
    -
    \sum_{i=1}^{N} 
    B_k(X_i)
  \right)
  \right|
    \sup_{z\in\R}
  F_{Y(1)}(z|X_k)
  \\
    &
    \ 
    \le
    \ 
  \sqrt{N}
  \norm{\delta}_1
    \end{split}
  \end{align}
  The last inequality is due to $F_{Y(1)}\in[0,1]$ and the assumption that $(w(X_i))$ satisfies the box constraints of Problem~\ref{bw:1:primal}.
  Since we assume 
$\sqrt{N}\norm{\delta}_1\overset{\P}{\to}0$
it holds
$\sup_{z\in\R}|R_1(z)|\overset{\P}{\to}0$.
\end{proof}
\begin{remark}
  We want to comment on the box constraints of Problem~\ref{bw:1:primal}, that is,
 \begin{gather*}
      \left| 
      \frac{1}{N} 
      \left( 
      \sum_{i = 1}^{n} 
      w(X_i)
      B_k(X_i)
      -
      \sum_{i=1}^{N} 
      B_k(X_i)
      \right)
    \right|
    \ 
    \le 
    \ 
    \delta_k
    \qquad
    \text{for all}\ 
    k \in \left\{ 1, \ldots, N \right\}
    \,.
  \end{gather*}
  Note, that the first sum goes over $\left\{ 1,\ldots,n \right\}$ while the second sum goes over $\left\{ 1,\ldots,N \right\}$.
  A second, equivalent version of the constraints is
  \begin{gather*}
      \left| 
      \frac{1}{N} 
      \left( 
      \sum_{i = 1}^{N} 
      T_i
      w(X_i)
      B_k(X_i)
      -
      \sum_{i=1}^{N} 
      B_k(X_i)
      \right)
    \right|
    \ 
    \le 
    \ 
    \delta_k
    \qquad
    \text{for all}\ 
    k \in \left\{ 1, \ldots, N \right\}
    \,.
  \end{gather*}
  Now both sums go over $\left\{ 1,\ldots,N \right\}$ and the
  indicator of treatment $T_i$ takes care that in the first sum only the terms with $i\le n$ are effective. 
  Having this flexibility with the versions helps. I regard the first version as suitable for non-probabilistic computations, although $n$ is of course a random variable. On the other hand, the second version is more honest, exactly telling the dependence on the indicator of treatment. This version is useful in probabilistic computations. 

  Also we want to comment on the assumption on $\norm{\delta}$.
  Playing around with norm equivalences we discover that 
  $\sqrt{N}\norm{\delta}_1\overset{\P}{\to}0$ for $N\to \infty$ is the weakest
  (natural) assumption to
  control $R_1$.
  Indeed, other ways to continue the second row in \eqref{R_1:1} are
  \begin{gather*}
    (\,\cdots)
    \ 
  \le
    \ 
  \sqrt{N}
  \norm{\delta}_2
  \left( 
  \sum_{k=1}^{N} 
  \left( 
    \sup_{z\in\R}
  F_{Y(1)}(z|X_k)
  \right)^2
\right)^{1/2}
\ 
\le
\ 
N
  \norm{\delta}_2\,,
  \end{gather*}
  by the Cauchy-Schwarz inequality and
  $
  F_{Y(1)}\in [0,1]
  $,
or
\begin{gather*}
  (\,\cdots)
  \ 
  \le
  \ 
  \sqrt{N}
  \norm{\delta}_\infty
  \sum_{k=1}^{N} 
    \sup_{z\in\R}
  F_{Y(1)}(z|X_k)
  \ 
  \le
  \ 
  N^{3/2}
  \norm{\delta}_\infty
  \,.
\end{gather*}
Since $\delta\in \R^N$, however, it holds
\begin{gather*}
  \sqrt{N}\norm{\delta}_1
  \ 
  \le
  \ 
  N\norm{\delta}_2
  \ 
  \le
  \ 
  N^{3/2}\norm{\delta}_\infty
  \,.
\end{gather*}
With hindsight, the assumption 
$\sqrt{N}\norm{\delta}_1\overset{\P}{\to}0$ for $N\to \infty$ 
  also 
  suffices 
  to control the second (or first) occurrence of a term, that we control by assumptions on $\norm{\delta}$.
This is the \textbf{second term} of \eqref{c:1}, where we estimate
\begin{gather*}
  \inner{\delta}{\left| \Delta \right|}
  \ 
  =
  \ 
  \sum_{k=1}^{N} 
  \delta_k
  \left| \Delta_k \right|
  \ 
  \le
  \ 
  \norm{\delta}_1
  \norm{\Delta}_\infty
  \ 
  \le
  \ 
  \norm{\delta}_1
  \norm{\Delta}_2
  \ 
  \le
  \ 
  \norm{\delta}_1
  \varepsilon
  \ 
  \overset{\P}{\to}
  \ 
  0
  \quad
  \text{for}\ 
  N\to \infty
  \,.
\end{gather*}

\end{remark}
  \begin{lemma}
    \label{aa:mean:l:r2}
    Let the conditions of Theorem~\ref{aa:weights:th} hold true.
    Furthermore assume, that the width of the partitioning estimate $h_N$ and a conditional version of the distribution function of $Y(1)$ satisfy
\begin{gather*}
  \sqrt{N}
  \sup_{z\in\R}
  \omega
  \left( 
    F_{Y(1)}(z|\cdot)
    ,h_N
  \right)
  \to
  0
  \qquad
  \text{for}\ 
  N\to \infty
  \,,
\end{gather*}
where $\omega$ is the modulus of continuity.
Then it holds
$\sup_{z\in\R}|R_2(z)|\overset{\P}{\to}0$.
  \end{lemma}
\begin{proof}
\begin{align*}
&
  \sup_{z\in\R}
  \left| 
  R_2(z)
  \right|
  \\
  &
  \ 
  \le
  \ 
  \sqrt{N}
    \sum_{i=1}^{N} 
    \left[ 
  \frac{
    T_i\cdot w(X_i) -1 }{N}
  \sup_{z\in\R}
    \left| 
  F_{Y(1)}(z|X_i)
    \ 
    -
    \ 
    \sum_{k=1}^{N} 
    B_k(X_i)
    \cdot
  F_{Y(1)}(z|X_k)
    \right|
    \right]
    \\
    &
  \ 
    \le
  \ 
    \sqrt{N}
  \sup_{z\in\R}
  \omega
  \left( 
    F_{Y(1)}(z|\cdot)
    ,h_N
  \right)
  \sum_{i=1}^{N} 
  \frac{
    T_i\cdot w(X_i) +1 }{N}
    \\
    &
  \ 
    =
  \ 
    2
    \sqrt{N}
  \sup_{z\in\R}
  \omega
  \left( 
    F_{Y(1)}(z|\cdot)
    ,h_N
  \right)
  \,.
\end{align*}
The equality is due to 
\begin{gather}
  1
  \ 
  =
  \ 
\frac{1}{N}\sum_{i=1}^{n}w_i^\dagger
  \ 
  =
  \ 
\frac{1}{N}\sum_{i=1}^{n}w(X_i)
  \ 
=
  \ 
\frac{1}{N}\sum_{i=1}^{N}T_iw(X_i)
\,,
\end{gather}
that is, $w(X_i)$ satisfy the second constraint of Problem~\ref{bw:1:primal}.
The second inequality follows from 
$\sum_{k=1}^{N}B_k(X)=1$ and the convexity of the absolute value. 
Indeed,
\begin{align*}
&
  \sup_{z\in\R}
  \left| 
  F_{Y(1)}(z|X_i)
  -
  \sum_{k=1}^{N} 
  B_k(X_i)
  \cdot
  F_{Y(1)}(z|X_k)
  \right|
  \\
  &
  \ 
  \le
  \ 
  \sum_{k=1}^{N} 
  \frac{\mathbf{1}{\left\{ X_k\in A_N(X_i) \right\}}}
  {
    \sum_{j=1}^{N} 
\mathbf{1}{\left\{ X_j\in A_N(X_i) \right\}}
  }
  \sup_{z\in\R}
  \left| 
  F_{Y(1)}(z|X_i)
  -
  F_{Y(1)}(z|X_k)
  \right|
  \\
  &
  \ 
  \le
  \ 
  \sup_{z\in\R}
  \omega
  \left( 
    F_{Y(1)}(z|\cdot)
    ,h_N
  \right)
  \,.
\end{align*}

Since we assume
\begin{gather*}
  \sqrt{N}
  \sup_{z\in\R}
  \omega
  \left( 
    F_{Y(1)}(z|\cdot)
    ,h_N
  \right)
  \to
  0
  \qquad
  \text{for}\ 
  N\to \infty
  \,,
\end{gather*}
it follows
$\sup_{z\in\R}|R_2(z)|\overset{\P}{\to}0$.
\end{proof}
\begin{remark}
In the original paper \cite{Wang2019} the authors derive concrete learning rates for the weights and employ them in bounding this term. They obtain a multiplied learning rate, which is sufficiently fast. Their approach, however, calls for concrete learning rates of the weights. Arguably, the process of deriving such rates is the most complicated part of the paper. 
I found out, that we don't need concrete rates for the weights. 
Consistency of the weights is enough and gives us an (arbitrarily slow but sufficient) learning rate to establish the results.
We don't even need rates for the weights to control $R_2$.
They only play a role in bounding $R_3$.

We also want to comment on the assumption
\begin{gather*}
  \sqrt{N}
  \sup_{z\in\R}
  \omega
  \left( 
    F_{Y(1)}(z|\cdot)
    ,h_N
  \right)
  \to
  0
  \qquad
  \text{for}\ 
  N\to \infty
  \,,
\end{gather*}
I decided to keep this more general (and abstract) assumption, althogh
there are many (more concrete, yet stronger) assumptions on the regularity of
$
    F_{Y(1)}(z|\cdot)
$
and the convergence speed of $h_N$.
If for example 
$
    F_{Y(1)}(z|\cdot)
$
is $\alpha$-Hölder continuous with $\alpha\in(0,1]$ for all $z\in\R$, it suffices $\sqrt{N}h_N^\alpha\to0$ to control $R_2$.
\end{remark}

To control the remaining terms $R_3$ and $R_4$ we use empirical processes. We introduce the concept and the results we need in the next paragraphs. 
For an introduction to empirical processes see \cite{Vaart2000}. More advanced techniques are in \cite{vaart2013}.

Let 
$
  \left( 
    \Omega,
    \mathcal{A},
    \P
  \right)
$
be a probability space,
$
  \left( 
    \mathcal{Z},
    \Sigma
  \right)
$
a measurable space, and 
$
  \xi_1,\ldots,\xi_N
  :
  \left( 
    \Omega,
    \mathcal{A},
    \P
  \right)
  \to
  \left( 
    \mathcal{Z},
    \Sigma
  \right)
$
a sample 
of independent and identically-distributed
random variables
with probability distribution $\P_{\!\xi}$.
A family $\mathcal{F}$ of measurable functions 
$
  f:
  \left( 
    \mathcal{Z},
    \Sigma
  \right)
    \to
  \left( 
    \R,
    \mathcal{B}(\R)
  \right)
$
induces a stochastic process by
\begin{gather}
  f
  \ 
  \mapsto
  \ 
  \G_N f 
  \ 
  :=
  \ 
  \frac{1}{\sqrt{n}}
  \sum_{i=1}^{N} 
  (
    f(\xi_i)
    -
    \E_\xi[f]
  \,.
\end{gather}
We call this the  \textbf{empirical process} $\G_N$ indexed by $\mathcal{F}$.
We define the (random) norm
\begin{gather}
  \norm{\G_n}_\mathcal{F}
  :=
  \sup_
        { f \in \mathcal{F}}
        \left|
          \G_N f
        \right|
        .
\end{gather}
\begin{remark}
We stress that 
$
  \norm{\G_n}_\mathcal{F}
$
often ceases to be measurable, even in simple situations~\cite[page 3]{vaart2013}.
To deal with this, we introduce the notion of \textbf{outer expectation} $\E^*$, that is,
\begin{gather}
  \E^*[Z]
  \ 
  :=
  \ 
    \inf
  \left\{ 
    \E[U]
  \ 
  \lvert
  \ 
    U\ge Z,
    \ 
    U:
  \left( 
    \Omega,
    \mathcal{A},
    \P
  \right)
  \to 
  \left( 
    \overline{\R},
    \mathcal{B}(\overline{\R})
  \right)
  \text{measurable and}
  \ 
  \E[U]<\infty
  \right\}
  \,.
\end{gather}
In our application the technical difficulties halt at this point, because we only consider $Z$ with $\E^*[Z]<\infty$. Then there exists a smallest measurable function $Z^*$ dominating $Z$ with
$\E^*[Z]=\E[Z^*]$. Thus, we may assume $Z$ to be measurable in this regard.


\end{remark}
In our application we need concentration inequalities for 
$
  \norm{\G_n}_\mathcal{F}
$.
One easy way is to use maximal inequalities for the expectation together with Markov's inequality. There are also Bernstein-like inequalities for empirical processes. We need to introduce more concepts.

Given two functions $\underline{f}$ and $\overline{f}$, the bracket
$[\underline{f},\overline{f}]$ 
is the set of all functions $f$ with 
$\underline{f}\le f \le \overline{f}$.
We define a
$(\varepsilon, L^{r}(\P))$-bracket
to be a bracket
$[\underline{f},\overline{f}]$ with
$\norm{\overline{f}-\underline{f}}_{ L^r(\P)}< \varepsilon$.
The bracketing number 
$
N_{[\,]}(\varepsilon, \mathcal{F}, L^r(\P))
$
is 
the minimum number of 
$(\varepsilon, L^{r}(\P))$-brackets needed to cover $\mathcal{F}$.
For most classes $\mathcal{F}$ the bracketing number grows to infinity for $\varepsilon\to 0$.
To measure the speed of convergence we introduce the bracketing integral
\begin{gather}
     J
    _{[]}
    (
    \delta
    ,
    \mathcal{F}
    ,
    L_r(\P)
    )
    =
  \int_0^{\delta}
      \sqrt{
        \log 
      N_{[\,]}
\left( \varepsilon, \mathcal{F}_N, L^r(\P) \right)
    }
    \,
    d\varepsilon
    \,.
\end{gather}
An envelope function $F$ of a class $\mathcal{F}$ satisfies 
$|f(x)|\le F(x)< \infty$ for all $f\in\mathcal{F}$ and all $x$.
\begin{theorem}
  \label{th:max_ineq}
  \emph{(Maximal inequality)}
  For any class $\mathcal{F}$ of measurable functions with envelope function $F,$
  \begin{gather}
    \E^*
    _\P
    [
    \norm{
      \G
      _n
      }
      _\mathcal{F}
    ]
    \lesssim
    J
    _{[]}
    (
    \norm{
      F
    }
    _{ L^2(\P)}
    ,
    \mathcal{F}
    ,
    L_2(\P)
    )
    .
  \end{gather}
\end{theorem}
\begin{proof}
  \cite[Corollary~19.35]{Vaart2000}
\end{proof}
%
%et 
%  $(\varepsilon_N)\subset(0,1]$ be a decreasing sequence with 
%  $\varepsilon_N\to 0$ as $N\to\infty$ and define for
%  $N\in\mathbb{N}$ and $z\in\R$ the function
% \begin{align*}
%    f_{\varepsilon_N,\pm}^z
%    \ 
%    :
%    \ 
%    &
%      \left\{ 0,1 \right\}
%      \times
%      \mathcal{X}
%      \times
%      \mathcal{Y}
%    \ 
%    \to
%    \ 
%    \R
%    \\
%    &
%      (T,X,Y(T))
%      \ 
%      \mapsto
%      \ 
%      \mathbf{1}
%      _
%      {\left\{ 
%          \left|
%      w(X)-\frac{1}{\pi(X)}
%          \right|
%          \,
%          \le
%          \,
%          \varepsilon_N/2
%      \right\}}
%      \left( 
%      w(X)-\frac{1}{\pi(X)}
%      \right)
%      ^{\pm}
%      T
%      \left( 
%        \mathbf{1}
%        _{\left\{  Y(T)\,\le\,z \right\}}
%        -
%        F_{Y(1)}(z|X)
%      \right)
%      \,,
%  \end{align*}
%  and define 
%  \begin{gather*}
%    \mathcal{F}_N^\pm
%    :=
%    \left\{ 
%    f_{\varepsilon_N,\pm}^z
%    |
%    z\in\R
%    \right\}
%    \,.
%  \end{gather*}
%\begin{lemma}
%  Consider the weights function
%  \begin{align*}
%    w
%    \colon
%    \mathcal{X}
%    \times
%    \R^{N+1}
%    \ 
%    \to
%    \ 
%    \R
%    \,,
%    \quad
%    (X,(\lambda,\lambda_0))
%    \ 
%    \mapsto
%    \ 
%    (
%    f^{'}
%    )^{-1}
%    \left( 
%      \inner{B(X)}{\lambda}
%      +
%      \lambda_0
%    \right)
%    \,.
%  \end{align*}
%    If the optimal solution 
%    $
%    (\lambda^\dagger,\lambda_0^\dagger)
%    $ to Problem? exists and is measurable for all $N\in\mathbb{N}$ 
%    then $
%    w
%    \circ
%    (
%    X
%    ,
%    (\lambda^\dagger,\lambda_0^\dagger)
%    )
%    $
%    is measurable.
%\end{lemma}
%\begin{lemma}
%  \label{bounded_f_lemma}
%    If the optimal solution $(\lambda^\dagger,\lambda_0^\dagger)$ to Problem? exists and is measurable for all $N\in\mathbb{N}$ 
%  the function class
%  $
%    \mathcal{F}_N^\pm
%  $
%    is measurable for all $N\in\mathbb{N}$.
%    Furthermore, $\mathcal{F}_N^\pm$ is bounded above by $\varepsilon_N$ and
%    \begin{gather*}
%     J_{[\, ]}
%    \left( 
%      \varepsilon_N
%    ,
%    \mathcal{F}_N^\pm
%    ,
%     L^2(\P)
%    \right)
%    \ 
%    \to
%    \ 
%    0
%    \qquad
%    \text{for}
%    \ 
%    N
%    \to
%    \infty
%\,. 
%    \end{gather*}
%\end{lemma}
%\begin{proof}
%  If the optimal solution $(\lambda^\dagger,\lambda_0^\dagger)$ to Problem? exists and is measurable for all $N\in\mathbb{N}$ 
%  then the weights function $w$ is measurable. Indeed,
%  this follows from
%  \begin{gather*}
%    w(X)=
%    (
%    f^{'}
%    )
%    ^{-1}
%    \left( \inner{B(X)}{\lambda^\dagger}+\lambda^\dagger_0 \right)
%    \,,
%  \end{gather*}
%  the measurability of the basis functions $B$ and the continuity of $
%    (
%    f^{'}
%    )
%    ^{-1}
%    $. Thus $f_{\varepsilon_N,\pm}^z$ are measurable for all $N\in\mathbb{N}$ and all $z\in\R$. Clearly, 
%    $\mathcal{F}_N^\pm$ is bounded above by $\varepsilon_N$.
%    Since 
%    \begin{gather*}
%       \mathbf{1}
%      _
%      {\left\{ 
%          \left|
%      w(X)-\frac{1}{\pi(X)}
%          \right|
%          \,
%          \le
%          \,
%          \varepsilon_N/2
%      \right\}}
%      \left( 
%      w(X)-\frac{1}{\pi(X)}
%      \right)
%      ^{\pm}
%    \end{gather*}
%    is non-negative and bounded above by $\varepsilon_N$
%    it follows from Lemma~\ref
%    {lem:brack_n}
%  \begin{gather*}
%    N_{[\,]}
%    (
%    \varepsilon
%    ,
%    \mathcal{F}_N^\pm, L^2(\P))
%    \ 
%    \lesssim
%    \ 
%    \left( 
%    \frac
%    {\varepsilon_N}
%    {\varepsilon}
%    \right)^2
%    \ 
%    \lesssim
%    \ 
%    \left( 
%    \frac
%    {1}
%    {\varepsilon}
%    \right)^2
%    \qquad
%    \text{for all}
%    \ 
%    \varepsilon>0
%    \,.
%  \end{gather*}
%Thus
%    \begin{align*}
%     J_{[\, ]}
%    \left( 
%      \varepsilon_N
%    ,
%    \mathcal{F}_N^\pm
%    ,
%     L^2(\P)
%    \right)
%    &
%    \ 
%=
%\  
%\int^{\varepsilon_N}_0
%\sqrt{
%\log
%    N_{[\,]}
%    (
%    \varepsilon
%    ,
%    \mathcal{F}_N^\pm, L^2(\P))
%}
%\, 
%d\varepsilon
%\\
%&
%\ 
%\lesssim
%\ 
%\int^{\varepsilon_N}_0
%\log
%\left( 
%  \frac{1}{\varepsilon}
%\right)
%\, 
%d\varepsilon
%\ 
%=
%\ 
%\varepsilon_N
%-
%\varepsilon_N \log(\varepsilon_N)
%\ 
%\to
%\ 
%0 
%\qquad
%\text{for}\ 
%N\to\infty
%\,.
%    \end{align*}
%\end{proof}
%In the next lemma we get rid of the $\varepsilon_N$ bound
%\begin{lemma}
%
%  \label{lemma_fpm}
% Consider for $z\in\R$ the function
% \begin{align*}
%    f_{\pm}^z
%    \ 
%    :
%    \ 
%      \left\{ 0,1 \right\}
%      \times
%      \mathcal{X}
%      \times
%      \mathcal{Y}
%    &
%    \ 
%    \to
%    \ 
%    \R
%    \\
%      (T,X,Y(T))
%    &
%      \ 
%      \mapsto
%      \ 
%      \left( 
%      w(X)-\frac{1}{\pi(X)}
%      \right)
%      ^{\pm}
%      T
%      \left( 
%        \mathbf{1}
%        _{\left\{  Y(T)\,\le\,z \right\}}
%        -
%        F_{Y(1)}(z|X)
%      \right)
%      \,.
%  \end{align*}
%  It holds
%  $\sup_{z\in\R}\left| \G_N f^z_\pm \right|\overset{\P}{\to}0$.
%  Furthermore, for
% \begin{align*}
%    f_z
%    \ 
%    :
%    \ 
%      \left\{ 0,1 \right\}
%      \times
%      \mathcal{X}
%      \times
%      \mathcal{Y}
%    &
%    \ 
%    \to
%    \ 
%    \R
%    \\
%      (T,X,Y(T))
%    &
%      \ 
%      \mapsto
%      \ 
%      \left( 
%      w(X)-\frac{1}{\pi(X)}
%      \right)
%      T
%      \left( 
%        \mathbf{1}
%        _{\left\{  Y(T)\,\le\,z \right\}}
%        -
%        F_{Y(1)}(z|X)
%      \right)
%      \,.
%  \end{align*}
%  it holds
%  $\sup_{z\in\R}\left| \G_N f_z \right|\overset{\P}{\to}0$.
%\end{lemma}
%\begin{proof}
%  It holds for all $z\in\R$
%  $
%  $
%  \begin{gather*}
%  f^z_\pm
%      (T,X,Y(T))
%      \ 
%      \lesssim
%      \ 
%     \left(
%      w(X)- \frac{1}{\pi(X)}
%      \right)
%      ^\pm
%      \,.
%  \end{gather*}
%  By Theorem~\ref{aa:weights:th}
%  there exists a decreasing sequence 
%  $(\varepsilon_N)\subset(0,1]$
%  with $\varepsilon_N\to 0$
%  and
%  $
%  \P[
%\left| 
%      w(X)- 1/\pi(X)
%\right|
%\le
%\varepsilon_N/2
%  ]
%  \to 1
%  $
%  for $N\to\infty$.
%  Therefore
%  \begin{gather*}
%  \P
%  \left[
%  f^z_\pm
%  \in
%  \mathcal{F}_N^\pm
%  \
%  \forall
%  z\in\R
%  \right]
%    =
%  \P[
%\left| 
%      w(X)- 1/\pi(X)
%\right|
%\le
%\varepsilon_N/2
%  ]
%  \to 1
%  \,.
%  \end{gather*}
%Then, for all $\varepsilon>0$ it holds
%\begin{align*}
%  \P
%  \left[
%  \sup_{z\in\R}
%  \left| 
%  \G_N
%  f^z_\pm
%  \right|
%  \le \varepsilon
%  \right]
%  &
%  \ 
%  \ge
%  \ 
%  \P
%  \left[
%  f^z_\pm
%  \in
%  \mathcal{F}_N^\pm
%  \
%  \forall
%  z\in\R
%  \,,
%  \ 
%  \norm{\G_N}^*_{\mathcal{F}_N^\pm}
%  \le \varepsilon
%  \right]
%  \\
%  &
%  \ 
%  \ge
%  \ 
%  \P
%  \left[
%  f^z_\pm
%  \in
%  \mathcal{F}_N^\pm
%  \
%  \forall
%  z\in\R
%  \right]
%  \ 
%  -
%  \ 
%  \P
%  \left[
%  \norm{\G_N}^*_{\mathcal{F}_N^\pm}
%  \le \varepsilon
%  \right]
%  \\
%  &
%  \ 
%  \to
%  \ 
%  1
%  \,.
%\end{align*}
%The convergence of the last term to 0 is due to 
%Lemma~\ref{bounded_f_lemma} and Lemma~\ref{markov_max_lemma}.
%Thus
%  $\sup_{z\in\R}\left| \G_N f^z_\pm \right|\overset{\P}{\to}0$.
%  Note, that 
%  \begin{gather*}
%\G_N f_z 
%=
%\G_N f^z_+
%-
%\G_N f^z_-
%\qquad
%\text{for all}\ 
%z\in\R
%\,.
%  \end{gather*}
%  Thus by Slutzky's Theorem\cite[Theorem~13.18]{Klenke2020}
%  it holds
%  $\sup_{z\in\R}\left| \G_N f_z \right|\overset{\P}{\to}0$.
%\end{proof}
%%\begin{lemma}
%%  Define for
%%  $z\in\R$
%% \begin{align*}
%%    f_z
%%    \ 
%%    :
%%    \ 
%%      \left\{ 0,1 \right\}
%%      \times
%%      \mathcal{X}
%%      \times
%%      \mathcal{Y}
%%&
%%    \ 
%%    \to
%%    \ 
%%      \R
%%    \\
%%    \left(
%%      T,X,Y(T)
%%    \right)
%%    &
%%      \ 
%%      \mapsto
%%      \ 
%%      \frac{T}{\pi(X)}
%%      \left( 
%%        \mathbf{1}
%%        _{\left\{  Y(T)\,\le\,z \right\}}
%%        -
%%        F_{Y(1)}(z|X)
%%      \right)
%%      \ 
%%      +
%%      \ 
%%      \left( 
%%        F_{Y(1)}(z|X)
%%        -
%%        F_{Y(1)}(z)
%%      \right)
%%      \,,
%%  \end{align*}
%%and consider the function class
%%$
%%  \mathcal{G}
%%  \ 
%%  :=
%%  \ 
%%  \left\{ 
%%    f_z
%%  \ 
%%    |
%%  \ 
%%    z\in\R
%%  \right\}
%%$
%%\begin{gather}
%%    \,.
%%\end{gather}
%%  Then $\mathcal{G}$ is a class of measurable functions 
%%  and it holds
%%  \begin{gather}
%%    N_{[\,]}(\varepsilon,\mathcal{G}, L^2(\P))
%%    \le
%%    ??
%%    \qquad
%%    \text{for all}
%%    \ 
%%    \varepsilon>0
%%    \,.
%%  \end{gather}
%%\end{lemma}
%%\begin{lemma}
%%  \label{lemma_max_ineq}
%%  Consider a function class $\mathcal{F}$ with unit ball
%%  $
%%  B_{\mathcal{F}}
%%  :=
%%  \left\{ 
%%    f\in \mathcal{F}
%%    \colon
%%    \norm{f}_\infty
%%    \le
%%    1
%%  \right\}
%%  $.
%%  Let $(\varepsilon_N)$ be a sequence converging to 0
%%  and let 
%%  $
%%  \left( 
%%    \mathcal{F}_N
%%  \right)
%%    :=
%%    \left( 
%%      C\cdot
%%    \varepsilon_N\cdot B_\mathcal{F}
%%    \right)
%%  $
%%  denote the sequence of scaled unit balls in $\mathcal{F}$.
%%  Assume that 
%%  there exists
%%  $k<2$ such that 
%%  the covering number of the unit ball in $\mathcal{F}$
%%  satisfies
%%  \begin{gather}
%%        \log 
%%      N_{[\,]}
%%\left( \varepsilon, B_\mathcal{F}, L^2(\P) \right)
%%\ 
%%\lesssim
%%\ 
%%\left( \frac{1}{\varepsilon} \right)^k
%%\qquad
%%\text{for all}\ 
%%\varepsilon>0
%%\,.
%%  \end{gather}
%%  Then it holds
%%  $
%%      \norm{G_N}^*_{\mathcal{F}_N}
%%    \ 
%%    \overset{\P}
%%    {\to}
%%    \ 
%%    0
%%  $
%%  for $N\to \infty$. 
%%\end{lemma}
%%\begin{proof}
%%  By maximal inequalities it holds
%%  \begin{align*}
%%    \E^*
%%    \left[ 
%%      \norm{G_N}_{\mathcal{F}_N}
%%    \right]
%%    &
%%      \ 
%%      \lesssim
%%      \ 
%%      J_{[\,]}\left( \varepsilon_N, \mathcal{F}_N, L^2(\P) \right)
%%      \\
%%    &
%%      \ 
%%      =
%%      \ 
%%      \int_0^{\varepsilon_N}
%%      \sqrt{
%%        \log 
%%      N_{[\,]}
%%\left( \varepsilon, \mathcal{F}_N, L^2(\P) \right)
%%    }
%%    \,
%%    d\varepsilon
%%    \\
%%    &
%%    \ 
%%    =
%%    \ 
%%      \int_0^{\varepsilon_N}
%%      \sqrt{\log 
%%      N_{[\,]}
%%\left( \varepsilon/(C\cdot \varepsilon_N), B_\mathcal{F}, L^2(\P) \right)
%%    }
%%    \,
%%    d\varepsilon
%%    \\
%%    &
%%    \ 
%%    \lesssim
%%    \ 
%%      \int_0^{\varepsilon_N}
%%      \left( 
%%      \frac{\varepsilon_N}{\varepsilon}
%%    \right)^{k/2}
%%    \,
%%    d\varepsilon
%%    \\
%%    &
%%    \ 
%%    =
%%    \ 
%%  \varepsilon_N^{k/2}
%%  \frac{1}{1-k/2}
%%  \varepsilon_N^{1-k/2}
%%  \\
%%    &
%%    \ 
%%  \lesssim
%%    \ 
%%  \varepsilon_N
%%  \\
%%    &
%%    \ 
%%  \to
%%  \ 
%%  0
%%  \qquad
%%  \text{for}\ 
%%  N\to
%%  \infty
%%  \,.
%%  \end{align*}
%%  Note, that $k<2$.
%%  By the boundedness of $\E^*$ there is no measurability problem.
%%  By Markov's Inequality it holds
%%  \begin{align*}
%%    \P
%%    \left[ 
%%      \norm{G_N}^*_{\mathcal{F}_N}\ge \varepsilon
%%    \right]
%%    \le
%%    \varepsilon^{-1}
%%    \,
%%    \E^*
%%    \left[ 
%%      \norm{G_N}_{\mathcal{F}_N}
%%    \right]
%%    \ 
%%    \to
%%    \ 
%%    0
%%    \qquad
%%    \text{for}\ 
%%    N\to\infty
%%    \,.
%%  \end{align*}
%%\end{proof}
%%The next two lemmas connect $R_3$ to the theory of empirical processes.
%%\begin{lemma}
%%  \label{aa:mean:l:fz}
%%  Consider 
%%  the (random) function
%%  $
%%  f_D^z
%%  $ given by
%%  \begin{gather}
%%    f_{D}^z(T,X,Y(T))
%%    :=
%%    T
%%    \left( 
%%    w(D,X)- \frac{1}{\pi(X)}
%%    \right)
%%    \left( 
%%    \mathbf{1}{\left\{ Y(T) \le z \right\}}
%%    -
%%  F_{Y(1)}(z|X)
%%    \right)
%%    \,.
%%  \end{gather}
%%  Assume that 
%%  there exists a function class $\mathcal{F}$ satisfying the requirements of Lemma~\ref{lemma_max_ineq} and that
%%  $
%%  f_D^z
%%  \in \mathcal{F}
%%  $
%%  for all $z\in\R$ almost surely.
%%  It then holds
%%  $
%%  \sup_{z\in\R} \left| G_Nf_D^z \right|
%%  \overset{\P}{\to}0$ for 
%%  $N\to\infty$.
%%\end{lemma}
%%\begin{proof}
%%  By the consistency of the weights there exists a learning rate $(\varepsilon_N)$ such that
%%  \begin{gather}
%%    \P
%%    \left[ 
%%      \left| 
%%      w(X,D)
%%      -
%%      \frac{1}{\pi(X)}
%%      \right|
%%      \le
%%      \varepsilon_N
%%    \right]
%%    \to 1 
%%    \qquad
%%    \text{for}\ 
%%    N\to \infty
%%    \,.
%%  \end{gather}
%%  Let
%%  $\mathcal{F}_N:=\varepsilon_NB_\mathcal{F}$ as in 
%%  Lemma~\ref{lemma_max_ineq}.
%%  It holds
%%\begin{gather}
%%      \sup_{z\in\R}
%%      \left| 
%%    f_D^z
%%      \right|
%%      \lesssim
%%      \left| 
%%      w(X,D)
%%      -
%%      \frac{1}{\pi(X)}
%%      \right|
%%      \le
%%      \varepsilon_N
%%\end{gather}
%%with probability going to 1 as $N\to\infty$.
%%Thus
%% \begin{gather}
%%    \P
%%    \left[ 
%%    f_D^z
%%    \in
%%  \mathcal{F}_N
%%  \ \forall\,z\in\R
%%    \right]
%%    =
%%    \P
%%    \left[ 
%%      \sup_{z\in\R}
%%      \left| 
%%    f_D^z
%%      \right|
%%      \lesssim
%%      \varepsilon_N
%%    \right]
%%    \to 1
%%    \qquad
%%    \text{as}
%%    \ 
%%    N\to\infty
%%    \,.
%%  \end{gather}
%%  Then it holds
%%  for all $\varepsilon>0$ 
%%  \begin{align*}
%%    \P
%%    \left[ 
%%      \sup_{z\in\R}
%%  \left| G_Nf_D^z \right|
%%  \le
%%  \varepsilon
%%    \right]
%%    &
%%    \ 
%%    \ge
%%    \ 
%%    \P
%%    \left[ 
%%      \sup_{z\in\R}
%%  \left| G_Nf_D^z \right|
%%  \le
%%  \norm{G_N}^*_{\mathcal{F}_N}
%%  \le
%%  \varepsilon
%%    \right]
%%    \\
%%    &
%%    \ 
%%    \ge
%%    \ 
%%    \P
%%    \left[ 
%%    f_D^z
%%    \in
%%  \mathcal{F}_N
%%  \ \forall\,z\in\R
%%      \ 
%%      \text{and}\ 
%%  \norm{G_N}^*_{\mathcal{F}_N}
%%  \le
%%  \varepsilon
%%    \right]
%%    \\
%%    &
%%    \ 
%%    \ge
%%    \ 
%%    \P
%%    \left[ 
%%    f_D^z
%%    \in
%%  \mathcal{F}_N
%%  \ \forall\,z\in\R
%%    \right]
%%    -
%%    \P
%%    \left[ 
%%  \norm{G_N}^*_{\mathcal{F}_N}
%%    \ 
%%  \ge
%%    \ 
%%  \varepsilon
%%    \right]
%%    \\
%%    &
%%    \ 
%%    \to
%%    \ 
%%    1
%%    \,.
%%  \end{align*}
%%  The convergence of the second term is due to Lemma~\ref{lemma_max_ineq}.
%%
%%\end{proof}
%%
%\begin{lemma}
%  \label{aa:mean:l:r3}
%  Assume conditional unconfoundedness, that is,
%  \begin{gather}
%  (Y(0),Y(1))\perp T \ |\ X
%  \,.
%  \end{gather}
%  Then for all
%  $z\in\R$
%  it holds
%  $
%  G_Nf_z=R_3(z)
%  $.
%  Furthermore, under conditions it holds
%  $\sup_{z\in\R}\left| R_3(z) \right|\overset{\P}{\to}0$.
%\end{lemma}
%\begin{proof}
%  A standard computation shows
%  \begin{gather}
%    \E
%    \left[ 
%    \frac{T}{\pi(X)}
%    \left( 
%    \mathbf{1}{\left\{ Y(T) \le z \right\}}
%    -
%  F_{Y(1)}(z|X)
%    \right)
%    \right]
%    =0
%    \,.
%  \end{gather}
%  Furthermore
%  \begin{align*}
%    &
%    \E
%    \left[ 
%      Tw(X,D)
%    \left( 
%    \mathbf{1}{\left\{ Y(T) \le z \right\}}
%    -
%  F_{Y(1)}(z|X)
%    \right)
%    \right]
%    \\
%    &
%    \ 
%    =
%    \ 
%    \E
%    \left[ 
%      \E
%      \left[ 
%      w(X,D)
%      \left( 
%    \mathbf{1}{\left\{ Y(1) \le z \right\}}
%    -
%  F_{Y(1)}(z|X)
%      \right)
%  |T=1,X,D
%      \right]
%    \right]
%    \\
%    &
%    \ 
%    =
%    \ 
%    \E
%    \left[ 
%      w(X,D)
%      \E
%      \left[ 
%    \mathbf{1}{\left\{ Y(1) \le z \right\}}
%    -
%  F_{Y(1)}(z|X)
%  |X,D
%      \right]
%    \right]
%    \\
%    &
%    \ 
%    =
%    \ 
%    \E
%    \left[ 
%      w(X,D)
%      \E
%      \left[ 
%    \mathbf{1}{\left\{ Y(1) \le z \right\}}
%    -
%  F_{Y(1)}(z|X)
%  |X
%      \right]
%    \right]
%    \\
%    &
%    \ 
%    =
%    \ 
%    0
%  \end{align*}
%  The second equality is due to the assumption of 
%  $(Y(0),Y(1))\perp T |X$.
%  The third equality is due to $X\perp D$.
%  Thus
%  $
%    \E
%    f_D^z
%    =
%    0
%  $
%\end{proof}
%

Next, we define some auxiliary functions.
  For $z\in\R$ we define the function
  \begin{align*}
    f_z
    \ 
    :
    \ 
      \left\{ 0,1 \right\}
      \times
      \mathcal{X}
      \times
      \mathcal{Y}
    &
    \ 
    \to
    \ 
    \R
    \\
      (t,x,y)
    &
      \ 
      \mapsto
      \ 
      t
      \left( 
        \mathbf{1}
        _{\left\{  y\,\le\,z \right\}}
        -
        F_{Y(1)}(z|x)
      \right)
      \,,
  \end{align*}
  and the function classes
  \begin{gather}
    \label{F_g}
    \begin{split}
    \mathcal{F}
    &
    \ 
    :=
    \ 
    \left\{ 
      f_z
      \ 
      |
      \ 
      z\in\R\ 
    \right\}
    \\
    \mathcal{G}
    &
    \ 
    :=
    \ 
    \left\{ 
      \frac{f_z}{\pi(\cdot)}
      +
      F_{Y(1)}(z|\cdot)
      -
      F_{Y(1)}(z)
      \ 
      \colon
      \ 
      z\in\R\ 
    \right\}
    \,.
    \end{split}
  \end{gather}
  \begin{assumption}
    \label{aa:assumption:treatment_str_ign}
    It holds 
    \begin{gather*}
    (Y(0),Y(1))\, \perp\,  T \ |\  X
    \qquad
    \text{and}
    \qquad
      0
      \ 
      <
      \ 
      \pi(x)
      \ 
      <
      \ 
      1
      \quad
      \text{for all}\ 
      x\in\mathcal{X}
      \,.
    \end{gather*}
  \end{assumption}
  \begin{lemma}
    \label{aa:mean:r3:lem:fz_E}
    It holds
    $f_z(T,X,Y(T))\in L^1(\P)$
    and 
    $f_z(T,X,Y(T))\perp D_N$
    for all $z\in\R$.
    If also Assumption~\ref{aa:assumption:treatment_str_ign} holds,
    then 
    for all $z\in\R$
    \begin{gather*}
      \E
      \left[
        f_z
        \left( 
          T,
          X,
          Y(T)
        \right)
        \,
        |
        \,
        X
      \right]
      \ 
      =
      \ 
      0
      \qquad
      \text{almost surely.}
    \end{gather*}
  \end{lemma}
  \begin{proof}
    Since $f_z$ is bounded by 1,
    it holds
    $f_z(T,X,Y(T))\in L^1(\P)$.
    Since
    \begin{gather*}
    (T,X,Y(T))
    \ 
    \perp 
    \ 
    D_N
    \ 
    =
    \ 
    (T_i,X_i)_{i\in \left\{ 1,\ldots,N \right\}}
    \end{gather*}
    it holds
    $f_z(T,X,Y(T))\perp D_N$
    for all $z\in\R$.
    For the third statement, note that
    \begin{align*}
      \E
      \left[
        f_z
        \left( 
          T,
          X,
          Y(T)
        \right)
        \,
        |
        \,
        X
      \right]
      &
      \ 
      =
      \ 
      \E
      \left[
      T
      \left( 
        \mathbf{1}
        _{\left\{  Y(T)\,\le\,z \right\}}
        -
        F_{Y(1)}(z|X)
      \right)
        \,
        |
        \,
        X
      \right]
      \\
      &
      \ 
      =
      \ 
      \E
      \left[
        \mathbf{1}
        _{\left\{  Y(1)\,\le\,z \right\}}
        -
        F_{Y(1)}(z|X)
        \,
        |
        \,
        X
        ,
        T=1
      \right]
      \pi(X)
      \\
      &
      \ 
      =
      \ 
      \left( 
      \E
      \left[
        \mathbf{1}
        _{\left\{  Y(1)\,\le\,z \right\}}
        \,
        |
        \,
        X
      \right]
      \ 
        -
      \ 
        F_{Y(1)}(z|X)
      \right)
      \pi(X)
      \\
      &
      \ 
      =
      \ 
      0
      \qquad
      \text{almost surely.}
    \end{align*}
    The third equality is due to Assumption~\ref{aa:assumption:treatment_str_ign}.
  \end{proof}
The next lemma provides bracketing numbers for specific function classes needed to control $R_3$ and $R_4$.
\newpage
\begin{lemma}
  \label{aa:mean:l:br}
  The function class $\mathcal{F}$ and $\mathcal{G}$ defined in \eqref{F_g} are measurable.
  Furthermore, 
  \begin{gather*}
    N_{[\,]}
    (
    \varepsilon
    ,
    \mathcal{F}, L^2(\P))
    \ 
    \lesssim
    \ 
    \left( 
      \frac{1}{\varepsilon}
    \right)^2
    \qquad
    \text{for all}
    \ 
    \varepsilon>0
    \,.
  \end{gather*}
  If $1/\pi(X)\in L^2(\P)$, it also holds 
  \begin{gather*}
    N_{[\,]}
    (
    \varepsilon
    ,
    \mathcal{G}, L^2(\P))
    \ 
    \lesssim
    \ 
    \left( 
    \frac{
      1+
    \norm{1/\pi(X)}_{ L^2(\P)}
    }
    {\varepsilon}
    \right)^4
    \qquad
    \text{for all}
    \ 
    \varepsilon>0
    \,.
  \end{gather*}
%  Furthermore, 
%  consider the function class
%  \begin{gather*}
%    \mathcal{G}
%    :=
%    \left\{ 
%      f_{1/\pi}^z
%      +
%        F_{Y(1)}(z|\cdot)
%        -
%        F_{Y(1)}(z)
%      \ 
%      |
%      \ 
%      z\in\R
%    \right\}
%    \,.
%  \end{gather*}
%  If $1/\pi(X)\in  L^2(\P)$
%  it holds
%  \begin{gather}
%    N_{[\,]}(\varepsilon,\mathcal{G}, L^2(\P))
%    \le
%    ??
%    \qquad
%    \text{for all}
%    \ 
%    \varepsilon>0
%    \,.
%  \end{gather}
\end{lemma}
\begin{proof}
  As in \cite[Example~19.6]{Vaart2000}
  we choose for
  $\varepsilon>0$ and $m\in\mathbb{N}$
  \begin{gather*}
  -\infty=z_0\ <\ z_1\ <\ \cdots\ <\ z_{m-1}\ <\ z_m=\infty
  \,
  \end{gather*}
  such that
  \begin{gather}
    \label{size_z}
    \P
    \left[ 
      Y(1)\in \left[ z_{l-1},z_l \right]\,
    \right]
    \ 
    \le
    \ 
    \varepsilon
    \qquad
    \text{for all}\ 
    l\in \left\{ 1,\ldots,m \right\}
  \end{gather}
  and $m \le 2/\varepsilon$.
  Next, we define $m$ brackets by
\begin{align*}
  \overline{f_l}
  (t,x,y)
  &
  \ 
  :=
  \ 
      t
      \left( 
        \mathbf{1}
        _{\left\{  y\,\le\,z_{l} \right\}}
        -
        F_{Y(1)}(z_{l-1}|x)
      \right)
      \,,
      \\
  \underline{f_l}
  (t,x,y)
  &
  \ 
  :=
  \ 
      t
      \left( 
        \mathbf{1}
        _{\left\{  y\,\le\,z_{l-1} \right\}}
        -
        F_{Y(1)}(z_l|x)
      \right)
      \,,
\end{align*}
for $l\in \left\{ 1,\ldots,m \right\}$.
These brackets cover $\mathcal{F}$.
Indeed,
\begin{gather*}
  \text{for all}\ 
  z\in\R
  \ 
  \text{there exists} \ 
l\in \left\{ 1,\ldots,m \right\}
\qquad 
\text{such that}\qquad
z_{l-1}
\ 
\le
\ 
z
\ 
\le
\ 
z_l
\,.
\end{gather*}
By the monotonicity of 
$
        \mathbf{1}
      _{\left\{  y\,\le\,(\cdot) \right\}}
$
and
$
        F_{Y(1)}(\cdot|x)
$
and the non-negativity of $T$ it follows
\begin{gather*}
  \text{for all}\ 
  z\in\R
  \ 
  \text{there exists} \ 
l\in \left\{ 1,\ldots,m \right\}
\qquad 
\text{such that}\qquad
  \underline{f_l}
  \ 
  \le
  \ 
  f_z
  \ 
  \le
  \ 
  \overline{f_l}
  \,.
\end{gather*}
Thus, the $m$ brackets 
$
[
  \underline{f_l}
  ,
  \overline{f_l}
]
$
cover $\mathcal{F}$.

Let's calculate the size of the brackets.
It holds
\begin{align*}
  &
\E
\left[ 
      T
      \cdot
      \left( 
        \mathbf{1}
        _{\left\{  Y(T)\,\le\,z_{l} \right\}}
        -
        F_{Y(1)}(z_{l-1}|X)
        \ 
        -
        \ 
        \mathbf{1}
        _{\left\{  Y(T)\,\le\,z_{l-1} \right\}}
        +
        F_{Y(1)}(z_{l}|X)
      \right)
      \,
\right]
\\
  &
  \ 
=
  \ 
\E
\left[ 
      T
      \cdot
      \left( 
        \mathbf{1}
        _{\left\{
        Y(T)
        \,
        \in 
        \,
    [z_{l-1},z_l]
\right\}}
\ 
        +
\ 
        \P
        \left[ 
          Y(1)
          \in
    [z_{l-1},z_l]
        \,
    |
        \,
    X
        \right]
      \right)
      \,
\right]
\\
  &
  \ 
\le
  \ 
\E
\left[ 
  \,
  \pi(X)
  \cdot
        \P
        \left[ 
          Y(1)
          \in
    [z_{l-1},z_l]
          \,
    |
          \,
    X
        \right]
          \ 
\right]
\ 
+
\ 
\varepsilon
\\
  &
  \ 
\le
  \ 
2
\,
\varepsilon
\,.
\end{align*}
We used \eqref{size_z}, $0\le T,\pi(X)\le 1$ and Lemma~\ref{ps_weights_lemma}.
It follows
\begin{align*}
  &
  \norm{
    \left( 
  \overline{f_l}
-
  \underline{f_l}
    \right)
  (T,X,Y(T))
}_
{ L^2(\P)}
\\
&
\ 
\lesssim
\ 
\E
\left[ 
  \,
      T
      \cdot
      \left( 
        \mathbf{1}
        _{\left\{
        Y(T)
        \,
        \in 
        \,
    [z_{l-1},z_l]
\right\}}
\ 
        +
\ 
        \P
        \left[ 
          Y(1)
          \in
    [z_{l-1},z_l]
        \,
    |
        \,
    X
        \right]
      \right)
      \,
   \right]^{1/2}
\ 
\lesssim
\ 
\varepsilon^{1/2}
\,.
\end{align*}
Since $m\le 2/\varepsilon$ it holds
  \begin{align*}
    N_{[\,]}
    \left(
\varepsilon^{1/2}
    ,
    \,
    \mathcal{F}\,,\, L^2(\P)
    \right)
    &
    \ 
    \lesssim
    \ 
    \frac{1}{\varepsilon}
    \intertext{and thus}
    N_{[\,]}
    (
    \varepsilon
    ,
    \mathcal{F}, L^2(\P))
    &
    \ 
    \lesssim
    \ 
    \left( 
      \frac{1}{\varepsilon}
    \right)^2
    \,.
  \end{align*}
  Next, we look at $\mathcal{G}$. To this end, we define 
  $m$ brackets by
 \begin{align*}
    \overline{g_l}
    (t,x,y)
    \ 
    :=
    \ 
    \frac{t}{\pi(x)}
    \left( 
      \mathbf{1}{\left\{  y\,\le\,z_{l} \right\}}
      -
      F_{Y(1)}(z_{l-1}|x)
    \right)
    \ 
    +
    \ 
    F_{Y(1)}(z_{l}|x)
-
F_{Y(1)}(z_{l-1})
\,,
\\
    \underline{g_l}
    (t,x,y)
    \ 
    :=
    \ 
    \frac{t}{\pi(x)}
    \left( 
      \mathbf{1}{\left\{  y\,\le\,z_{l-1} \right\}}
      -
      F_{Y(1)}(z_l|x)
    \right)
    \ 
    +
    \ 
    F_{Y(1)}(z_{l-1}|x)
-
      F_{Y(1)}(z_l)
\,,
  \end{align*}
  for $l\in \left\{ 1,\ldots,m \right\}$.
  With the same arguments as before, we see that these brackets cover $\mathcal{G}$.
  Let's calculate the size.
  It holds
  \begin{align*}
    &
    \norm{
      \frac{T}{\pi(X)}
      \left( 
      \mathbf{1}{
      \left\{ 
      Y(T)\in [z_{l-1},z_l] 
    \right\}
    }
      +
      \P
      \left[ 
      Y(1)\in [z_{l-1},z_l] 
      \,
      |
      \,
      X
      \right]
      \right)
    }_{ L^2(\P)}
    \\
    &
    \
    \lesssim
    \
    \left( 
      \E
      \left[ 
        \frac{1}{\pi(X)}
        \frac{T}{\pi(X)}
      \left( 
      \mathbf{1}{
      \left\{ 
      Y(T)\in [z_{l-1},z_l] 
    \right\}
    }
      +
      \P
      \left[ 
      Y(1)\in [z_{l-1},z_l] 
      \,
      |
      \,
      X
      \right]
      \right)
      \right]
    \right)
    ^{1/2}
    \\
    &
    \
    \lesssim
    \
    \left( 
      \E
      \left[ 
        \frac{1}{\pi(X)}
      \P
      \left[ 
      Y(1)\in [z_{l-1},z_l] 
      \,
      |
      \,
      X
      \right]
      \right]
    \right)
    ^{1/2}
    \\
    &
    \ 
    \lesssim
    \ 
    \left( 
      \norm{1/\pi(X)}_{ L^2(\P)}
      \sqrt{\varepsilon}
    \right)
    ^{1/2}
    \ 
    =
    \ 
    \varepsilon^{1/4}
    \norm{1/\pi(X)}_{ L^2(\P)}^{1/2}
  \end{align*}
  and
  \begin{align*}
     \norm{
      \P
      \left[ 
      Y(1)\in [z_{l-1},z_l] 
      \,
      |
      \,
      X
      \right]
      \ 
     + 
      \ 
      \P
      \left[ 
      Y(1)\in [z_{l-1},z_l] \,
      \right]
    }_{ L^2(\P)}
    \ 
    \lesssim
    \ 
    \varepsilon^{1/2}
    \,.
  \end{align*}
  Thus
  \begin{align*}
  \norm{
    \left( 
  \overline{g_l}
-
  \underline{g_l}
    \right)
  (T,X,Y(T))
}_
{ L^2(\P)}
    &
\ 
\lesssim
\ 
\varepsilon^{1/4}
\left( 
  1
  +
    \norm{1/\pi(X)}_{ L^2(\P)}^{1/2}
\right)
\\
    &
\ 
\lesssim
\ 
\varepsilon^{1/4}
\left( 
  1
  +
    \norm{1/\pi(X)}_{ L^2(\P)}
\right)
\,.
  \end{align*}
As before, it follows
\begin{gather*}
    N_{[\,]}
    (
    \varepsilon
    ,
    \mathcal{G}, L^2(\P))
    \ 
    \lesssim
    \ 
    \left( 
    \frac{
      1+
    \norm{1/\pi(X)}_{ L^2(\P)}
    }
    {\varepsilon}
    \right)^4
    \,.
\end{gather*}
\end{proof}

\begin{lemma}
  \label{markov_max_lemma}
  Let $(\mathcal{H}_N)$ be a sequence of measurable function classes with envelope functions $(H_N)$.
  If
  \begin{gather}
    J_{[\, ]}
    \left( 
    \norm{F_N}_{ L^2(\P)}
    ,
    \mathcal{H}_N
    ,
     L^2(\P)
    \right)
    \ 
    \to
    \ 
    0
    \qquad
    \text{for}
    \ 
    N
    \to
    \infty
  \end{gather}
  it holds 
  $
  \norm{\G_N}^*_{\mathcal{H}_N}\overset{\P}{\to}0
  $.
\end{lemma}
\begin{proof}
  By Markov's inequality and Theorem~\ref{th:max_ineq} it holds for all $\varepsilon>0$
  \begin{align*}
    &
    \P
    [
  \norm{\G_N}^*_{\mathcal{H}_N}
  \ge
  \varepsilon
    ]
    \\
    &
    \ 
    \le
    \ 
    \varepsilon^{-1}
    \E
    [
  \norm{\G_N}^*_{\mathcal{H}_N}
    ]
    \ 
    =
    \ 
    \varepsilon^{-1}
    \E^*
    [
  \norm{\G_N}_{\mathcal{H}_N}
    ]
    \ 
    \lesssim
    \ 
    \varepsilon^{-1}
    J_{[\, ]}
    \left( 
    \norm{H_N}_{ L^2(\P)}
    ,
    \mathcal{H}_N
    ,
     L^2(\P)
    \right)
    \ 
    \to
    \ 
    0
  \end{align*}
  for $N\to\infty$. 
\end{proof}
A technical lemma for products of function classes.

Define the product of two function classes
\begin{gather*}
  \mathcal{F}\cdot \mathcal{G}
  :=
  \left\{ 
    f\cdot g
    \colon
    f\in\mathcal{F},
    g\in\mathcal{G}
  \right\}\,.
\end{gather*}
\begin{lemma}
  \label{lem_prod_br}
  Let
  $\mathcal{F}$ and $\mathcal{G}$ be two function classes 
  with envelope functions $F$ and $G$ satisfying
  $\norm{F}_\infty,\norm{G}_\infty\le 1$.
  For all $\varepsilon>0$ and all $r\in [1,\infty)$ it holds
  \begin{gather*}
    N_{[\,]}(2\varepsilon,\mathcal{F}\cdot\mathcal{G},\mathrm{L}_r(\P))
    \
    \le
    \ 
    N_{[\,]}(\varepsilon,\mathcal{F},\mathrm{L}_r(\P))
    \cdot
    N_{[\,]}(\varepsilon,\mathcal{G},\mathrm{L}_r(\P))
    \,.
  \end{gather*}
\end{lemma}
\begin{proof}
  Let $f\in\mathcal{F}$ and $g\in\mathcal{G}$.
  We can choose two 
  $(\varepsilon,L^r(\P))$
  brackets
  $[\underline{f},\overline{f}]$
  and
  $[\underline{g},\overline{g}]$
  containing $f$ and $g$ with 
  $\norm{\underline{f}}_\infty,\norm{\overline{f}}_\infty\le\norm{F}_\infty\le 1$
  and
  $\norm{\underline{g}}_\infty,\norm{\overline{g}}_\infty\le\norm{G}_\infty\le 1$.
  We the get an 
  $(2\varepsilon,L^r(\P))$
  $[\underline{h},\overline{h}]$
  bracket, containing $f\cdot g$, by
\end{proof}

\begin{lemma}
  \label{aa:r3:lemma:1}
  Let
$(\varepsilon_N)\subset(0,1]$
be 
a decreasing sequence
with $\varepsilon_N\to 0$ for $N\to\infty$ 
and let Assumption~\ref{aa:assumption:1} hold true
for a sequence of function classes $\mathcal{F}_N$.
Then
\begin{gather*}
  J_{[\,]}(
\norm{F_N}_{L^2(\P)}
,\mathcal{F}_N\cdot\mathcal{F},\mathrm{L}_2(\P))
  \to 0
  \quad
  \text{and}
  \quad
  \norm{\G_N}^*_{\mathcal{F}_N\cdot\mathcal{F}}\overset{\P}{\to}0
  \qquad
  \text{for}\ 
  N\to\infty
  \,.
\end{gather*}
\end{lemma}
\begin{proof}
  By Assumption~\ref{aa:assumption:1}
  and Lemma~\ref{aa:mean:l:br} it holds
for some $k<2$
\begin{gather*}
\norm{F_N}_{L^2(\P)}
\ 
\le
\ 
\varepsilon_N
\quad
\text{and}
\quad
  \log
  N_{[\,]}(\varepsilon,\mathcal{F}_N,\mathrm{L}_2(\P))
  \ 
  \lesssim
  \ 
  \left( 
  \frac{1}{\varepsilon}
  \right)^k
  \quad
  \text{for all}
  \ 
  N\in\mathbb{N}
  \,,
\end{gather*}
and
  \begin{gather*}
    N_{[\,]}
    (
    \varepsilon
    ,
    \mathcal{F}, L^2(\P))
    \ 
    \lesssim
    \ 
    \left( 
      \frac{1}{\varepsilon}
    \right)^2
    \qquad
    \text{for all}
    \ 
    \varepsilon>0
    \,.
  \end{gather*}
  Since $\mathcal{F}_N$ and $\mathcal{F}$ have envelope function smaller 1, we can apply Lemma~\ref{lem_prod_br} to get
  \begin{gather*}
  \log
  N_{[\,]}(\varepsilon,\mathcal{F}_N\cdot\mathcal{F},\mathrm{L}_2(\P))
  \ 
  \lesssim
  \ 
  \left( 
  \frac{1}{\varepsilon}
  \right)^k
  +
  \log
  (1/\varepsilon)
  \ 
  \lesssim
  \ 
  \left( 
  \frac{1}{\varepsilon}
  \right)^k
  \quad
  \text{for all}\ 
  \varepsilon>0
  \,.
  \end{gather*}
  Since 
  $k/2\in(0,1)$
  it holds
\begin{align*}
  J_{[\,]}(
\norm{F_N}_{L^2(\P)}
,\mathcal{F}_N\cdot\mathcal{F},\mathrm{L}_2(\P))
  &
  \ 
=
  \ 
\int_0^{
\norm{F_N}_{L^2(\P)}
}
\sqrt{
  \log
  N_{[\,]}(\varepsilon,\mathcal{F}_N\cdot\mathcal{F},\mathrm{L}_2(\P))
}
\,d\varepsilon
\\
&
\ 
\lesssim
\ 
\int_0^{
  \varepsilon_N
}
  \left( 
  \frac{1}{\varepsilon}
\right)^{k/2}
\,d\varepsilon
\\
&
\ 
=
\ 
\frac{
\varepsilon_N^{1-k/2}
}{1-k/2}
\ 
\to 0
\ 
\qquad
\text{for}
\ 
N\to\infty
\,.
\end{align*}
The second statement follows from Lemma~\ref{markov_max_lemma}
for 
$\mathcal{H}_N:=\mathcal{F}_N\cdot\mathcal{F}$ and
$H_N:=F_N$.
\end{proof}

\begin{lemma}
  \label{ps_weights_lemma}
  Let
  $
  g_1\colon
  \mathcal{X}\to\R
  $
  and
  $
  g_2\colon
  \mathcal{Y}\to\R
  $
  be a measurable functions.
  It holds
  \begin{gather*}
    \E
    \left[
    \frac{T}{\pi(X)}
    g_1(X)
    \right]
    \ 
    =
    \ 
    \E
    \left[
    g_1(X)
    \right]
    \,.
  \end{gather*}
  If Assumption~\ref{aa:assumption:treatment_str_ign} holds true, then
  \begin{gather*}
    \E
    \left[
    \frac{T}{\pi(X)}
    g_2(Y(T))
    \right]
    \ 
    =
    \ 
    \E
    \left[
    f(Y(1))
    \right]
    \,.
  \end{gather*}
\end{lemma}
\begin{lemma}
  \label{aa:mean:r3:lem:conv}
  Under conditions it holds
  $\sup_{z\in\R}\left| R_3(z) \right|\overset{\P}{\to}0$.
\end{lemma}
\begin{proof}
  Let $N\ge\underline{N}$,
  $z\in\R$,
  and 
  let
  $g^\dagger$ 
  denote the function \eqref{ghost_function} with 
  $
(\lambda^\dagger,\lambda_0^\dagger)
  $.
  If
  \begin{gather*}
    \left| 
    w^\dagger(X)- \frac{1}{\pi(X)} 
    \right|
    \ 
    \le
    \ 
    \varepsilon_N
  \end{gather*}
  it holds
  \begin{gather*}
    g^\dagger
    (X)
    \cdot
    f_z
    (T,X,Y(T))
    \ 
    =
    \ 
  \left( 
      w^\dagger(X)
      -
      \frac{1}{\pi(X)}
  \right)
      T
      \left( 
        \mathbf{1}
        _{\left\{  Y(T)\,\le\,z \right\}}
        -
        F_{Y(1)}(z|X)
      \right)
      \,.
  \end{gather*}
  By Lemma~\ref{aa:mean:r3:lem:fz_E}
  it holds 
  \begin{align*}
    &
  f_z(T,X,Y(T))
  \ 
  \in
  \ 
  L^1(\P)\,, 
  \\
  &
  f_z(T,X,Y(T))
  \ 
  \perp
  \ 
  D_N
  \,,
  \\
  \E
  [
  &
  f_z(T,X,Y(T))
    |X
  ]
  \ 
  =
  \ 
  0
  \,.
  \end{align*}
  Thus, 
  it follows from Lemma~\ref{w.Z=0}
  \begin{gather*}
    \E
    \left[
      w^\dagger(X)
      \cdot
      f_z(T,X,Y(T))
    \right]
    \ 
    =
    \ 
    0
    \,.
  \end{gather*}
  Since
  \begin{gather*}
    \E
    \left[
      \frac{T}{\pi(X)}
      f_z(T,X,Y(T))
    \right]
    \ 
    =
    \ 
    \E
    \left[
      \frac{T}{\pi(X)}
      \left( 
        \mathbf{1}
        _{\left\{  Y(T)\,\le\,z \right\}}
        -
        F_{Y(1)}(z|X)
      \right)
    \right]
    \  
    =
    \ 
    0
  \end{gather*}
  by Lemma~\ref{ps_weights_lemma},
  it follows
  \begin{align*}
    &
  \E
  \left[
    g^\dagger(X)
    f_z(T,X,Y(T))
  \right]
  \\
    &
  \ 
  =
  \ 
    \E
    \left[
      w^\dagger(X)
      \cdot
      f_z(T,X,Y(T))
    \right]
    \ 
    -
    \ 
    \E
    \left[
      \frac{T}{\pi(X)}
      f_z(T,X,Y(T))
    \right]
    \ 
    =
    \ 
  0
  \,.
  \end{align*}
  But then 
  \begin{align*}
    R_3(z)
    &
    \ 
    =
    \ 
  \frac{1}
  {
\sqrt{N}
  }
    \sum_{i=1}^{N} 
    \left[ 
    \left( 
    w(X_i) 
    -
    \frac{1}{\pi(X_i)}
    \right)
    T_i
    \left( 
    \mathbf{1}{\left\{ Y_i \le z \right\}}
    -
  F_{Y(1)}(z|X_i)
    \right)
    \right]
    \ 
    =
    \ 
    \G_N
    \left( 
      g^\dagger
      \cdot
      f_z
    \right)
    \,.
  \end{align*}
  It follows
\begin{align*}
    \P
    \left[ 
      \sup_{z\in\R}
     | 
    R_3(z)
    |
      \ge
      \varepsilon
    \right]
    &
    \ 
    \le
    \ 
    \P
    \left[ 
      \sup_{z\in\R}
     | 
    R_3(z)
    |
      \ge
      \varepsilon
      \ 
      \text{and}
      \ 
    | 
    w^\dagger(X)- 1/\pi(X)
    |
    \ 
    \le
    \ 
    \varepsilon_N
    \right]
    \\
    &
    \quad
    +
    \ 
    \P
    \left[ 
    | 
    w^\dagger(X)- 1/\pi(X)
    |
    \ 
    >
    \ 
    \varepsilon_N
    \right]
    \\
    &
    \ 
    \le
    \ 
    \P
    \left[ 
      \norm{\G_N}^*_{\mathcal{F}_N\cdot\mathcal{F}}
      \ge
      \varepsilon
    \right]
    \ 
    +
    \ 
    \ 
    \P
    \left[ 
    | 
    w^\dagger(X)- 1/\pi(X)
    |
    \ 
    >
    \ 
    \varepsilon_N
    \right]
    \\
    &
    \ 
    \to
    \ 
    0
    \,.
\end{align*}
The convergence of the first term follows from
Lemma~\ref{aa:r3:lemma:1}.
The convergence of the second term follows from
Theorem~\ref{aa:weights:th}.

\end{proof}


Until now, all parts of the error decomposition converge to 0.
The last term $R_4$ will decide the profile of the limiting process.
To this end we need the following concept.
\begin{definition}
  We call a class 
  $\mathcal{F}$ of measurable functions 
$\P$-Donsker
if the sequence of processes 
$\left\{ G_N f \colon f\in\mathcal{F}\right\}$
converges in
$l^\infty(\mathcal{F})$
to a tight limit process.
\end{definition}

\begin{theorem}
  Every class $\mathcal{F}$ of measurable functions 
  with
  $
    J
    _{[]}
    (
    1
    ,
    \mathcal{F}
    ,
    L_2(\P)
    )
    <\infty
  $
  is

  $\P$-Donsker, that is,
  the sequence of processes 
$\left\{ G_N f \colon f\in\mathcal{F}\right\}$
  converges 
  in
$l^\infty(\mathcal{F})$
to a Gaussian process with mean 0 and covariance function given by
\begin{gather}
  \mathrm{Cov}(f,g)
  :=
  \E[fg]-\E[f]\E[g]
  \,.
\end{gather}
\end{theorem}
\begin{proof}
  \cite[Theorem~19.5]{Vaart2000}
\end{proof}



\begin{lemma}
  \label{aa:mean:l:r4}
  Let
  $1/\pi(X)\in L^2(\P)$.
  $R_4$ converges
  converges in
  $l^\infty(\R)$
  to a Gaussian process with mean 0 and covariance
\begin{align*}
  &
  \mathbf{Cov}
  (z_1,z_2)
  \\
  &
  =\ 
  \E
  \left[ 
 \frac{
 F_{Y(1)}(z_1 \land z_2\,|\,X)
}{\pi(X)}
\ 
-
\ 
 \frac{1-\pi(X)}{\pi(X)}
 F_{Y(1)}(z_1|X)
 \cdot
 F_{Y(1)}(z_2|X)
  \right]
  \ 
 -
 \ 
 F_{Y(1)}(z_1)
 \cdot
 F_{Y(1)}(z_2)
\end{align*}

\end{lemma}
\begin{proof}
  By Lemma~\ref{aa:mean:r3:lem:fz_E} it follows
  \begin{align*}
    \E
    \left[
      \frac{f_z(T,X,Y(T))}{\pi(X)}
      +
      F_{Y(1)}(z|X)
      -
      F_{Y(1)}(z)
      \right]
      \ 
      =
      \ 
      \E
      \left[
      \frac{1}{\pi(X)}
      \E
      \left[
        f_z(T,X,Y(T))
        |X
      \right]
      \right]
      \ 
      =
      \ 
      0
      \,.
  \end{align*}
  Thus
  \begin{align*}
    R_4(z)
    &
  \
  =
  \ 
  \frac{1}{
  \sqrt{N}
  }
    \sum_{i=1}^{N} 
    \frac{T_i}{\pi(X_i)}
    \left( 
    \mathbf{1}{\left\{ Y_i \le z \right\}}
    -
  F_{Y(1)}(z|X_i)
    \right)
    \ 
    +
    \ 
    \left( 
  F_{Y(1)}(z|X_i)
    -
  F_{Y(1)}(z)
    \right)
    \\
    &
    \ 
  =
    \ 
  \frac{1}{
  \sqrt{N}
  }
    \sum_{i=1}^{N} 
      \frac{f_z(T_i,X_i,Y_i)}{\pi(X_i)}
      +
      \left( 
      F_{Y(1)}(z|X_i)
      -
      F_{Y(1)}(z)
      \right)
      \\
      &
      \ 
      =
      \ 
      \G_N 
      \left(
       \frac{f_z}{\pi(\cdot)}
      +
      F_{Y(1)}(z|\cdot)
      -
      F_{Y(1)}(z)
      \right)
      \,.
  \end{align*}
  By Lemma~\ref{aa:mean:l:br}
  it holds
  \begin{align*}
    &
  \log
  N_{[\,]}
    (
    \varepsilon
    ,
    \mathcal{G}, L^2(\P))
    \\
    &
    \ 
    \lesssim
    \ 
    \log
    \left(
      \frac
      {
      1+
    \norm{1/\pi(X)}_{ L^2(\P)}
      }
      {\varepsilon}
    \right)
    \ 
    \lesssim
    \ 
      \frac
      {
      1+
    \norm{1/\pi(X)}_{ L^2(\P)}
      }
      {\varepsilon}
    \qquad
    \text{for all}
    \ 
    \varepsilon\in (0,1)
    \,.
  \end{align*}
  Thus
  \begin{gather*}
    J_{[\,]}(1,\mathcal{G},L^2(\P))
    \ 
    \lesssim
    \ 
    \int_0^1
    \sqrt
    {
      \frac
      {
      1+
    \norm{1/\pi(X)}_{ L^2(\P)}
      }
      {\varepsilon}
    }
    \,
    d\varepsilon
    \ 
    \lesssim
    \ 
      1+
    \norm{1/\pi(X)}_{ L^2(\P)}
    \ 
    <
    \ 
    \infty
    \,.
  \end{gather*}
But then $\mathcal{G}$ is $\P$-Donsker.
By the Donsker Theorem \cite[Theorem~19.5]{Vaart2000}
the process $R_4$ converges in $l^\infty(\R)$ to a Gaussian process, called $\P$-Brownian bridge, with mean 0.
We now calculate the covariance of the limiting process.
\subsubsection*{Covariance}
\begin{align*}
  &
  \E
  \left[
  \left( 
  f^{z_1}_{1/\pi}
  +
  F_{Y(1)}(z_1|X)
  -
F_{Y(1)}(z_1)
  \right)
  \left( 
  f^{z_2}_{1/\pi}
  +
  F_{Y(1)}(z_2|X)
  -
F_{Y(1)}(z_2)
  \right)
  \right]
  \\
  &
  \ 
  =
  \ 
\E
\left[
  f^{z_1}_{1/\pi}
  \cdot
  f^{z_2}_{1/\pi}
\right]
\\
  &
  \quad
  +
  \ 
  \E
  \left[
  f^{z_1}_{1/\pi}
  \left( 
  F_{Y(1)}(z_2|X)
  -
F_{Y(1)}(z_2)
  \right)
  \right]
  \ 
  +
  \ 
  \E
  \left[
  f^{z_2}_{1/\pi}
  \left( 
  F_{Y(1)}(z_1|X)
  -
F_{Y(1)}(z_1)
  \right)
  \right]
  \\
  &
  \quad
  +
  \ 
  \E
  \left[
  \left( 
  F_{Y(1)}(z_1|X)
  -
F_{Y(1)}(z_1)
  \right)
  \left( 
  F_{Y(1)}(z_2|X)
  -
F_{Y(1)}(z_2)
  \right)
  \right]
  \\
  &
  \ 
  =:
  \ 
  C_0
  \quad 
  +
  \quad 
  C_1
  +
  C_2
  \quad 
  +
  \quad 
  C_3
  \,.
\end{align*}
It holds
\begin{align*}
  C_0 
  &
  \ 
  =
  \ 
\E
\left[
  f^{z_1}_{1/\pi}
  \cdot
  f^{z_2}_{1/\pi}
\right]
\\
&
\ 
=
\ 
\E
\left[
\frac{1}{\pi(X)}
\frac{T}{\pi(X)}
\left( 
\mathbf{1}{\left\{ Y(T)\,\le\, z_1 \right\}}
-
F_{Y(1)}(z_1|X)
\right)
\left( 
\mathbf{1}{\left\{ Y(T)\,\le\, z_2 \right\}}
-
F_{Y(1)}(z_2|X)
\right)
\right]
\\
&
\ 
=
\ 
\E
\left[
\frac{1}{\pi(X)}
\left( 
\mathbf{1}{\left\{ Y(1)\,\le\, z_1 \right\}}
-
F_{Y(1)}(z_1|X)
\right)
\left( 
\mathbf{1}{\left\{ Y(1)\,\le\, z_2 \right\}}
-
F_{Y(1)}(z_2|X)
\right)
\right]
\\
&
\ 
=
\ 
\E
\left[
\frac{1}{\pi(X)}
\left( 
F_{Y(1)}(z_1\land z_2|X)
\ 
-
\ 
F_{Y(1)}(z_1|X)
\cdot
F_{Y(1)}(z_2|X)
\right)
\right]
\,.
\end{align*}
\begin{align*}
  C_1
  &
  \ 
  =
  \ 
 \E
  \left[
  f^{z_1}_{1/\pi}
  \left( 
  F_{Y(1)}(z_2|X)
  -
F_{Y(1)}(z_2)
  \right)
  \right]
  \\
  &
  \ 
  =
  \ 
 \E
  \left[
\frac{T}{\pi(X)}
\left( 
\mathbf{1}{\left\{ Y(T)\,\le\, z_1 \right\}}
-
F_{Y(1)}(z_1|X)
\right)
  \left( 
  F_{Y(1)}(z_2|X)
  -
F_{Y(1)}(z_2)
  \right)
  \right]
  \\
  &
  \ 
  =
  \ 
 \E
  \left[
\left( 
\mathbf{1}{\left\{ Y(1)\,\le\, z_1 \right\}}
-
F_{Y(1)}(z_1|X)
\right)
  \left( 
  F_{Y(1)}(z_2|X)
  -
F_{Y(1)}(z_2)
  \right)
  \right]
  \\
  &
  \ 
  =
  \ 
  0
  \,.
\end{align*}
In the same way we see $C_2=0$.
\begin{align*}
  C_3
  &
  \ 
  =
  \ 
  \E
  \left[
  \left( 
  F_{Y(1)}(z_1|X)
  -
F_{Y(1)}(z_1)
  \right)
  \left( 
  F_{Y(1)}(z_2|X)
  -
F_{Y(1)}(z_2)
  \right)
  \right]
  \\
  &
  \ 
  =
  \ 
  \E
  \left[
  F_{Y(1)}(z_1|X)
  \cdot
  F_{Y(1)}(z_2|X)
  \right]
  \ 
  -
  \ 
  F_{Y(1)}(z_1)
  \cdot
  F_{Y(1)}(z_2)
  \,.
\end{align*}
Adding up the results gives us \eqref{cov:lp}.
\end{proof}
We have gathered all the results to prove Theorem~\ref{aa:mean:th}.
\begin{proof}
  \emph{(Theorem~\ref{aa:mean:th})}
  We connect the statement of the theorem to the error decomposition by Lemma~\ref{aa:mean:lemma_decomp}.
  By Lemma~\ref{aa:mean:l:r1}, Lemma~\ref{aa:mean:l:r2},
  Lemma~\ref{aa:mean:r3:lem:conv}
   it follows 
   $\sup_{z\in\R}|R_i(z)|\overset{\P}{\to}0$ for $i=1,2,3$.
   Thus, by Slutzky's theorem (cf.\cite[Theorem~13.18]{Klenke2020})
   the behaviour of the limiting process is the one of Lemma~\ref{aa:mean:l:r4}.
\end{proof}
