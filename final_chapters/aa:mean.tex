\subsection*{Notation}
Throughout this section we use the following notation.
Let $F_{Y(1)}$ denote the distribution function of $Y(1)$, that is,
\begin{gather*}
  F_{Y(1)}
  \ 
  \colon
  \ 
  \R
  \ 
  \to
  \ 
  [0,1]
  \, 
  , 
  \qquad
  z
  \ 
  \mapsto
  \ 
  \P
  [
  Y(1)
  \le
  z
  ]
  \,.
\end{gather*}
Let $F_{Y(1)}(\cdot|x)$ denote the distribution function of $Y(1)$ conditional on $X=x\in\mathcal{X}$, that is,
\begin{gather*}
  F_{Y(1)}
  (z|x)
\ 
  =
\ 
  \P
  [
  Y(1)
  \le
  z
  \,
  |
  \,
  X=x
  ]
  \qquad
  \text{for all}\ 
  (z,x)\in\R\times\mathcal{X}
  \,.
\end{gather*}
We recall from Definition~\ref{def:weights_function} the weight function 
\begin{gather*}
  w
  \colon
  \mathcal{X}
  \times
  \R^{N+1}
  \to
  \R
  \,,\qquad
  \left( 
  x
  ,
  \lambda
  ,
  \lambda_0
  \right)
  \mapsto
    (
    f^{'}
    )^{-1}
    \left( 
      \inner{B(x)}{\lambda^\dagger}
      +
      \lambda_0^\dagger
    \right)
    \,,
\end{gather*}
and that, if 
  the optimal solution
  $(\lambda^\dagger,\lambda_0^\dagger)$
  to Problem ? exists, we write
  \begin{gather*}
    w^\dagger(x)
    =
    w(x,\lambda^\dagger,\lambda^\dagger_0)
    \qquad
    \text{for all}\ 
    x\in\mathcal{X}\,.
  \end{gather*}
We introduced the basis functions $(B_k)$ in ?. 
Let $\pi(\cdot)$ denote the propensity score function, that is,
\begin{gather*}
  \pi
  \colon
  \mathcal{X}
  \to
  [0,1]
  \,,
  \qquad
  x\mapsto
  \P[T=1\,|\,X=x]
  \,.
\end{gather*}
Let the symbol
$\lesssim$ denote the lesser-equal order multiplied by a generic constant $C>1$ that is independent of $N$ or any other quantitative assumption. We always take $C$ large enough.
For example
\begin{gather*}
  17\cdot f(x)\ \lesssim\  f(x)
  \,.
\end{gather*}
This helps keeping the complexity of notation at a (necessary) minimum.

\subsection*{Goal of this section}
We illustrate the flexibility of 
the weighted mean estimator by 
extending the method of \cite{Wang2019} to
estimates of 
the distribution function of $Y(1)$, that is, $F_{Y(1)}$.
For the asymptotic analysis of estimating the mean $\E[Y(1)]$ see \cite[Proof of Theorem~3]{Wang2019}.
To make this extension, the central observation is, that we can adapt the error decomposition in \cite[page 27]{Wang2019} 
to estimates of the distribution function $F_{Y(1)}$ of $Y(1)$.
We do this in Lemma~\ref{aa:mean:lemma_decomp}.
With this modification, we aim at proving
the convergence of
\begin{gather}
    \sqrt{N}
    \left( 
  \frac{1}{N}
    \sum_{i=1}^{n} 
    w^\dagger(X_i)
    \mathbf{1}\left\{ Y_i(T_i)\,\le\, z \right\}
    \ 
    -
    \ 
    F_{Y(1)}(z)
    \right)
    _{z\in\R}
    \,
  \end{gather}
  in
  $l^\infty(\R)$
  to a Gaussian process with mean 0 and covariance specified in Theorem~\ref{aa:mean:th}.
  We show, that this is possible under mild assumptions.
  We discuss this in the next section.
\begin{ftheorem}
  \label{aa:mean:th}
  Under conditions 
the stochastic process
\begin{gather}
    \sqrt{N}
    \left( 
  \frac{1}{N}
    \sum_{i=1}^{n} 
    w_i^\dagger
    \mathbf{1}
    _{\left\{ Y_i\,\le\, z \right\}}
    \ 
    -
    \ 
    F_{Y(1)}(z)
    \right)
    _{z\in\R}
    \,
  \end{gather}
  converges in
  $l^\infty(\R)$
  to a Gaussian process with mean 0 and covariance
\begin{align}
  \label{cov:lp}
 \begin{split}
  &
  \mathbf{Cov}
  (z_1,z_2)
  \\
  &
  =\ 
  \E
  \left[ 
 \frac{
 F_{Y(1)}(z_1 \land z_2\,|\,X)
}{\pi(X)}
\ 
-
\ 
 \frac{1-\pi(X)}{\pi(X)}
 F_{Y(1)}(z_1|X)
 \cdot
 F_{Y(1)}(z_2|X)
  \right]
  \\
  &
  \qquad 
 -
 \ 
 F_{Y(1)}(z_1)
 \cdot
 F_{Y(1)}(z_2)
 \end{split}
\end{align}
\end{ftheorem}

\pagebreak
\begin{lemma}
  \label{aa:mean:lemma_decomp}
  It holds
  \begin{gather}
  \sqrt{N}
\left( 
    \frac{1}{N}
    \sum_{i=1}^{n} 
    w(X_i)
    \mathbf{1}{\left\{ Y_i \le z \right\}}
    \ 
    -
    \ 
    F_{Y(1)}(z)
    \right)
    _{z\in\R}
    \ 
    =
    \ 
    R_1
    \ 
    +
    \ 
    R_2
    \ 
    +
    \ 
    R_3
    \ 
    +
    \ 
    R_4
  \end{gather}
  with
\begin{align*}
  R_1
  &
  \ 
  :=
  \ 
  \sqrt{N}
  \sum_{k=1}^{N} 
  \left[ 
  \frac{1}{N}
  \left( 
    \sum_{i=1}^{n} 
    w(X_i)
    B_k(X_i)
    -
    \sum_{i=1}^{N} 
    B_k(X_i)
  \right)
  F_{Y(1)}(z|X_k)
  \right]
  _{z\in\R}
  \,,
  %%%% 1 %%%%
  \\
  R_2
  &
  \
  :=
  \ 
  \sqrt{N}
    \sum_{i=1}^{N} 
    \left[ 
  \frac{
    T_i\cdot w(X_i) -1 }{N}
    \left( 
  F_{Y(1)}(z|X_i)
    \ 
    -
    \ 
    \sum_{k=1}^{N} 
    B_k(X_i)
    \cdot
  F_{Y(1)}(z|X_k)
    \right)
    \right]
  _{z\in\R}
  \,,
  %%%% 2 %%%%
  \\
  R_3
  &
  \
  :=
  \ 
  \sqrt{N}
  \left( 
  \frac{1}{N}
    \sum_{i=1}^{N} 
    \left[ 
    T_i
    \left( 
    w(X_i) 
    -
    \frac{1}{\pi(X_i)}
    \right)
    \left( 
    \mathbf{1}{\left\{ Y_i \le z \right\}}
    -
  F_{Y(1)}(z|X_i)
    \right)
    \right]
  \right)
  _{z\in\R}
  \,,
  %%%% 3 %%%%
  \\
  R_4
  &
  \
  :=
  \ 
  \sqrt{N}
  \left( 
  \frac{1}{N}
    \sum_{i=1}^{N} 
    \frac{T_i}{\pi(X_i)}
    \left( 
    \mathbf{1}{\left\{ Y_i \le z \right\}}
    -
  F_{Y(1)}(z|X_i)
    \right)
    \ 
    +
    \ 
    \left( 
  F_{Y(1)}(z|X_i)
    -
  F_{Y(1)}(z)
    \right)
  \right)
  _{z\in\R}
  \,.
  \end{align*}
\end{lemma}
\nopagebreak
\begin{proof}
  We fix $z\in\R$.
  It holds
  \begin{align*}
    &
    \frac{1}{N}
    \sum_{i=1}^{N} 
    w^\dagger(X_i)
    \cdot
    T_i
    \cdot
    \mathbf{1}{\left\{ Y_i(T_i)\, \le\, z \right\}}
    \\
    &
    \ 
    =
    \ 
    \frac{1}{N}
    \sum_{i=1}^{N} 
    \left( 
    w^\dagger(X_i)
    -
    \frac{1}{\pi(X_i)}
    \right)
    T_i
    \cdot
    \mathbf{1}{\left\{ Y_i(T_i)\, \le\, z \right\}}
    \\
    &
    \quad 
    +
    \ 
    \frac{1}{N}
    \sum_{i=1}^{N} 
    \frac{T_i}{\pi(X_i)}
    \mathbf{1}{\left\{ Y_i(T_i)\, \le\, z \right\}}
    \\
    &
    \ 
    =
    \ 
    \frac{1}{N}
    \sum_{i=1}^{N} 
    \left( 
    w^\dagger(X_i)
    -
    \frac{1}{\pi(X_i)}
    \right)
    T_i
    \left( 
    \mathbf{1}{\left\{ Y_i(T_i)\, \le\, z \right\}}
    -
    F_{Y(1)}(z|X_i)
    \right)
    \\
    &
    \quad 
    +
    \ 
    \frac{1}{N}
    \sum_{i=1}^{N} 
    \frac{T_i}{\pi(X_i)}
    \left( 
    \mathbf{1}{\left\{ Y_i(T_i)\, \le\, z \right\}}
    -
    F_{Y(1)}(z|X_i)
    \right)
    \\
    &
    \qquad 
    +
    \ 
    \frac{1}{N}
    \sum_{i=1}^{N} 
    w^\dagger(X_i)\cdot T_i\cdot
    F_{Y(1)}(z|X_i)
    \\
    &
    \ 
    =
    \ 
    R_3(z)/\sqrt{N}
    \\
    &
    \quad 
    +
    \ 
    \frac{1}{N}
    \sum_{i=1}^{N} 
    \frac{T_i}{\pi(X_i)}
    \left( 
    \mathbf{1}{\left\{ Y_i(T_i)\, \le\, z \right\}}
    -
    F_{Y(1)}(z|X_i)
    \right)
    +
    \left( 
    F_{Y(1)}(z|X_i)
    -
    F_{Y(1)}(z)
    \right)
    \\
    &
    \qquad
    +
    \ 
    \frac{1}{N}
    \sum_{i=1}^{N} 
    \left( 
    w^\dagger(X_i)\cdot T_i
    \ 
    -
    \ 
    1
    \right)
    F_{Y(1)}(z|X_i)
    \\
    &
    \quad\qquad
    +
    \ 
    F_{Y(1)}(z)
    \\
    &
    \ 
    =
    \ 
    R_3(z)/\sqrt{N}
    \\
    &
    \quad
    +
    \ 
    R_4(z)/\sqrt{N}
    \\
    &
    \qquad
    +
    \ 
    \frac{1}{N}
    \sum_{i=1}^{N} 
    \left( 
    w^\dagger(X_i)\cdot T_i
    \ 
    -
    \ 
    1
    \right)
    \left( 
    F_{Y(1)}(z|X_i)
    -
    \sum_{k=1}^{N} 
    B_k(X_i)
    \cdot
  F_{Y(1)}(z|X_k)
    \right)
    \\
    &
    \quad\qquad
    +
    \ 
    \frac{1}{N}
    \sum_{i=1}^{N} 
    \left( 
    w^\dagger(X_i)\cdot T_i
    \ 
    -
    \ 
    1
    \right)
    \sum_{k=1}^{N} 
    B_k(X_i)
    \cdot
  F_{Y(1)}(z|X_k)
    \\
    &
    \qquad\qquad
    +
    \ 
    F_{Y(1)}(z)
\\
    &
    \ 
    =
    \ 
    R_3(z)/\sqrt{N}
    \\
    &
    \quad
    +
    \ 
    R_4(z)/\sqrt{N}
    \\
    &
    \qquad
    +
    \ 
    R_2(z)/\sqrt{N}
    \\
    &
    \quad\qquad
    +
    \ 
    \sum_{k=1}^{N} 
    \frac{1}{N}
    \sum_{i=1}^{N} 
    \left( 
    w^\dagger(X_i)\cdot T_i
    B_k(X_i)
    \ 
    -
    \ 
    B_k(X_i)
    \right)
    \cdot
  F_{Y(1)}(z|X_k)
    \\
    &
    \qquad\qquad
    +
    \ 
    F_{Y(1)}(z)
    \\
    &
    \ 
    =
    \ 
    \left( 
R_3(z)
    \ 
    +
    \ 
    R_4(z)
    \ 
    +
    \ 
    R_2(z)
    \ 
    +
    \ 
    R_1(z)
    \right)
    /\sqrt{N}
    \ 
    +
    \ 
    F_{Y(1)}(z)
    \,.
  \end{align*}
  This holds for all $z\in\R$.
  Multiplying with $\sqrt{N}$ yields the result.
\end{proof}
Let $F_{Y(1)}(z|x):=\P[Y(1)\le z|X=x]$ denote a conditional version of the distribution function of $Y(1)$ at $x\in\mathcal{X}$.
We also need the propensity score
$
  \pi(x):=\P[T=1|X=x]
$
and the weights function
$
  w(x)
  :=
  (
  f^{'}
  )^{-1}
  \left( 
    \inner{B(x)}{\lambda^\dagger}
    +
    \lambda_0^\dagger
  \right)
$.
\begin{lemma}
  \label{aa:mean:l:r1}
Let the weights function $w$ satisfy the box constraints in 
Problem~\ref{bw:1:primal} and 
$\sqrt{N}\norm{\delta}_1\overset{\P}{\to}0$.
Then it holds
$\sup_{z\in\R}|R_1(z)|\overset{\P}{\to}0$.
  \end{lemma}
\begin{proof}
%  By Theorem~\ref{dual_solution_th}
%  it holds $w_i^\dagger=w(X_i)$ for $i\in \left\{ 1,\ldots,n \right\}$, that is, for $i\le n$ we can identify $w(X_i)$ with the optimal
%  solution to 
%  problem~\ref{bw:1:primal}. 
%  Thus the constraints of the problem apply.
%   Let's bound $R_1$.
It holds
  \begin{align}
    \label{R_1:1}
    \begin{split}
    \sup_{z\in\R}
    \left| 
    R_1(z)
    \right|
    &
    \ 
    =
    \ 
    \sup_{z\in\R}
    \left| 
  \sqrt{N}
  \sum_{k=1}^{N} 
  \left[ 
  \frac{1}{N}
  \left( 
    \sum_{i=1}^{n} 
    w(X_i)
    B_k(X_i)
    -
    \sum_{i=1}^{N} 
    B_k(X_i)
  \right)
  F_{Y(1)}(z|X_k)
  \right]
    \right|
    \\
    &
    \ 
    \le
    \ 
  \sqrt{N}
  \sum_{k=1}^{N} 
  \left| 
  \frac{1}{N}
  \left( 
    \sum_{i=1}^{n} 
    w(X_i)
    B_k(X_i)
    -
    \sum_{i=1}^{N} 
    B_k(X_i)
  \right)
  \right|
    \sup_{z\in\R}
  F_{Y(1)}(z|X_k)
  \\
    &
    \ 
    \le
    \ 
  \sqrt{N}
  \norm{\delta}_1
    \end{split}
  \end{align}
  The last inequality is due to $F_{Y(1)}\in[0,1]$ and the assumption that $(w(X_i))$ satisfies the box constraints of Problem~\ref{bw:1:primal}.
  Since we assume 
$\sqrt{N}\norm{\delta}_1\overset{\P}{\to}0$
it holds
$\sup_{z\in\R}|R_1(z)|\overset{\P}{\to}0$.
\end{proof}
\begin{remark}
  We want to comment on the box constraints of Problem~\ref{bw:1:primal}, that is,
 \begin{gather*}
      \left| 
      \frac{1}{N} 
      \left( 
      \sum_{i = 1}^{n} 
      w(X_i)
      B_k(X_i)
      -
      \sum_{i=1}^{N} 
      B_k(X_i)
      \right)
    \right|
    \ 
    \le 
    \ 
    \delta_k
    \qquad
    \text{for all}\ 
    k \in \left\{ 1, \ldots, N \right\}
    \,.
  \end{gather*}
  Note, that the first sum goes over $\left\{ 1,\ldots,n \right\}$ while the second sum goes over $\left\{ 1,\ldots,N \right\}$.
  A second, equivalent version of the constraints is
  \begin{gather*}
      \left| 
      \frac{1}{N} 
      \left( 
      \sum_{i = 1}^{N} 
      T_i
      w(X_i)
      B_k(X_i)
      -
      \sum_{i=1}^{N} 
      B_k(X_i)
      \right)
    \right|
    \ 
    \le 
    \ 
    \delta_k
    \qquad
    \text{for all}\ 
    k \in \left\{ 1, \ldots, N \right\}
    \,.
  \end{gather*}
  Now both sums go over $\left\{ 1,\ldots,N \right\}$ and the
  indicator of treatment $T_i$ takes care that in the first sum only the terms with $i\le n$ are effective. 
  Having this flexibility with the versions helps. I regard the first version as suitable for non-probabilistic computations, although $n$ is of course a random variable. On the other hand, the second version is more honest, exactly telling the dependence on the indicator of treatment. This version is useful in probabilistic computations. 

  Also we want to comment on the assumption on $\norm{\delta}$.
  Playing around with norm equivalences we discover that 
  $\sqrt{N}\norm{\delta}_1\overset{\P}{\to}0$ for $N\to \infty$ is the weakest
  (natural) assumption to
  control $R_1$.
  Indeed, other ways to continue the second row in \eqref{R_1:1} are
  \begin{gather*}
    (\,\cdots)
    \ 
  \le
    \ 
  \sqrt{N}
  \norm{\delta}_2
  \left( 
  \sum_{k=1}^{N} 
  \left( 
    \sup_{z\in\R}
  F_{Y(1)}(z|X_k)
  \right)^2
\right)^{1/2}
\ 
\le
\ 
N
  \norm{\delta}_2\,,
  \end{gather*}
  by the Cauchy-Schwarz inequality and
  $
  F_{Y(1)}\in [0,1]
  $,
or
\begin{gather*}
  (\,\cdots)
  \ 
  \le
  \ 
  \sqrt{N}
  \norm{\delta}_\infty
  \sum_{k=1}^{N} 
    \sup_{z\in\R}
  F_{Y(1)}(z|X_k)
  \ 
  \le
  \ 
  N^{3/2}
  \norm{\delta}_\infty
  \,.
\end{gather*}
Since $\delta\in \R^N$, however, it holds
\begin{gather*}
  \sqrt{N}\norm{\delta}_1
  \ 
  \le
  \ 
  N\norm{\delta}_2
  \ 
  \le
  \ 
  N^{3/2}\norm{\delta}_\infty
  \,.
\end{gather*}
With hindsight, the assumption 
$\sqrt{N}\norm{\delta}_1\overset{\P}{\to}0$ for $N\to \infty$ 
  also 
  suffices 
  to control the second (or first) occurrence of a term, that we control by assumptions on $\norm{\delta}$.
This is the \textbf{second term} of \eqref{c:1}, where we estimate
\begin{gather*}
  \inner{\delta}{\left| \Delta \right|}
  \ 
  =
  \ 
  \sum_{k=1}^{N} 
  \delta_k
  \left| \Delta_k \right|
  \ 
  \le
  \ 
  \norm{\delta}_1
  \norm{\Delta}_\infty
  \ 
  \le
  \ 
  \norm{\delta}_1
  \norm{\Delta}_2
  \ 
  \le
  \ 
  \norm{\delta}_1
  \varepsilon
  \ 
  \overset{\P}{\to}
  \ 
  0
  \quad
  \text{for}\ 
  N\to \infty
  \,.
\end{gather*}

\end{remark}
  \begin{lemma}
    \label{aa:mean:l:r2}
    Let the conditions of Theorem~\ref{aa:weights:th} hold true.
    Furthermore assume, that the width of the partitioning estimate $h_N$ and a conditional version of the distribution function of $Y(1)$ satisfy
\begin{gather*}
  \sqrt{N}
  \sup_{z\in\R}
  \omega
  \left( 
    F_{Y(1)}(z|\cdot)
    ,h_N
  \right)
  \to
  0
  \qquad
  \text{for}\ 
  N\to \infty
  \,,
\end{gather*}
where $\omega$ is the modulus of continuity.
Then it holds
$\sup_{z\in\R}|R_2(z)|\overset{\P}{\to}0$.
  \end{lemma}
\begin{proof}
\begin{align*}
&
  \sup_{z\in\R}
  \left| 
  R_2(z)
  \right|
  \\
  &
  \ 
  \le
  \ 
  \sqrt{N}
    \sum_{i=1}^{N} 
    \left[ 
  \frac{
    T_i\cdot w(X_i) -1 }{N}
  \sup_{z\in\R}
    \left| 
  F_{Y(1)}(z|X_i)
    \ 
    -
    \ 
    \sum_{k=1}^{N} 
    B_k(X_i)
    \cdot
  F_{Y(1)}(z|X_k)
    \right|
    \right]
    \\
    &
  \ 
    \le
  \ 
    \sqrt{N}
  \sup_{z\in\R}
  \omega
  \left( 
    F_{Y(1)}(z|\cdot)
    ,h_N
  \right)
  \sum_{i=1}^{N} 
  \frac{
    T_i\cdot w(X_i) +1 }{N}
    \\
    &
  \ 
    =
  \ 
    2
    \sqrt{N}
  \sup_{z\in\R}
  \omega
  \left( 
    F_{Y(1)}(z|\cdot)
    ,h_N
  \right)
  \,.
\end{align*}
The equality is due to 
\begin{gather}
  1
  \ 
  =
  \ 
\frac{1}{N}\sum_{i=1}^{n}w_i^\dagger
  \ 
  =
  \ 
\frac{1}{N}\sum_{i=1}^{n}w(X_i)
  \ 
=
  \ 
\frac{1}{N}\sum_{i=1}^{N}T_iw(X_i)
\,,
\end{gather}
that is, $w(X_i)$ satisfy the second constraint of Problem~\ref{bw:1:primal}.
The second inequality follows from 
$\sum_{k=1}^{N}B_k(X)=1$ and the convexity of the absolute value. 
Indeed,
\begin{align*}
&
  \sup_{z\in\R}
  \left| 
  F_{Y(1)}(z|X_i)
  -
  \sum_{k=1}^{N} 
  B_k(X_i)
  \cdot
  F_{Y(1)}(z|X_k)
  \right|
  \\
  &
  \ 
  \le
  \ 
  \sum_{k=1}^{N} 
  \frac{\mathbf{1}{\left\{ X_k\in A_N(X_i) \right\}}}
  {
    \sum_{j=1}^{N} 
\mathbf{1}{\left\{ X_j\in A_N(X_i) \right\}}
  }
  \sup_{z\in\R}
  \left| 
  F_{Y(1)}(z|X_i)
  -
  F_{Y(1)}(z|X_k)
  \right|
  \\
  &
  \ 
  \le
  \ 
  \sup_{z\in\R}
  \omega
  \left( 
    F_{Y(1)}(z|\cdot)
    ,h_N
  \right)
  \,.
\end{align*}

Since we assume
\begin{gather*}
  \sqrt{N}
  \sup_{z\in\R}
  \omega
  \left( 
    F_{Y(1)}(z|\cdot)
    ,h_N
  \right)
  \to
  0
  \qquad
  \text{for}\ 
  N\to \infty
  \,,
\end{gather*}
it follows
$\sup_{z\in\R}|R_2(z)|\overset{\P}{\to}0$.
\end{proof}
\begin{remark}
In the original paper \cite{Wang2019} the authors derive concrete learning rates for the weights and employ them in bounding this term. They obtain a multiplied learning rate, which is sufficiently fast. Their approach, however, calls for concrete learning rates of the weights. Arguably, the process of deriving such rates is the most complicated part of the paper. 
I found out, that we don't need concrete rates for the weights. 
Consistency of the weights is enough and gives us an (arbitrarily slow but sufficient) learning rate to establish the results.
We don't even need rates for the weights to control $R_2$.
They only play a role in bounding $R_3$.

We also want to comment on the assumption
\begin{gather*}
  \sqrt{N}
  \sup_{z\in\R}
  \omega
  \left( 
    F_{Y(1)}(z|\cdot)
    ,h_N
  \right)
  \to
  0
  \qquad
  \text{for}\ 
  N\to \infty
  \,,
\end{gather*}
I decided to keep this more general (and abstract) assumption, althogh
there are many (more concrete, yet stronger) assumptions on the regularity of
$
    F_{Y(1)}(z|\cdot)
$
and the convergence speed of $h_N$.
If for example 
$
    F_{Y(1)}(z|\cdot)
$
is $\alpha$-HÃ¶lder continuous with $\alpha\in(0,1]$ for all $z\in\R$, it suffices $\sqrt{N}h_N^\alpha\to0$ to control $R_2$.
\end{remark}


Next, we define some auxiliary functions.
  For $z\in\R$ we define the function
  \begin{align*}
    f_z
    \ 
    :
    \ 
      \left\{ 0,1 \right\}
      \times
      \mathcal{X}
      \times
      \mathcal{Y}
    &
    \ 
    \to
    \ 
    \R
    \\
      (t,x,y)
    &
      \ 
      \mapsto
      \ 
      t
      \left( 
        \mathbf{1}
        _{\left\{  y\,\le\,z \right\}}
        -
        F_{Y(1)}(z|x)
      \right)
      \,,
  \end{align*}
  and the function classes
  \begin{gather}
    \label{F_g}
    \begin{split}
    \mathcal{F}
    &
    \ 
    :=
    \ 
    \left\{ 
      f_z
      \ 
      |
      \ 
      z\in\R\ 
    \right\}
    \\
    \mathcal{G}
    &
    \ 
    :=
    \ 
    \left\{ 
      \frac{f_z}{\pi(\cdot)}
      +
      F_{Y(1)}(z|\cdot)
      -
      F_{Y(1)}(z)
      \ 
      \colon
      \ 
      z\in\R\ 
    \right\}
    \,.
    \end{split}
  \end{gather}
  \begin{assumption}
    \label{aa:assumption:treatment_str_ign}
    It holds 
    \begin{gather*}
    (Y(0),Y(1))\, \perp\,  T \ |\  X
    \qquad
    \text{and}
    \qquad
      0
      \ 
      <
      \ 
      \pi(x)
      \ 
      <
      \ 
      1
      \quad
      \text{for all}\ 
      x\in\mathcal{X}
      \,.
    \end{gather*}
  \end{assumption}
  \begin{lemma}
    \label{aa:mean:r3:lem:fz_E}
    It holds
    $f_z(T,X,Y(T))\in L^1(\P)$
    and 
    $f_z(T,X,Y(T))\perp D_N$
    for all $z\in\R$.
    If also Assumption~\ref{aa:assumption:treatment_str_ign} holds,
    then 
    for all $z\in\R$
    \begin{gather*}
      \E
      \left[
        f_z
        \left( 
          T,
          X,
          Y(T)
        \right)
        \,
        |
        \,
        X
      \right]
      \ 
      =
      \ 
      0
      \qquad
      \text{almost surely.}
    \end{gather*}
  \end{lemma}
  \begin{proof}
    Since $f_z$ is bounded by 1,
    it holds
    $f_z(T,X,Y(T))\in L^1(\P)$.
    Since
    \begin{gather*}
    (T,X,Y(T))
    \ 
    \perp 
    \ 
    D_N
    \ 
    =
    \ 
    (T_i,X_i)_{i\in \left\{ 1,\ldots,N \right\}}
    \end{gather*}
    it holds
    $f_z(T,X,Y(T))\perp D_N$
    for all $z\in\R$.
    For the third statement, note that
    \begin{align*}
      \E
      \left[
        f_z
        \left( 
          T,
          X,
          Y(T)
        \right)
        \,
        |
        \,
        X
      \right]
      &
      \ 
      =
      \ 
      \E
      \left[
      T
      \left( 
        \mathbf{1}
        _{\left\{  Y(T)\,\le\,z \right\}}
        -
        F_{Y(1)}(z|X)
      \right)
        \,
        |
        \,
        X
      \right]
      \\
      &
      \ 
      =
      \ 
      \E
      \left[
        \mathbf{1}
        _{\left\{  Y(1)\,\le\,z \right\}}
        -
        F_{Y(1)}(z|X)
        \,
        |
        \,
        X
        ,
        T=1
      \right]
      \pi(X)
      \\
      &
      \ 
      =
      \ 
      \left( 
      \E
      \left[
        \mathbf{1}
        _{\left\{  Y(1)\,\le\,z \right\}}
        \,
        |
        \,
        X
      \right]
      \ 
        -
      \ 
        F_{Y(1)}(z|X)
      \right)
      \pi(X)
      \\
      &
      \ 
      =
      \ 
      0
      \qquad
      \text{almost surely.}
    \end{align*}
    The third equality is due to Assumption~\ref{aa:assumption:treatment_str_ign}.
  \end{proof}
The next lemma provides bracketing numbers for specific function classes needed to control $R_3$ and $R_4$.
\newpage
\begin{lemma}
  \label{aa:mean:l:br}
  The function class $\mathcal{F}$ and $\mathcal{G}$ defined in \eqref{F_g} are measurable.
  Furthermore, 
  \begin{gather*}
    N_{[\,]}
    (
    \varepsilon
    ,
    \mathcal{F}, L^2(\P))
    \ 
    \lesssim
    \ 
    \left( 
      \frac{1}{\varepsilon}
    \right)^2
    \qquad
    \text{for all}
    \ 
    \varepsilon>0
    \,.
  \end{gather*}
  If $1/\pi(X)\in L^2(\P)$, it also holds 
  \begin{gather*}
    N_{[\,]}
    (
    \varepsilon
    ,
    \mathcal{G}, L^2(\P))
    \ 
    \lesssim
    \ 
    \left( 
    \frac{
      1+
    \norm{1/\pi(X)}_{ L^2(\P)}
    }
    {\varepsilon}
    \right)^4
    \qquad
    \text{for all}
    \ 
    \varepsilon>0
    \,.
  \end{gather*}
%  Furthermore, 
%  consider the function class
%  \begin{gather*}
%    \mathcal{G}
%    :=
%    \left\{ 
%      f_{1/\pi}^z
%      +
%        F_{Y(1)}(z|\cdot)
%        -
%        F_{Y(1)}(z)
%      \ 
%      |
%      \ 
%      z\in\R
%    \right\}
%    \,.
%  \end{gather*}
%  If $1/\pi(X)\in  L^2(\P)$
%  it holds
%  \begin{gather}
%    N_{[\,]}(\varepsilon,\mathcal{G}, L^2(\P))
%    \le
%    ??
%    \qquad
%    \text{for all}
%    \ 
%    \varepsilon>0
%    \,.
%  \end{gather}
\end{lemma}
\begin{proof}
  As in \cite[Example~19.6]{Vaart2000}
  we choose for
  $\varepsilon>0$ and $m\in\mathbb{N}$
  \begin{gather*}
  -\infty=z_0\ <\ z_1\ <\ \cdots\ <\ z_{m-1}\ <\ z_m=\infty
  \,
  \end{gather*}
  such that
  \begin{gather}
    \label{size_z}
    \P
    \left[ 
      Y(1)\in \left[ z_{l-1},z_l \right]\,
    \right]
    \ 
    \le
    \ 
    \varepsilon
    \qquad
    \text{for all}\ 
    l\in \left\{ 1,\ldots,m \right\}
  \end{gather}
  and $m \le 2/\varepsilon$.
  Next, we define $m$ brackets by
\begin{align*}
  \overline{f_l}
  (t,x,y)
  &
  \ 
  :=
  \ 
      t
      \left( 
        \mathbf{1}
        _{\left\{  y\,\le\,z_{l} \right\}}
        -
        F_{Y(1)}(z_{l-1}|x)
      \right)
      \,,
      \\
  \underline{f_l}
  (t,x,y)
  &
  \ 
  :=
  \ 
      t
      \left( 
        \mathbf{1}
        _{\left\{  y\,\le\,z_{l-1} \right\}}
        -
        F_{Y(1)}(z_l|x)
      \right)
      \,,
\end{align*}
for $l\in \left\{ 1,\ldots,m \right\}$.
These brackets cover $\mathcal{F}$.
Indeed,
\begin{gather*}
  \text{for all}\ 
  z\in\R
  \ 
  \text{there exists} \ 
l\in \left\{ 1,\ldots,m \right\}
\qquad 
\text{such that}\qquad
z_{l-1}
\ 
\le
\ 
z
\ 
\le
\ 
z_l
\,.
\end{gather*}
By the monotonicity of 
$
        \mathbf{1}
      _{\left\{  y\,\le\,(\cdot) \right\}}
$
and
$
        F_{Y(1)}(\cdot|x)
$
and the non-negativity of $T$ it follows
\begin{gather*}
  \text{for all}\ 
  z\in\R
  \ 
  \text{there exists} \ 
l\in \left\{ 1,\ldots,m \right\}
\qquad 
\text{such that}\qquad
  \underline{f_l}
  \ 
  \le
  \ 
  f_z
  \ 
  \le
  \ 
  \overline{f_l}
  \,.
\end{gather*}
Thus, the $m$ brackets 
$
[
  \underline{f_l}
  ,
  \overline{f_l}
]
$
cover $\mathcal{F}$.

Let's calculate the size of the brackets.
It holds
\begin{align*}
  &
\E
\left[ 
      T
      \cdot
      \left( 
        \mathbf{1}
        _{\left\{  Y(T)\,\le\,z_{l} \right\}}
        -
        F_{Y(1)}(z_{l-1}|X)
        \ 
        -
        \ 
        \mathbf{1}
        _{\left\{  Y(T)\,\le\,z_{l-1} \right\}}
        +
        F_{Y(1)}(z_{l}|X)
      \right)
      \,
\right]
\\
  &
  \ 
=
  \ 
\E
\left[ 
      T
      \cdot
      \left( 
        \mathbf{1}
        _{\left\{
        Y(T)
        \,
        \in 
        \,
    [z_{l-1},z_l]
\right\}}
\ 
        +
\ 
        \P
        \left[ 
          Y(1)
          \in
    [z_{l-1},z_l]
        \,
    |
        \,
    X
        \right]
      \right)
      \,
\right]
\\
  &
  \ 
\le
  \ 
\E
\left[ 
  \,
  \pi(X)
  \cdot
        \P
        \left[ 
          Y(1)
          \in
    [z_{l-1},z_l]
          \,
    |
          \,
    X
        \right]
          \ 
\right]
\ 
+
\ 
\varepsilon
\\
  &
  \ 
\le
  \ 
2
\,
\varepsilon
\,.
\end{align*}
We used \eqref{size_z}, $0\le T,\pi(X)\le 1$ and Lemma~\ref{ps_weights_lemma}.
It follows
\begin{align*}
  &
  \norm{
    \left( 
  \overline{f_l}
-
  \underline{f_l}
    \right)
  (T,X,Y(T))
}_
{ L^2(\P)}
\\
&
\ 
\lesssim
\ 
\E
\left[ 
  \,
      T
      \cdot
      \left( 
        \mathbf{1}
        _{\left\{
        Y(T)
        \,
        \in 
        \,
    [z_{l-1},z_l]
\right\}}
\ 
        +
\ 
        \P
        \left[ 
          Y(1)
          \in
    [z_{l-1},z_l]
        \,
    |
        \,
    X
        \right]
      \right)
      \,
   \right]^{1/2}
\ 
\lesssim
\ 
\varepsilon^{1/2}
\,.
\end{align*}
Since $m\le 2/\varepsilon$ it holds
  \begin{align*}
    N_{[\,]}
    \left(
\varepsilon^{1/2}
    ,
    \,
    \mathcal{F}\,,\, L^2(\P)
    \right)
    &
    \ 
    \lesssim
    \ 
    \frac{1}{\varepsilon}
    \intertext{and thus}
    N_{[\,]}
    (
    \varepsilon
    ,
    \mathcal{F}, L^2(\P))
    &
    \ 
    \lesssim
    \ 
    \left( 
      \frac{1}{\varepsilon}
    \right)^2
    \,.
  \end{align*}
  Next, we look at $\mathcal{G}$. To this end, we define 
  $m$ brackets by
 \begin{align*}
    \overline{g_l}
    (t,x,y)
    \ 
    :=
    \ 
    \frac{t}{\pi(x)}
    \left( 
      \mathbf{1}{\left\{  y\,\le\,z_{l} \right\}}
      -
      F_{Y(1)}(z_{l-1}|x)
    \right)
    \ 
    +
    \ 
    F_{Y(1)}(z_{l}|x)
-
F_{Y(1)}(z_{l-1})
\,,
\\
    \underline{g_l}
    (t,x,y)
    \ 
    :=
    \ 
    \frac{t}{\pi(x)}
    \left( 
      \mathbf{1}{\left\{  y\,\le\,z_{l-1} \right\}}
      -
      F_{Y(1)}(z_l|x)
    \right)
    \ 
    +
    \ 
    F_{Y(1)}(z_{l-1}|x)
-
      F_{Y(1)}(z_l)
\,,
  \end{align*}
  for $l\in \left\{ 1,\ldots,m \right\}$.
  With the same arguments as before, we see that these brackets cover $\mathcal{G}$.
  Let's calculate the size.
  It holds
  \begin{align*}
    &
    \norm{
      \frac{T}{\pi(X)}
      \left( 
      \mathbf{1}{
      \left\{ 
      Y(T)\in [z_{l-1},z_l] 
    \right\}
    }
      +
      \P
      \left[ 
      Y(1)\in [z_{l-1},z_l] 
      \,
      |
      \,
      X
      \right]
      \right)
    }_{ L^2(\P)}
    \\
    &
    \
    \lesssim
    \
    \left( 
      \E
      \left[ 
        \frac{1}{\pi(X)}
        \frac{T}{\pi(X)}
      \left( 
      \mathbf{1}{
      \left\{ 
      Y(T)\in [z_{l-1},z_l] 
    \right\}
    }
      +
      \P
      \left[ 
      Y(1)\in [z_{l-1},z_l] 
      \,
      |
      \,
      X
      \right]
      \right)
      \right]
    \right)
    ^{1/2}
    \\
    &
    \
    \lesssim
    \
    \left( 
      \E
      \left[ 
        \frac{1}{\pi(X)}
      \P
      \left[ 
      Y(1)\in [z_{l-1},z_l] 
      \,
      |
      \,
      X
      \right]
      \right]
    \right)
    ^{1/2}
    \\
    &
    \ 
    \lesssim
    \ 
    \left( 
      \norm{1/\pi(X)}_{ L^2(\P)}
      \sqrt{\varepsilon}
    \right)
    ^{1/2}
    \ 
    =
    \ 
    \varepsilon^{1/4}
    \norm{1/\pi(X)}_{ L^2(\P)}^{1/2}
  \end{align*}
  and
  \begin{align*}
     \norm{
      \P
      \left[ 
      Y(1)\in [z_{l-1},z_l] 
      \,
      |
      \,
      X
      \right]
      \ 
     + 
      \ 
      \P
      \left[ 
      Y(1)\in [z_{l-1},z_l] \,
      \right]
    }_{ L^2(\P)}
    \ 
    \lesssim
    \ 
    \varepsilon^{1/2}
    \,.
  \end{align*}
  Thus
  \begin{align*}
  \norm{
    \left( 
  \overline{g_l}
-
  \underline{g_l}
    \right)
  (T,X,Y(T))
}_
{ L^2(\P)}
    &
\ 
\lesssim
\ 
\varepsilon^{1/4}
\left( 
  1
  +
    \norm{1/\pi(X)}_{ L^2(\P)}^{1/2}
\right)
\\
    &
\ 
\lesssim
\ 
\varepsilon^{1/4}
\left( 
  1
  +
    \norm{1/\pi(X)}_{ L^2(\P)}
\right)
\,.
  \end{align*}
As before, it follows
\begin{gather*}
    N_{[\,]}
    (
    \varepsilon
    ,
    \mathcal{G}, L^2(\P))
    \ 
    \lesssim
    \ 
    \left( 
    \frac{
      1+
    \norm{1/\pi(X)}_{ L^2(\P)}
    }
    {\varepsilon}
    \right)^4
    \,.
\end{gather*}
\end{proof}
\begin{lemma}
  \label{aa:mean:r3:lem:conv}
  Under conditions it holds
  $\sup_{z\in\R}\left| R_3(z) \right|\overset{\P}{\to}0$.
\end{lemma}
\begin{proof}
  Let $N\ge\underline{N}$,
  $z\in\R$,
  and 
  let
  $g^\dagger$ 
  denote the function \eqref{ghost_function} with 
  $
(\lambda^\dagger,\lambda_0^\dagger)
  $.
  If
  \begin{gather*}
    \left| 
    w^\dagger(X)- \frac{1}{\pi(X)} 
    \right|
    \ 
    \le
    \ 
    \varepsilon_N
  \end{gather*}
  it holds
  \begin{gather*}
    g^\dagger
    (X)
    \cdot
    f_z
    (T,X,Y(T))
    \ 
    =
    \ 
  \left( 
      w^\dagger(X)
      -
      \frac{1}{\pi(X)}
  \right)
      T
      \left( 
        \mathbf{1}
        _{\left\{  Y(T)\,\le\,z \right\}}
        -
        F_{Y(1)}(z|X)
      \right)
      \,.
  \end{gather*}
  By Lemma~\ref{aa:mean:r3:lem:fz_E}
  it holds 
  \begin{align*}
    &
  f_z(T,X,Y(T))
  \ 
  \in
  \ 
  L^1(\P)\,, 
  \\
  &
  f_z(T,X,Y(T))
  \ 
  \perp
  \ 
  D_N
  \,,
  \\
  \E
  [
  &
  f_z(T,X,Y(T))
    |X
  ]
  \ 
  =
  \ 
  0
  \,.
  \end{align*}
  Thus, 
  it follows from Lemma~\ref{w.Z=0}
  \begin{gather*}
    \E
    \left[
      w^\dagger(X)
      \cdot
      f_z(T,X,Y(T))
    \right]
    \ 
    =
    \ 
    0
    \,.
  \end{gather*}
  Since
  \begin{gather*}
    \E
    \left[
      \frac{T}{\pi(X)}
      f_z(T,X,Y(T))
    \right]
    \ 
    =
    \ 
    \E
    \left[
      \frac{T}{\pi(X)}
      \left( 
        \mathbf{1}
        _{\left\{  Y(T)\,\le\,z \right\}}
        -
        F_{Y(1)}(z|X)
      \right)
    \right]
    \  
    =
    \ 
    0
  \end{gather*}
  by Lemma~\ref{ps_weights_lemma},
  it follows
  \begin{align*}
    &
  \E
  \left[
    g^\dagger(X)
    f_z(T,X,Y(T))
  \right]
  \\
    &
  \ 
  =
  \ 
    \E
    \left[
      w^\dagger(X)
      \cdot
      f_z(T,X,Y(T))
    \right]
    \ 
    -
    \ 
    \E
    \left[
      \frac{T}{\pi(X)}
      f_z(T,X,Y(T))
    \right]
    \ 
    =
    \ 
  0
  \,.
  \end{align*}
  But then 
  \begin{align*}
    R_3(z)
    &
    \ 
    =
    \ 
  \frac{1}
  {
\sqrt{N}
  }
    \sum_{i=1}^{N} 
    \left[ 
    \left( 
    w(X_i) 
    -
    \frac{1}{\pi(X_i)}
    \right)
    T_i
    \left( 
    \mathbf{1}{\left\{ Y_i \le z \right\}}
    -
  F_{Y(1)}(z|X_i)
    \right)
    \right]
    \ 
    =
    \ 
    \G_N
    \left( 
      g^\dagger
      \cdot
      f_z
    \right)
    \,.
  \end{align*}
  It follows
\begin{align*}
    \P
    \left[ 
      \sup_{z\in\R}
     | 
    R_3(z)
    |
      \ge
      \varepsilon
    \right]
    &
    \ 
    \le
    \ 
    \P
    \left[ 
      \sup_{z\in\R}
     | 
    R_3(z)
    |
      \ge
      \varepsilon
      \ 
      \text{and}
      \ 
    | 
    w^\dagger(X)- 1/\pi(X)
    |
    \ 
    \le
    \ 
    \varepsilon_N
    \right]
    \\
    &
    \quad
    +
    \ 
    \P
    \left[ 
    | 
    w^\dagger(X)- 1/\pi(X)
    |
    \ 
    >
    \ 
    \varepsilon_N
    \right]
    \\
    &
    \ 
    \le
    \ 
    \P
    \left[ 
      \norm{\G_N}^*_{\mathcal{F}_N\cdot\mathcal{F}}
      \ge
      \varepsilon
    \right]
    \ 
    +
    \ 
    \ 
    \P
    \left[ 
    | 
    w^\dagger(X)- 1/\pi(X)
    |
    \ 
    >
    \ 
    \varepsilon_N
    \right]
    \\
    &
    \ 
    \to
    \ 
    0
    \,.
\end{align*}
The convergence of the first term follows from
Lemma~\ref{aa:r3:lemma:1}.
The convergence of the second term follows from
Theorem~\ref{aa:weights:th}.

\end{proof}


Until now, all parts of the error decomposition converge to 0.
The last term $R_4$ will decide the profile of the limiting process.
To this end we need the following concept.


\begin{lemma}
  \label{aa:mean:l:r4}
  Let
  $1/\pi(X)\in L^2(\P)$.
  $R_4$ converges
  converges in
  $l^\infty(\R)$
  to a Gaussian process with mean 0 and covariance
\begin{align*}
  &
  \mathbf{Cov}
  (z_1,z_2)
  \\
  &
  =\ 
  \E
  \left[ 
 \frac{
 F_{Y(1)}(z_1 \land z_2\,|\,X)
}{\pi(X)}
\ 
-
\ 
 \frac{1-\pi(X)}{\pi(X)}
 F_{Y(1)}(z_1|X)
 \cdot
 F_{Y(1)}(z_2|X)
  \right]
  \ 
 -
 \ 
 F_{Y(1)}(z_1)
 \cdot
 F_{Y(1)}(z_2)
\end{align*}

\end{lemma}
\begin{proof}
  By Lemma~\ref{aa:mean:r3:lem:fz_E} it follows
  \begin{align*}
    \E
    \left[
      \frac{f_z(T,X,Y(T))}{\pi(X)}
      +
      F_{Y(1)}(z|X)
      -
      F_{Y(1)}(z)
      \right]
      \ 
      =
      \ 
      \E
      \left[
      \frac{1}{\pi(X)}
      \E
      \left[
        f_z(T,X,Y(T))
        |X
      \right]
      \right]
      \ 
      =
      \ 
      0
      \,.
  \end{align*}
  Thus
  \begin{align*}
    R_4(z)
    &
  \
  =
  \ 
  \frac{1}{
  \sqrt{N}
  }
    \sum_{i=1}^{N} 
    \frac{T_i}{\pi(X_i)}
    \left( 
    \mathbf{1}{\left\{ Y_i \le z \right\}}
    -
  F_{Y(1)}(z|X_i)
    \right)
    \ 
    +
    \ 
    \left( 
  F_{Y(1)}(z|X_i)
    -
  F_{Y(1)}(z)
    \right)
    \\
    &
    \ 
  =
    \ 
  \frac{1}{
  \sqrt{N}
  }
    \sum_{i=1}^{N} 
      \frac{f_z(T_i,X_i,Y_i)}{\pi(X_i)}
      +
      \left( 
      F_{Y(1)}(z|X_i)
      -
      F_{Y(1)}(z)
      \right)
      \\
      &
      \ 
      =
      \ 
      \G_N 
      \left(
       \frac{f_z}{\pi(\cdot)}
      +
      F_{Y(1)}(z|\cdot)
      -
      F_{Y(1)}(z)
      \right)
      \,.
  \end{align*}
  By Lemma~\ref{aa:mean:l:br}
  it holds
  \begin{align*}
    &
  \log
  N_{[\,]}
    (
    \varepsilon
    ,
    \mathcal{G}, L^2(\P))
    \\
    &
    \ 
    \lesssim
    \ 
    \log
    \left(
      \frac
      {
      1+
    \norm{1/\pi(X)}_{ L^2(\P)}
      }
      {\varepsilon}
    \right)
    \ 
    \lesssim
    \ 
      \frac
      {
      1+
    \norm{1/\pi(X)}_{ L^2(\P)}
      }
      {\varepsilon}
    \qquad
    \text{for all}
    \ 
    \varepsilon\in (0,1)
    \,.
  \end{align*}
  Thus
  \begin{gather*}
    J_{[\,]}(1,\mathcal{G},L^2(\P))
    \ 
    \lesssim
    \ 
    \int_0^1
    \sqrt
    {
      \frac
      {
      1+
    \norm{1/\pi(X)}_{ L^2(\P)}
      }
      {\varepsilon}
    }
    \,
    d\varepsilon
    \ 
    \lesssim
    \ 
      1+
    \norm{1/\pi(X)}_{ L^2(\P)}
    \ 
    <
    \ 
    \infty
    \,.
  \end{gather*}
But then $\mathcal{G}$ is $\P$-Donsker.
By the Donsker Theorem \cite[Theorem~19.5]{Vaart2000}
the process $R_4$ converges in $l^\infty(\R)$ to a Gaussian process, called $\P$-Brownian bridge, with mean 0.
We now calculate the covariance of the limiting process.
\subsubsection*{Covariance}
\begin{align*}
  &
  \E
  \left[
  \left( 
  f^{z_1}_{1/\pi}
  +
  F_{Y(1)}(z_1|X)
  -
F_{Y(1)}(z_1)
  \right)
  \left( 
  f^{z_2}_{1/\pi}
  +
  F_{Y(1)}(z_2|X)
  -
F_{Y(1)}(z_2)
  \right)
  \right]
  \\
  &
  \ 
  =
  \ 
\E
\left[
  f^{z_1}_{1/\pi}
  \cdot
  f^{z_2}_{1/\pi}
\right]
\\
  &
  \quad
  +
  \ 
  \E
  \left[
  f^{z_1}_{1/\pi}
  \left( 
  F_{Y(1)}(z_2|X)
  -
F_{Y(1)}(z_2)
  \right)
  \right]
  \ 
  +
  \ 
  \E
  \left[
  f^{z_2}_{1/\pi}
  \left( 
  F_{Y(1)}(z_1|X)
  -
F_{Y(1)}(z_1)
  \right)
  \right]
  \\
  &
  \quad
  +
  \ 
  \E
  \left[
  \left( 
  F_{Y(1)}(z_1|X)
  -
F_{Y(1)}(z_1)
  \right)
  \left( 
  F_{Y(1)}(z_2|X)
  -
F_{Y(1)}(z_2)
  \right)
  \right]
  \\
  &
  \ 
  =:
  \ 
  C_0
  \quad 
  +
  \quad 
  C_1
  +
  C_2
  \quad 
  +
  \quad 
  C_3
  \,.
\end{align*}
It holds
\begin{align*}
  C_0 
  &
  \ 
  =
  \ 
\E
\left[
  f^{z_1}_{1/\pi}
  \cdot
  f^{z_2}_{1/\pi}
\right]
\\
&
\ 
=
\ 
\E
\left[
\frac{1}{\pi(X)}
\frac{T}{\pi(X)}
\left( 
\mathbf{1}{\left\{ Y(T)\,\le\, z_1 \right\}}
-
F_{Y(1)}(z_1|X)
\right)
\left( 
\mathbf{1}{\left\{ Y(T)\,\le\, z_2 \right\}}
-
F_{Y(1)}(z_2|X)
\right)
\right]
\\
&
\ 
=
\ 
\E
\left[
\frac{1}{\pi(X)}
\left( 
\mathbf{1}{\left\{ Y(1)\,\le\, z_1 \right\}}
-
F_{Y(1)}(z_1|X)
\right)
\left( 
\mathbf{1}{\left\{ Y(1)\,\le\, z_2 \right\}}
-
F_{Y(1)}(z_2|X)
\right)
\right]
\\
&
\ 
=
\ 
\E
\left[
\frac{1}{\pi(X)}
\left( 
F_{Y(1)}(z_1\land z_2|X)
\ 
-
\ 
F_{Y(1)}(z_1|X)
\cdot
F_{Y(1)}(z_2|X)
\right)
\right]
\,.
\end{align*}
\begin{align*}
  C_1
  &
  \ 
  =
  \ 
 \E
  \left[
  f^{z_1}_{1/\pi}
  \left( 
  F_{Y(1)}(z_2|X)
  -
F_{Y(1)}(z_2)
  \right)
  \right]
  \\
  &
  \ 
  =
  \ 
 \E
  \left[
\frac{T}{\pi(X)}
\left( 
\mathbf{1}{\left\{ Y(T)\,\le\, z_1 \right\}}
-
F_{Y(1)}(z_1|X)
\right)
  \left( 
  F_{Y(1)}(z_2|X)
  -
F_{Y(1)}(z_2)
  \right)
  \right]
  \\
  &
  \ 
  =
  \ 
 \E
  \left[
\left( 
\mathbf{1}{\left\{ Y(1)\,\le\, z_1 \right\}}
-
F_{Y(1)}(z_1|X)
\right)
  \left( 
  F_{Y(1)}(z_2|X)
  -
F_{Y(1)}(z_2)
  \right)
  \right]
  \\
  &
  \ 
  =
  \ 
  0
  \,.
\end{align*}
In the same way we see $C_2=0$.
\begin{align*}
  C_3
  &
  \ 
  =
  \ 
  \E
  \left[
  \left( 
  F_{Y(1)}(z_1|X)
  -
F_{Y(1)}(z_1)
  \right)
  \left( 
  F_{Y(1)}(z_2|X)
  -
F_{Y(1)}(z_2)
  \right)
  \right]
  \\
  &
  \ 
  =
  \ 
  \E
  \left[
  F_{Y(1)}(z_1|X)
  \cdot
  F_{Y(1)}(z_2|X)
  \right]
  \ 
  -
  \ 
  F_{Y(1)}(z_1)
  \cdot
  F_{Y(1)}(z_2)
  \,.
\end{align*}
Adding up the results gives us \eqref{cov:lp}.
\end{proof}
We have gathered all the results to prove Theorem~\ref{aa:mean:th}.
\begin{proof}
  \emph{(Theorem~\ref{aa:mean:th})}
  We connect the statement of the theorem to the error decomposition by Lemma~\ref{aa:mean:lemma_decomp}.
  By Lemma~\ref{aa:mean:l:r1}, Lemma~\ref{aa:mean:l:r2},
  Lemma~\ref{aa:mean:r3:lem:conv}
   it follows 
   $\sup_{z\in\R}|R_i(z)|\overset{\P}{\to}0$ for $i=1,2,3$.
   Thus, by Slutzky's theorem (cf.\cite[Theorem~13.18]{Klenke2020})
   the behaviour of the limiting process is the one of Lemma~\ref{aa:mean:l:r4}.
\end{proof}
