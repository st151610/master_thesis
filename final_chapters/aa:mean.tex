We adapt the error decomposition in \cite[page 27]{Wang2019} 
to estimates of the distribution function $F_{Y(1)}$ of $Y(1)$, 
that is,
\begin{gather}
  F_{Y(1)}
  \ 
  \colon
  \ 
  \R
  \ 
  \to
  \ 
  [0,1]
  \, 
  , 
  \qquad
  z
  \ 
  \mapsto
  \ 
  \P
  [
  Y(1)
  \le
  z
  ]
  \,.
\end{gather}
\begin{assumption}
  \label{F_N_br_n}
  For any decreasing sequence
  $(\varepsilon_N)$ with $\varepsilon_N\to 0$ for $N\to\infty$,
there exists a sequence of (measurable) function classes
$(\mathcal{F}_N)$
with envelope functions
$(F_N)$,
satisfying 
for some $k<2$
\begin{gather*}
\norm{F_N}_{L^2(\P)}
\ 
\le
\ 
\varepsilon_N
\quad
\text{and}
\quad
  \log
  N_{[\,]}(\varepsilon,\mathcal{F}_N,\mathrm{L}_2(\P))
  \ 
  \lesssim
  \ 
  \left( 
  \frac{1}{\varepsilon}
  \right)^k
  \quad
  \text{for all}
  \ 
  N\in\mathbb{N}
  \,,
\end{gather*}
such that for all $(\lambda,\lambda_0)\in\R^{N+1}$ and for all $N\in\mathbb{N}$ the function
\begin{gather}
  \label{ghost_function}
  \mathcal{X}
  \ 
  \to
  \ 
  \R
  \,,
  \qquad
  x
  \ 
  \mapsto
  \ 
  \mathbf{1}_{
    \left\{ 
      \sup_{y\in A_N(x)}
      \left| 
      w(y,\lambda,\lambda_0)
      -
      \frac{1}{\pi(y)}
      \right|
      \,
      \le
      \,
      \varepsilon_N
    \right\}
  }
  \left( 
      w(x,\lambda,\lambda_0)
      -
      \frac{1}{\pi(x)}
  \right)
\end{gather}
is contained in $\mathcal{F}_N$.
\end{assumption}
\begin{remark}
  We can derive Assumption~\ref{F_N_br_n} from regularity assumptions on the inverse propensity score 
  $1/\pi(\cdot)$ and the distribution of $X$.
  To this end, we want to employ \cite[Corollary~2.7.4]{vaart2013}.
  To do this, the crucial observation is, that
  \begin{gather*}
    w(\cdot,\lambda,\lambda_0)
    \quad
    \text{is constant on each cell}\ 
    A_N\in\mathcal{P}_N
    \,.
  \end{gather*}
  Thus, the regularity of the function \eqref{ghost_function}
  on each cell $A_N\in\mathcal{P}_N$ is decided by 
  $1/\pi(\cdot)$.
  Thus, we assume that there exists
  $\alpha>d/2$, where $\mathcal{X}\subseteq \R^d$, such
  that for 
  $V:=d/\alpha$.
 and for all 
$
(j,N)\in\mathbb{N}^2
$
there exists 
$M_{N,j}\ge \varepsilon_N$ such that 
\begin{gather*}
  \frac{1}{\pi(\cdot)}
  \in C^\alpha_{M_{N,j}}(A_{N,j})
  \quad
  \text{and}
  \quad
  \sum_{j=1}^{\infty} 
  M_{N,j}^{2V/(V+2)}
  \P
  [
  X\in A_{N,j}
  ]^{V/(V+2)}
  \ 
  \lesssim
  \ 
  1
  \,.
\end{gather*}
Then the function \ref{ghost_function}, restricted to $A_{N,j}$, is in 
$
  C^\alpha_{M_{N,j}}(A_{N,j})
$
for all 
$
(j,N)\in\mathbb{N}^2
$.
Then \cite[Corollary~2.7.4]{vaart2013} gives us the statement 
in
Assumption~\ref{F_N_br_n}.
\end{remark}
\begin{lemma}
  Let
$(\varepsilon_N)\subset(0,1]$
be 
a decreasing sequence
with $\varepsilon_N\to 0$ for $N\to\infty$ 
and let Assumption~\ref{F_N_br_n} hold true
for a sequence of function classes $\mathcal{F}_N$.
Then
\begin{gather*}
  J_{[\,]}(
\norm{F_N}_{L^2(\P)}
,\mathcal{F}_N\cdot\mathcal{F},\mathrm{L}_2(\P))
  \to 0
  \quad
  \text{and}
  \quad
  \norm{\G_N}^*_{\mathcal{F}_N\cdot\mathcal{F}}\overset{\P}{\to}0
  \qquad
  \text{for}\ 
  N\to\infty
  \,.
\end{gather*}
\end{lemma}
\begin{proof}
  By Assumption~\ref{F_N_br_n}
  and Lemma~\ref{bracketing_number_lem} it holds
for some $k<2$
\begin{gather*}
\norm{F_N}_{L^2(\P)}
\ 
\le
\ 
\varepsilon_N
\quad
\text{and}
\quad
  \log
  N_{[\,]}(\varepsilon,\mathcal{F}_N,\mathrm{L}_2(\P))
  \ 
  \lesssim
  \ 
  \left( 
  \frac{1}{\varepsilon}
  \right)^k
  \quad
  \text{for all}
  \ 
  N\in\mathbb{N}
  \,,
\end{gather*}
and
  \begin{gather*}
    N_{[\,]}
    (
    \varepsilon
    ,
    \mathcal{F}, L^2(\P))
    \ 
    \lesssim
    \ 
    \left( 
      \frac{1}{\varepsilon}
    \right)^2
    \qquad
    \text{for all}
    \ 
    \varepsilon>0
    \,.
  \end{gather*}
  Since $\mathcal{F}_N$ and $\mathcal{F}$ have envelope function smaller 1, we can apply Lemma~\ref{lem_prod_br} to get
  \begin{gather*}
  \log
  N_{[\,]}(\varepsilon,\mathcal{F}_N\cdot\mathcal{F},\mathrm{L}_2(\P))
  \ 
  \lesssim
  \ 
  \left( 
  \frac{1}{\varepsilon}
  \right)^k
  +
  \log
  (1/\varepsilon)
  \ 
  \lesssim
  \ 
  \left( 
  \frac{1}{\varepsilon}
  \right)^k
  \quad
  \text{for all}\ 
  \varepsilon>0
  \,.
  \end{gather*}
  Since 
  $k/2\in(0,1)$
  it holds
\begin{align*}
  J_{[\,]}(
\norm{F_N}_{L^2(\P)}
,\mathcal{F}_N\cdot\mathcal{F},\mathrm{L}_2(\P))
  &
  \ 
=
  \ 
\int_0^{
\norm{F_N}_{L^2(\P)}
}
\sqrt{
  \log
  N_{[\,]}(\varepsilon,\mathcal{F}_N\cdot\mathcal{F},\mathrm{L}_2(\P))
}
\,d\varepsilon
\\
&
\ 
\lesssim
\ 
\int_0^{
  \varepsilon_N
}
  \left( 
  \frac{1}{\varepsilon}
\right)^{k/2}
\,d\varepsilon
\\
&
\ 
=
\ 
\frac{
\varepsilon_N^{1-k/2}
}{1-k/2}
\ 
\to 0
\ 
\qquad
\text{for}
\ 
N\to\infty
\,.
\end{align*}
The second statement follows from Lemma~\ref{markov_max_lemma}
since $F_N$ is also an envelope function of $\mathcal{F}_N\cdot\mathcal{F}$.
\end{proof}

A technical lemma for products of function classes.

Define the product of two function classes
\begin{gather*}
  \mathcal{F}\cdot \mathcal{G}
  :=
  \left\{ 
    f\cdot g
    \colon
    f\in\mathcal{F},
    g\in\mathcal{G}
  \right\}\,.
\end{gather*}
\begin{lemma}
  \label{lem_prod_br}
  Let
  $\mathcal{F}$ and $\mathcal{G}$ be two function classes 
  with envelope functions $F$ and $G$ satisfying
  $\norm{F}_\infty,\norm{G}_\infty\le 1$.
  For all $\varepsilon>0$ and all $r\in [1,\infty)$ it holds
  \begin{gather*}
    N_{[\,]}(2\varepsilon,\mathcal{F}\cdot\mathcal{G},\mathrm{L}_r(\P))
    \
    \le
    \ 
    N_{[\,]}(\varepsilon,\mathcal{F},\mathrm{L}_r(\P))
    \cdot
    N_{[\,]}(\varepsilon,\mathcal{G},\mathrm{L}_r(\P))
    \,.
  \end{gather*}
\end{lemma}
\begin{proof}
  Let $f\in\mathcal{F}$ and $g\in\mathcal{G}$.
  We can choose two 
  $(\varepsilon,L^r(\P))$
  brackets
  $[\underline{f},\overline{f}]$
  and
  $[\underline{g},\overline{g}]$
  containing $f$ and $g$ with 
  $\norm{\underline{f}}_\infty,\norm{\overline{f}}_\infty\le\norm{F}_\infty\le 1$
  and
  $\norm{\underline{g}}_\infty,\norm{\overline{g}}_\infty\le\norm{G}_\infty\le 1$.
  We the get an 
  $(2\varepsilon,L^r(\P))$
  $[\underline{h},\overline{h}]$
  bracket, containing $f\cdot g$, by
\end{proof}
Next, we define some auxiliary functions.
  For $z\in\R$ we define the function
  \begin{align*}
    f_z
    \ 
    :
    \ 
      \left\{ 0,1 \right\}
      \times
      \mathcal{X}
      \times
      \mathcal{Y}
    &
    \ 
    \to
    \ 
    \R
    \\
      (T,X,Y(T))
    &
      \ 
      \mapsto
      \ 
      T
      \left( 
        \mathbf{1}
        _{\left\{  Y(T)\,\le\,z \right\}}
        -
        F_{Y(1)}(z|X)
      \right)
      \,,
  \end{align*}
  and the function classes
  \begin{gather}
    \label{F_g}
    \begin{split}
    \mathcal{F}
    &
    \ 
    :=
    \ 
    \left\{ 
      f_z
      \ 
      |
      \ 
      z\in\R\ 
    \right\}
    \\
    \mathcal{G}
    &
    \ 
    :=
    \ 
    \left\{ 
      \frac{f_z}{\pi(\cdot)}
      +
      F_{Y(1)}(z|\cdot)
      -
      F_{Y(1)}(z)
      \ 
      \colon
      \ 
      z\in\R\ 
    \right\}
    \,.
    \end{split}
  \end{gather}
  \begin{lemma}
    Assume that treatment assignment is strongly ignorable.
    It holds
    for all $z\in\R$
    \begin{gather*}
      \E
      \left[
        f_z
        \left( 
          T,
          X,
          Y(T)
        \right)
        \,
        |
        \,
        X
      \right]
      \ 
      =
      \ 
      0
      \qquad
      \text{almost surely.}
    \end{gather*}
  \end{lemma}
  \begin{proof}
    It holds
    \begin{align*}
      \E
      \left[
        f_z
        \left( 
          T,
          X,
          Y(T)
        \right)
        \,
        |
        \,
        X
      \right]
      &
      \ 
      =
      \ 
      \E
      \left[
      T
      \left( 
        \mathbf{1}
        _{\left\{  Y(T)\,\le\,z \right\}}
        -
        F_{Y(1)}(z|X)
      \right)
        \,
        |
        \,
        X
      \right]
      \\
      &
      \ 
      =
      \ 
      \E
      \left[
        \mathbf{1}
        _{\left\{  Y(1)\,\le\,z \right\}}
        -
        F_{Y(1)}(z|X)
        \,
        |
        \,
        X
        ,
        T=1
      \right]
      \pi(X)
      \\
      &
      \ 
      =
      \ 
      \left( 
      \E
      \left[
        \mathbf{1}
        _{\left\{  Y(1)\,\le\,z \right\}}
        \,
        |
        \,
        X
      \right]
      \ 
        -
      \ 
        F_{Y(1)}(z|X)
      \right)
      \pi(X)
      \\
      &
      \ 
      =
      \ 
      0
      \qquad
    \end{align*}
    almost surely.
    We use the assumption to get the third equality.
  \end{proof}
The next lemma provides bracketing numbers for specific function classes needed to control $R_3$ and $R_4$.
\newpage
\begin{lemma}
  \label{bracketing_number_lem}
  The function class $\mathcal{F}$ and $\mathcal{G}$ defined in \eqref{F_g} are measurable.
  Furthermore, 
  \begin{gather*}
    N_{[\,]}
    (
    \varepsilon
    ,
    \mathcal{F}, L^2(\P))
    \ 
    \lesssim
    \ 
    \left( 
      \frac{1}{\varepsilon}
    \right)^2
    \qquad
    \text{for all}
    \ 
    \varepsilon>0
    \,.
  \end{gather*}
  If $1/\pi(X)\in L^2(\P)$, it also holds 
  \begin{gather*}
    N_{[\,]}
    (
    \varepsilon
    ,
    \mathcal{G}, L^2(\P))
    \ 
    \lesssim
    \ 
    \left( 
    \frac{
      1+
    \norm{1/\pi(X)}_{ L^2(\P)}
    }
    {\varepsilon}
    \right)^4
    \qquad
    \text{for all}
    \ 
    \varepsilon>0
    \,.
  \end{gather*}
%  Furthermore, 
%  consider the function class
%  \begin{gather*}
%    \mathcal{G}
%    :=
%    \left\{ 
%      f_{1/\pi}^z
%      +
%        F_{Y(1)}(z|\cdot)
%        -
%        F_{Y(1)}(z)
%      \ 
%      |
%      \ 
%      z\in\R
%    \right\}
%    \,.
%  \end{gather*}
%  If $1/\pi(X)\in  L^2(\P)$
%  it holds
%  \begin{gather}
%    N_{[\,]}(\varepsilon,\mathcal{G}, L^2(\P))
%    \le
%    ??
%    \qquad
%    \text{for all}
%    \ 
%    \varepsilon>0
%    \,.
%  \end{gather}
\end{lemma}
\begin{proof}
  As in \cite[Example~19.6]{Vaart2000}
  we choose for
  $\varepsilon>0$ and $m\in\mathbb{N}$
  \begin{gather*}
  -\infty=z_0\ <\ z_1\ <\ \cdots\ <\ z_{m-1}\ <\ z_m=\infty
  \,
  \end{gather*}
  such that
  \begin{gather}
    \label{size_z}
    \P
    \left[ 
      Y(1)\in \left[ z_{l-1},z_l \right]\,
    \right]
    \ 
    \le
    \ 
    \varepsilon
    \qquad
    \text{for all}\ 
    l\in \left\{ 1,\ldots,m \right\}
  \end{gather}
  and $m \le 2/\varepsilon$.
  Next, we define $m$ brackets by
\begin{align*}
  \overline{f^l}
  (T,X,Y(T))
  &
  \ 
  :=
  \ 
      T
      \left( 
        \mathbf{1}
        _{\left\{  Y(T)\,\le\,z_{l} \right\}}
        -
        F_{Y(1)}(z_{l-1}|X)
      \right)
      \,,
      \\
  \underline{f^l}
  (T,X,Y(T))
  &
  \ 
  :=
  \ 
      T
      \left( 
        \mathbf{1}
        _{\left\{  Y(T)\,\le\,z_{l-1} \right\}}
        -
        F_{Y(1)}(z_l|X)
      \right)
      \,,
\end{align*}
for $l\in \left\{ 1,\ldots,m \right\}$.
These brackets cover $\mathcal{F}$.
Indeed,
\begin{gather*}
  \text{for all}\ 
  z\in\R
  \ 
  \text{there exists} \ 
l\in \left\{ 1,\ldots,m \right\}
\qquad 
\text{such that}\qquad
z_{l-1}
\ 
\le
\ 
z
\ 
\le
\ 
z_l
\,.
\end{gather*}
By the monotonicity of 
$
        \mathbf{1}
      _{\left\{  Y(T)\,\le\,(\cdot) \right\}}
$
and
$
        F_{Y(1)}(\cdot|X)
$
and the non-negativity of $T$ it follows
\begin{gather*}
  \text{for all}\ 
  z\in\R
  \ 
  \text{there exists} \ 
l\in \left\{ 1,\ldots,m \right\}
\qquad 
\text{such that}\qquad
  \underline{f^l}
  \ 
  \le
  \ 
  f_z
  \ 
  \le
  \ 
  \overline{f^l}
  \,.
\end{gather*}
Thus, the $m$ brackets 
$
[
  \underline{f^l}
  ,
  \overline{f^l}
]
$
cover $\mathcal{F}$.

Let's calculate the size of the brackets.
It holds
\begin{align*}
  &
\E
\left[ 
      T
      \cdot
      \left( 
        \mathbf{1}
        _{\left\{  Y(T)\,\le\,z_{l} \right\}}
        -
        F_{Y(1)}(z_{l-1}|X)
        \ 
        -
        \ 
        \mathbf{1}
        _{\left\{  Y(T)\,\le\,z_{l-1} \right\}}
        +
        F_{Y(1)}(z_{l}|X)
      \right)
      \,
\right]
\\
  &
  \ 
=
  \ 
\E
\left[ 
      T
      \cdot
      \left( 
        \mathbf{1}
        _{\left\{
        Y(T)
        \,
        \in 
        \,
    [z_{l-1},z_l]
\right\}}
\ 
        +
\ 
        \P
        \left[ 
          Y(1)
          \in
    [z_{l-1},z_l]
        \,
    |
        \,
    X
        \right]
      \right)
      \,
\right]
\\
  &
  \ 
\le
  \ 
\E
\left[ 
  \,
  \pi(X)
  \cdot
        \P
        \left[ 
          Y(1)
          \in
    [z_{l-1},z_l]
          \,
    |
          \,
    X
        \right]
          \ 
\right]
\ 
+
\ 
\varepsilon
\\
  &
  \ 
\le
  \ 
2
\,
\varepsilon
\,.
\end{align*}
We used \eqref{size_z}, $0\le T,\pi(X)\le 1$ and the properties of conditional expectation.
It follows
\begin{align*}
  \norm{
  \overline{f^l}
-
  \underline{f^l}
}_
{ L^2(\P)}
\ 
\lesssim
\ 
\E
\left[ 
  \,
      T
      \cdot
      \left( 
        \mathbf{1}
        _{\left\{
        Y(T)
        \,
        \in 
        \,
    [z_{l-1},z_l]
\right\}}
\ 
        +
\ 
        \P
        \left[ 
          Y(1)
          \in
    [z_{l-1},z_l]
        \,
    |
        \,
    X
        \right]
      \right)
      \,
   \right]^{1/2}
\ 
\lesssim
\ 
\varepsilon^{1/2}
\,.
\end{align*}
Since $m\le 2/\varepsilon$ it holds
  \begin{align*}
    N_{[\,]}
    \left(
\varepsilon^{1/2}
    ,
    \,
    \mathcal{F}\,,\, L^2(\P)
    \right)
    &
    \ 
    \lesssim
    \ 
    \frac{1}{\varepsilon}
    \intertext{and thus}
    N_{[\,]}
    (
    \varepsilon
    ,
    \mathcal{F}, L^2(\P))
    &
    \ 
    \lesssim
    \ 
    \left( 
      \frac{1}{\varepsilon}
    \right)^2
    \,.
  \end{align*}
  Next, we look at $\mathcal{G}$. To this end, we define 
  $m$ brackets by
 \begin{align*}
    \overline{g^l}(T,X,Y(T))
    \ 
    :=
    \ 
    \frac{T}{\pi(X)}
    \left( 
      \mathbf{1}_{\left\{  Y(T)\,\le\,z_{l} \right\}}
      -
      F_{Y(1)}(z_{l-1}|X)
    \right)
    \ 
    +
    \ 
    F_{Y(1)}(z_{l}|X)
-
F_{Y(1)}(z_{l-1})
\,,
\\
    \underline{g^l}(T,X,Y(T))
    \ 
    :=
    \ 
    \frac{T}{\pi(X)}
    \left( 
      \mathbf{1}_{\left\{  Y(T)\,\le\,z_{l-1} \right\}}
      -
      F_{Y(1)}(z_l|X)
    \right)
    \ 
    +
    \ 
    F_{Y(1)}(z_{l-1}|X)
-
      F_{Y(1)}(z_l)
\,,
  \end{align*}
  for $l\in \left\{ 1,\ldots,m \right\}$.
  With the same arguments as before, we see that these brackets cover $\mathcal{G}$.
  Let's calculate the size.
  It holds
  \begin{align*}
    &
    \norm{
      \frac{T}{\pi(X)}
      \left( 
      \mathbf{1}_{
      \left\{ 
      Y(T)\in [z_{l-1},z_l] 
    \right\}
    }
      +
      \P
      \left[ 
      Y(1)\in [z_{l-1},z_l] 
      \,
      |
      \,
      X
      \right]
      \right)
    }_{ L^2(\P)}
    \\
    &
    \
    \lesssim
    \
    \left( 
      \E
      \left[ 
        \frac{1}{\pi(X)}
        \frac{T}{\pi(X)}
      \left( 
      \mathbf{1}_{
      \left\{ 
      Y(T)\in [z_{l-1},z_l] 
    \right\}
    }
      +
      \P
      \left[ 
      Y(1)\in [z_{l-1},z_l] 
      \,
      |
      \,
      X
      \right]
      \right)
      \right]
    \right)
    ^{1/2}
    \\
    &
    \
    \lesssim
    \
    \left( 
      \E
      \left[ 
        \frac{1}{\pi(X)}
      \P
      \left[ 
      Y(1)\in [z_{l-1},z_l] 
      \,
      |
      \,
      X
      \right]
      \right]
    \right)
    ^{1/2}
    \\
    &
    \ 
    \lesssim
    \ 
    \left( 
      \norm{1/\pi(X)}_{ L^2(\P)}
      \sqrt{\varepsilon}
    \right)
    ^{1/2}
    \ 
    =
    \ 
    \varepsilon^{1/4}
    \norm{1/\pi(X)}_{ L^2(\P)}^{1/2}
  \end{align*}
  and
  \begin{align*}
     \norm{
      \P
      \left[ 
      Y(1)\in [z_{l-1},z_l] 
      \,
      |
      \,
      X
      \right]
      \ 
     + 
      \ 
      \P
      \left[ 
      Y(1)\in [z_{l-1},z_l] \,
      \right]
    }_{ L^2(\P)}
    \ 
    \lesssim
    \ 
    \varepsilon^{1/2}
    \,.
  \end{align*}
  Thus
  \begin{gather*}
  \norm{
  \overline{g^l}
-
  \underline{g^l}
}_
{ L^2(\P)}
\ 
\lesssim
\ 
\varepsilon^{1/4}
\left( 
  1
  +
    \norm{1/\pi(X)}_{ L^2(\P)}^{1/2}
\right)
\ 
\lesssim
\ 
\varepsilon^{1/4}
\left( 
  1
  +
    \norm{1/\pi(X)}_{ L^2(\P)}
\right)
\,.
  \end{gather*}
As before, it follows
\begin{gather*}
    N_{[\,]}
    (
    \varepsilon
    ,
    \mathcal{G}, L^2(\P))
    \ 
    \lesssim
    \ 
    \left( 
    \frac{
      1+
    \norm{1/\pi(X)}_{ L^2(\P)}
    }
    {\varepsilon}
    \right)^4
    \,.
\end{gather*}
\end{proof}

\begin{lemma}
  \label{markov_max_lemma}
  Let $(\mathcal{F}_N)$ be a sequence of measurable function classes with envelope functions $(F_N)$.
  If
  \begin{gather}
    J_{[\, ]}
    \left( 
    \norm{F_N}_{ L^2(\P)}
    ,
    \mathcal{F}_N
    ,
     L^2(\P)
    \right)
    \ 
    \to
    \ 
    0
    \qquad
    \text{for}
    \ 
    N
    \to
    \infty
  \end{gather}
  it holds 
  $
  \norm{\G_N}^*_{\mathcal{F}_N}\overset{\P}{\to}0
  $.
\end{lemma}
\begin{proof}
  By Markov's inequality and Theorem~\ref{th:max_ineq} it holds for all $\varepsilon>0$
  \begin{align*}
    &
    \P
    [
  \norm{\G_N}^*_{\mathcal{F}_N}
  \ge
  \varepsilon
    ]
    \\
    &
    \ 
    \le
    \ 
    \varepsilon^{-1}
    \E
    [
  \norm{\G_N}^*_{\mathcal{F}_N}
    ]
    \ 
    =
    \ 
    \varepsilon^{-1}
    \E^*
    [
  \norm{\G_N}_{\mathcal{F}_N}
    ]
    \ 
    \lesssim
    \ 
    \varepsilon^{-1}
    J_{[\, ]}
    \left( 
    \norm{F_N}_{ L^2(\P)}
    ,
    \mathcal{F}_N
    ,
     L^2(\P)
    \right)
    \ 
    \to
    \ 
    0
  \end{align*}
  for $N\to\infty$. 
\end{proof}
L
\begin{ftheorem}
  \label{aa:mean:th}
  Under conditions 
the stochastic process
\begin{gather}
    \sqrt{N}
    \left( 
  \frac{1}{N}
    \sum_{i=1}^{n} 
    w_i^\dagger
    \mathbf{1}
    _{\left\{ Y_i\,\le\, z \right\}}
    \ 
    -
    \ 
    F_{Y(1)}(z)
    \right)
    _{z\in\R}
    \,
  \end{gather}
  converges in
  $l^\infty(\R)$
  to a Gaussian process with mean 0 and covariance
\begin{align}
  \label{cov:lp}
  &
  \mathbf{Cov}
  (z_1,z_2)
  \\
  &
  =\ 
  \E
  \left[ 
 \frac{
 F_{Y(1)}(z_1 \land z_2\,|\,X)
}{\pi(X)}
\ 
-
\ 
 \frac{1-\pi(X)}{\pi(X)}
 F_{Y(1)}(z_1|X)
 \cdot
 F_{Y(1)}(z_2|X)
  \right]
  \ 
 -
 \ 
 F_{Y(1)}(z_1)
 \cdot
 F_{Y(1)}(z_2)
\end{align}
\end{ftheorem}

\begin{lemma}
  \label{aa:mean:lemma_decomp}
  It holds
  \begin{gather}
  \sqrt{N}
\left( 
    \frac{1}{N}
    \sum_{i=1}^{n} 
    w(X_i)
    \mathbf{1}_{\left\{ Y_i \le z \right\}}
    \ 
    -
    \ 
    F_{Y(1)}(z)
    \right)
    _{z\in\R}
    \ 
    =
    \ 
    R_1
    \ 
    +
    \ 
    R_2
    \ 
    +
    \ 
    R_3
    \ 
    +
    \ 
    R_4
  \end{gather}
  with
\begin{align*}
  R_1
  &
  \ 
  :=
  \ 
  \sqrt{N}
  \sum_{k=1}^{N} 
  \left[ 
  \frac{1}{N}
  \left( 
    \sum_{i=1}^{n} 
    w(X_i)
    B_k(X_i)
    -
    \sum_{i=1}^{N} 
    B_k(X_i)
  \right)
  F_{Y(1)}(z|X_k)
  \right]
  _{z\in\R}
  \,,
  %%%% 1 %%%%
  \\
  R_2
  &
  \
  :=
  \ 
  \sqrt{N}
    \sum_{i=1}^{N} 
    \left[ 
  \frac{
    T_i\cdot w(X_i) -1 }{N}
    \left( 
  F_{Y(1)}(z|X_i)
    \ 
    -
    \ 
    \sum_{k=1}^{N} 
    B_k(X_i)
    \cdot
  F_{Y(1)}(z|X_k)
    \right)
    \right]
  _{z\in\R}
  \,,
  %%%% 2 %%%%
  \\
  R_3
  &
  \
  :=
  \ 
  \sqrt{N}
  \left( 
  \frac{1}{N}
    \sum_{i=1}^{N} 
    \left[ 
    T_i
    \left( 
    w(X_i) 
    -
    \frac{1}{\pi(X_i)}
    \right)
    \left( 
    \mathbf{1}_{\left\{ Y_i \le z \right\}}
    -
  F_{Y(1)}(z|X_i)
    \right)
    \right]
  \right)
  _{z\in\R}
  \,,
  %%%% 3 %%%%
  \\
  R_4
  &
  \
  :=
  \ 
  \sqrt{N}
  \left( 
  \frac{1}{N}
    \sum_{i=1}^{N} 
    \frac{T_i}{\pi(X_i)}
    \left( 
    \mathbf{1}_{\left\{ Y_i \le z \right\}}
    -
  F_{Y(1)}(z|X_i)
    \right)
    \ 
    +
    \ 
    \left( 
  F_{Y(1)}(z|X_i)
    -
  F_{Y(1)}(z)
    \right)
  \right)
  _{z\in\R}
  \,.
  \end{align*}

\end{lemma}
\begin{proof}
  An elementary but long calculation yields the result. We omit the details.
\end{proof}

Let $F_{Y(1)}(z|x):=\P[Y(1)\le z|X=x]$ denote a conditional version of the distribution function of $Y(1)$ at $x\in\mathcal{X}$.
We also need the propensity score
$
  \pi(x):=\P[T=1|X=x]
$
and the weights function
$
  w(x)
  :=
  (
  f^{'}
  )^{-1}
  \left( 
    \inner{B(x)}{\lambda^\dagger}
    +
    \lambda_0^\dagger
  \right)
$.
\begin{lemma}
  \label{aa:mean:l:r1}
Let the weights function $w$ satisfy the box constraints in 
Problem~\ref{bw:1:primal} and 
$\sqrt{N}\norm{\delta}_1\overset{\P}{\to}0$.
Then it holds
$\sup_{z\in\R}|R_1(z)|\overset{\P}{\to}0$.
  \end{lemma}
\begin{proof}
%  By Theorem~\ref{dual_solution_th}
%  it holds $w_i^\dagger=w(X_i)$ for $i\in \left\{ 1,\ldots,n \right\}$, that is, for $i\le n$ we can identify $w(X_i)$ with the optimal
%  solution to 
%  problem~\ref{bw:1:primal}. 
%  Thus the constraints of the problem apply.
%   Let's bound $R_1$.
It holds
  \begin{align}
    \label{R_1:1}
    \begin{split}
    \sup_{z\in\R}
    \left| 
    R_1(z)
    \right|
    &
    \ 
    =
    \ 
    \sup_{z\in\R}
    \left| 
  \sqrt{N}
  \sum_{k=1}^{N} 
  \left[ 
  \frac{1}{N}
  \left( 
    \sum_{i=1}^{n} 
    w(X_i)
    B_k(X_i)
    -
    \sum_{i=1}^{N} 
    B_k(X_i)
  \right)
  F_{Y(1)}(z|X_k)
  \right]
    \right|
    \\
    &
    \ 
    \le
    \ 
  \sqrt{N}
  \sum_{k=1}^{N} 
  \left| 
  \frac{1}{N}
  \left( 
    \sum_{i=1}^{n} 
    w(X_i)
    B_k(X_i)
    -
    \sum_{i=1}^{N} 
    B_k(X_i)
  \right)
  \right|
    \sup_{z\in\R}
  F_{Y(1)}(z|X_k)
  \\
    &
    \ 
    \le
    \ 
  \sqrt{N}
  \norm{\delta}_1
    \end{split}
  \end{align}
  The last inequality is due to $F_{Y(1)}\in[0,1]$ and the assumption that $(w(X_i))$ satisfies the box constraints of Problem~\ref{bw:1:primal}.
  Since we assume 
$\sqrt{N}\norm{\delta}_1\overset{\P}{\to}0$
it holds
$\sup_{z\in\R}|R_1(z)|\overset{\P}{\to}0$.
\end{proof}
\begin{remark}
  We want to comment on the box constraints of Problem~\ref{bw:1:primal}, that is,
 \begin{gather*}
      \left| 
      \frac{1}{N} 
      \left( 
      \sum_{i = 1}^{n} 
      w(X_i)
      B_k(X_i)
      -
      \sum_{i=1}^{N} 
      B_k(X_i)
      \right)
    \right|
    \ 
    \le 
    \ 
    \delta_k
    \qquad
    \text{for all}\ 
    k \in \left\{ 1, \ldots, N \right\}
    \,.
  \end{gather*}
  Note, that the first sum goes over $\left\{ 1,\ldots,n \right\}$ while the second sum goes over $\left\{ 1,\ldots,N \right\}$.
  A second, equivalent version of the constraints is
  \begin{gather*}
      \left| 
      \frac{1}{N} 
      \left( 
      \sum_{i = 1}^{N} 
      T_i
      w(X_i)
      B_k(X_i)
      -
      \sum_{i=1}^{N} 
      B_k(X_i)
      \right)
    \right|
    \ 
    \le 
    \ 
    \delta_k
    \qquad
    \text{for all}\ 
    k \in \left\{ 1, \ldots, N \right\}
    \,.
  \end{gather*}
  Now both sums go over $\left\{ 1,\ldots,N \right\}$ and the
  indicator of treatment $T_i$ takes care that in the first sum only the terms with $i\le n$ are effective. 
  Having this flexibility with the versions helps. I regard the first version as suitable for non-probabilistic computations, although $n$ is of course a random variable. On the other hand, the second version is more honest, exactly telling the dependence on the indicator of treatment. This version is useful in probabilistic computations. 

  Also we want to comment on the assumption on $\norm{\delta}$.
  Playing around with norm equivalences we discover that 
  $\sqrt{N}\norm{\delta}_1\overset{\P}{\to}0$ for $N\to \infty$ is the weakest
  (natural) assumption to
  control $R_1$.
  Indeed, other ways to continue the second row in \eqref{R_1:1} are
  \begin{gather*}
    (\,\cdots)
    \ 
  \le
    \ 
  \sqrt{N}
  \norm{\delta}_2
  \left( 
  \sum_{k=1}^{N} 
  \left( 
    \sup_{z\in\R}
  F_{Y(1)}(z|X_k)
  \right)^2
\right)^{1/2}
\ 
\le
\ 
N
  \norm{\delta}_2\,,
  \end{gather*}
  by the Cauchy-Schwarz inequality and
  $
  F_{Y(1)}\in [0,1]
  $,
or
\begin{gather*}
  (\,\cdots)
  \ 
  \le
  \ 
  \sqrt{N}
  \norm{\delta}_\infty
  \sum_{k=1}^{N} 
    \sup_{z\in\R}
  F_{Y(1)}(z|X_k)
  \ 
  \le
  \ 
  N^{3/2}
  \norm{\delta}_\infty
  \,.
\end{gather*}
Since $\delta\in \R^N$, however, it holds
\begin{gather*}
  \sqrt{N}\norm{\delta}_1
  \ 
  \le
  \ 
  N\norm{\delta}_2
  \ 
  \le
  \ 
  N^{3/2}\norm{\delta}_\infty
  \,.
\end{gather*}
With hindsight, the assumption 
$\sqrt{N}\norm{\delta}_1\overset{\P}{\to}0$ for $N\to \infty$ 
  also 
  suffices 
  to control the second (or first) occurrence of a term, that we control by assumptions on $\norm{\delta}$.
This is the \textbf{second term} of \eqref{c:1}, where we estimate
\begin{gather*}
  \inner{\delta}{\left| \Delta \right|}
  \ 
  =
  \ 
  \sum_{k=1}^{N} 
  \delta_k
  \left| \Delta_k \right|
  \ 
  \le
  \ 
  \norm{\delta}_1
  \norm{\Delta}_\infty
  \ 
  \le
  \ 
  \norm{\delta}_1
  \norm{\Delta}_2
  \ 
  \le
  \ 
  \norm{\delta}_1
  \varepsilon
  \ 
  \overset{\P}{\to}
  \ 
  0
  \quad
  \text{for}\ 
  N\to \infty
  \,.
\end{gather*}

\end{remark}
  \begin{lemma}
    \label{aa:mean:l:r2}
    Let the conditions of Theorem~\ref{aa:weights:th} hold true.
    Furthermore assume, that the width of the partitioning estimate $h_N$ and a conditional version of the distribution function of $Y(1)$ satisfy
\begin{gather*}
  \sqrt{N}
  \sup_{z\in\R}
  \omega
  \left( 
    F_{Y(1)}(z|\cdot)
    ,h_N
  \right)
  \to
  0
  \qquad
  \text{for}\ 
  N\to \infty
  \,,
\end{gather*}
where $\omega$ is the modulus of continuity.
Then it holds
$\sup_{z\in\R}|R_2(z)|\overset{\P}{\to}0$.
  \end{lemma}
\begin{proof}
\begin{align*}
&
  \sup_{z\in\R}
  \left| 
  R_2(z)
  \right|
  \\
  &
  \ 
  \le
  \ 
  \sqrt{N}
    \sum_{i=1}^{N} 
    \left[ 
  \frac{
    T_i\cdot w(X_i) -1 }{N}
  \sup_{z\in\R}
    \left| 
  F_{Y(1)}(z|X_i)
    \ 
    -
    \ 
    \sum_{k=1}^{N} 
    B_k(X_i)
    \cdot
  F_{Y(1)}(z|X_k)
    \right|
    \right]
    \\
    &
  \ 
    \le
  \ 
    \sqrt{N}
  \sup_{z\in\R}
  \omega
  \left( 
    F_{Y(1)}(z|\cdot)
    ,h_N
  \right)
  \sum_{i=1}^{N} 
  \frac{
    T_i\cdot w(X_i) +1 }{N}
    \\
    &
  \ 
    =
  \ 
    2
    \sqrt{N}
  \sup_{z\in\R}
  \omega
  \left( 
    F_{Y(1)}(z|\cdot)
    ,h_N
  \right)
  \,.
\end{align*}
The equality is due to 
\begin{gather}
  1
  \ 
  =
  \ 
\frac{1}{N}\sum_{i=1}^{n}w_i^\dagger
  \ 
  =
  \ 
\frac{1}{N}\sum_{i=1}^{n}w(X_i)
  \ 
=
  \ 
\frac{1}{N}\sum_{i=1}^{N}T_iw(X_i)
\,,
\end{gather}
that is, $w(X_i)$ satisfy the second constraint of Problem~\ref{bw:1:primal}.
The second inequality follows from 
$\sum_{k=1}^{N}B_k(X)=1$ and the convexity of the absolute value. 
Indeed,
\begin{align*}
&
  \sup_{z\in\R}
  \left| 
  F_{Y(1)}(z|X_i)
  -
  \sum_{k=1}^{N} 
  B_k(X_i)
  \cdot
  F_{Y(1)}(z|X_k)
  \right|
  \\
  &
  \ 
  \le
  \ 
  \sum_{k=1}^{N} 
  \frac{\mathbf{1}_{\left\{ X_k\in A_N(X_i) \right\}}}
  {
    \sum_{j=1}^{N} 
\mathbf{1}_{\left\{ X_j\in A_N(X_i) \right\}}
  }
  \sup_{z\in\R}
  \left| 
  F_{Y(1)}(z|X_i)
  -
  F_{Y(1)}(z|X_k)
  \right|
  \\
  &
  \ 
  \le
  \ 
  \sup_{z\in\R}
  \omega
  \left( 
    F_{Y(1)}(z|\cdot)
    ,h_N
  \right)
  \,.
\end{align*}

Since we assume
\begin{gather*}
  \sqrt{N}
  \sup_{z\in\R}
  \omega
  \left( 
    F_{Y(1)}(z|\cdot)
    ,h_N
  \right)
  \to
  0
  \qquad
  \text{for}\ 
  N\to \infty
  \,,
\end{gather*}
it follows
$\sup_{z\in\R}|R_2(z)|\overset{\P}{\to}0$.
\end{proof}
\begin{remark}
In the original paper \cite{Wang2019} the authors derive concrete learning rates for the weights and employ them in bounding this term. They obtain a multiplied learning rate, which is sufficiently fast. Their approach, however, calls for concrete learning rates of the weights. Arguably, the process of deriving such rates is the most complicated part of the paper. 
I found out, that we don't need concrete rates for the weights. 
Consistency of the weights is enough and gives us an (arbitrarily slow but sufficient) learning rate to establish the results.
We don't even need rates for the weights to control $R_2$.
They only play a role in bounding $R_3$.

We also want to comment on the assumption
\begin{gather*}
  \sqrt{N}
  \sup_{z\in\R}
  \omega
  \left( 
    F_{Y(1)}(z|\cdot)
    ,h_N
  \right)
  \to
  0
  \qquad
  \text{for}\ 
  N\to \infty
  \,,
\end{gather*}
I decided to keep this more general (and abstract) assumption, althogh
there are many (more concrete, yet stronger) assumptions on the regularity of
$
    F_{Y(1)}(z|\cdot)
$
and the convergence speed of $h_N$.
If for example 
$
    F_{Y(1)}(z|\cdot)
$
is $\alpha$-Hölder continuous with $\alpha\in(0,1]$ for all $z\in\R$, it suffices $\sqrt{N}h_N^\alpha\to0$ to control $R_2$.
\end{remark}

To control the remaining terms $R_3$ and $R_4$ we use empirical processes. We introduce the concept and the results we need in the next paragraphs. 
For an introduction to empirical processes see \cite{Vaart2000}. More advanced techniques are in \cite{vaart2013}.

Let 
$
  \left( 
    \Omega,
    \mathcal{A},
    \P
  \right)
$
be a probability space,
$
  \left( 
    \mathcal{Z},
    \Sigma
  \right)
$
a measurable space, and 
$
  \xi_1,\ldots,\xi_N
  :
  \left( 
    \Omega,
    \mathcal{A},
    \P
  \right)
  \to
  \left( 
    \mathcal{Z},
    \Sigma
  \right)
$
a sample 
of independent and identically-distributed
random variables
with probability distribution $\P_{\!\xi}$.
A family $\mathcal{F}$ of measurable functions 
$
  f:
  \left( 
    \mathcal{Z},
    \Sigma
  \right)
    \to
  \left( 
    \R,
    \mathcal{B}(\R)
  \right)
$
induces a stochastic process by
\begin{gather}
  f
  \ 
  \mapsto
  \ 
  \G_N f 
  \ 
  :=
  \ 
  \frac{1}{\sqrt{n}}
  \sum_{i=1}^{N} 
  (
    f(\xi_i)
    -
    \E_\xi[f]
  \,.
\end{gather}
We call this the  \textbf{empirical process} $\G_N$ indexed by $\mathcal{F}$.
We define the (random) norm
\begin{gather}
  \norm{\G_n}_\mathcal{F}
  :=
  \sup_
        { f \in \mathcal{F}}
        \left|
          \G_N f
        \right|
        .
\end{gather}
\begin{remark}
We stress that 
$
  \norm{\G_n}_\mathcal{F}
$
often ceases to be measurable, even in simple situations~\cite[page 3]{vaart2013}.
To deal with this, we introduce the notion of \textbf{outer expectation} $\E^*$, that is,
\begin{gather}
  \E^*[Z]
  \ 
  :=
  \ 
    \inf
  \left\{ 
    \E[U]
  \ 
  \lvert
  \ 
    U\ge Z,
    \ 
    U:
  \left( 
    \Omega,
    \mathcal{A},
    \P
  \right)
  \to 
  \left( 
    \overline{\R},
    \mathcal{B}(\overline{\R})
  \right)
  \text{measurable and}
  \ 
  \E[U]<\infty
  \right\}
  \,.
\end{gather}
In our application the technical difficulties halt at this point, because we only consider $Z$ with $\E^*[Z]<\infty$. Then there exists a smallest measurable function $Z^*$ dominating $Z$ with
$\E^*[Z]=\E[Z^*]$. Thus, we may assume $Z$ to be measurable in this regard.


\end{remark}
In our application we need concentration inequalities for 
$
  \norm{\G_n}_\mathcal{F}
$.
One easy way is to use maximal inequalities for the expectation together with Markov's inequality. There are also Bernstein-like inequalities for empirical processes. We need to introduce more concepts.

Given two functions $\underline{f}$ and $\overline{f}$, the bracket
$[\underline{f},\overline{f}]$ 
is the set of all functions $f$ with 
$\underline{f}\le f \le \overline{f}$.
We define a
$(\varepsilon, L^{r}(\P))$-bracket
to be a bracket
$[\underline{f},\overline{f}]$ with
$\norm{\overline{f}-\underline{f}}_{ L^r(\P)}< \varepsilon$.
The bracketing number 
$
N_{[\,]}(\varepsilon, \mathcal{F}, L^r(\P))
$
is 
the minimum number of 
$(\varepsilon, L^{r}(\P))$-brackets needed to cover $\mathcal{F}$.
For most classes $\mathcal{F}$ the bracketing number grows to infinity for $\varepsilon\to 0$.
To measure the speed of convergence we introduce the bracketing integral
\begin{gather}
     J
    _{[]}
    (
    \delta
    ,
    \mathcal{F}
    ,
    L_r(\P)
    )
    =
  \int_0^{\delta}
      \sqrt{
        \log 
      N_{[\,]}
\left( \varepsilon, \mathcal{F}_N, L^r(\P) \right)
    }
    \,
    d\varepsilon
    \,.
\end{gather}
An envelope function $F$ of a class $\mathcal{F}$ satisfies 
$|f(x)|\le F(x)< \infty$ for all $f\in\mathcal{F}$ and all $x$.
\begin{theorem}
  \label{th:max_ineq}
  \emph{(Maximal inequality)}
  For any class $\mathcal{F}$ of measurable functions with envelope function $F,$
  \begin{gather}
    \E^*
    _\P
    [
    \norm{
      \G
      _n
      }
      _\mathcal{F}
    ]
    \lesssim
    J
    _{[]}
    (
    \norm{
      F
    }
    _{ L^2(\P)}
    ,
    \mathcal{F}
    ,
    L_2(\P)
    )
    .
  \end{gather}
\end{theorem}
\begin{proof}
  \cite[Corollary~19.35]{Vaart2000}
\end{proof}

et 
  $(\varepsilon_N)\subset(0,1]$ be a decreasing sequence with 
  $\varepsilon_N\to 0$ as $N\to\infty$ and define for
  $N\in\mathbb{N}$ and $z\in\R$ the function
 \begin{align*}
    f_{\varepsilon_N,\pm}^z
    \ 
    :
    \ 
    &
      \left\{ 0,1 \right\}
      \times
      \mathcal{X}
      \times
      \mathcal{Y}
    \ 
    \to
    \ 
    \R
    \\
    &
      (T,X,Y(T))
      \ 
      \mapsto
      \ 
      \mathbf{1}
      _
      {\left\{ 
          \left|
      w(X)-\frac{1}{\pi(X)}
          \right|
          \,
          \le
          \,
          \varepsilon_N/2
      \right\}}
      \left( 
      w(X)-\frac{1}{\pi(X)}
      \right)
      ^{\pm}
      T
      \left( 
        \mathbf{1}
        _{\left\{  Y(T)\,\le\,z \right\}}
        -
        F_{Y(1)}(z|X)
      \right)
      \,,
  \end{align*}
  and define 
  \begin{gather*}
    \mathcal{F}_N^\pm
    :=
    \left\{ 
    f_{\varepsilon_N,\pm}^z
    |
    z\in\R
    \right\}
    \,.
  \end{gather*}
\begin{lemma}
  Consider the weights function
  \begin{align*}
    w
    \colon
    \mathcal{X}
    \times
    \R^{N+1}
    \ 
    \to
    \ 
    \R
    \,,
    \quad
    (X,(\lambda,\lambda_0))
    \ 
    \mapsto
    \ 
    (
    f^{'}
    )^{-1}
    \left( 
      \inner{B(X)}{\lambda}
      +
      \lambda_0
    \right)
    \,.
  \end{align*}
    If the optimal solution 
    $
    (\lambda^\dagger,\lambda_0^\dagger)
    $ to Problem? exists and is measurable for all $N\in\mathbb{N}$ 
    then $
    w
    \circ
    (
    X
    ,
    (\lambda^\dagger,\lambda_0^\dagger)
    )
    $
    is measurable.
\end{lemma}
\begin{lemma}
  \label{bounded_f_lemma}
    If the optimal solution $(\lambda^\dagger,\lambda_0^\dagger)$ to Problem? exists and is measurable for all $N\in\mathbb{N}$ 
  the function class
  $
    \mathcal{F}_N^\pm
  $
    is measurable for all $N\in\mathbb{N}$.
    Furthermore, $\mathcal{F}_N^\pm$ is bounded above by $\varepsilon_N$ and
    \begin{gather*}
     J_{[\, ]}
    \left( 
      \varepsilon_N
    ,
    \mathcal{F}_N^\pm
    ,
     L^2(\P)
    \right)
    \ 
    \to
    \ 
    0
    \qquad
    \text{for}
    \ 
    N
    \to
    \infty
\,. 
    \end{gather*}
\end{lemma}
\begin{proof}
  If the optimal solution $(\lambda^\dagger,\lambda_0^\dagger)$ to Problem? exists and is measurable for all $N\in\mathbb{N}$ 
  then the weights function $w$ is measurable. Indeed,
  this follows from
  \begin{gather*}
    w(X)=
    (
    f^{'}
    )
    ^{-1}
    \left( \inner{B(X)}{\lambda^\dagger}+\lambda^\dagger_0 \right)
    \,,
  \end{gather*}
  the measurability of the basis functions $B$ and the continuity of $
    (
    f^{'}
    )
    ^{-1}
    $. Thus $f_{\varepsilon_N,\pm}^z$ are measurable for all $N\in\mathbb{N}$ and all $z\in\R$. Clearly, 
    $\mathcal{F}_N^\pm$ is bounded above by $\varepsilon_N$.
    Since 
    \begin{gather*}
       \mathbf{1}
      _
      {\left\{ 
          \left|
      w(X)-\frac{1}{\pi(X)}
          \right|
          \,
          \le
          \,
          \varepsilon_N/2
      \right\}}
      \left( 
      w(X)-\frac{1}{\pi(X)}
      \right)
      ^{\pm}
    \end{gather*}
    is non-negative and bounded above by $\varepsilon_N$
    it follows from Lemma~\ref
    {lem:brack_n}
  \begin{gather*}
    N_{[\,]}
    (
    \varepsilon
    ,
    \mathcal{F}_N^\pm, L^2(\P))
    \ 
    \lesssim
    \ 
    \left( 
    \frac
    {\varepsilon_N}
    {\varepsilon}
    \right)^2
    \ 
    \lesssim
    \ 
    \left( 
    \frac
    {1}
    {\varepsilon}
    \right)^2
    \qquad
    \text{for all}
    \ 
    \varepsilon>0
    \,.
  \end{gather*}
Thus
    \begin{align*}
     J_{[\, ]}
    \left( 
      \varepsilon_N
    ,
    \mathcal{F}_N^\pm
    ,
     L^2(\P)
    \right)
    &
    \ 
=
\  
\int^{\varepsilon_N}_0
\sqrt{
\log
    N_{[\,]}
    (
    \varepsilon
    ,
    \mathcal{F}_N^\pm, L^2(\P))
}
\, 
d\varepsilon
\\
&
\ 
\lesssim
\ 
\int^{\varepsilon_N}_0
\log
\left( 
  \frac{1}{\varepsilon}
\right)
\, 
d\varepsilon
\ 
=
\ 
\varepsilon_N
-
\varepsilon_N \log(\varepsilon_N)
\ 
\to
\ 
0 
\qquad
\text{for}\ 
N\to\infty
\,.
    \end{align*}
\end{proof}
In the next lemma we get rid of the $\varepsilon_N$ bound
\begin{lemma}

  \label{lemma_fpm}
 Consider for $z\in\R$ the function
 \begin{align*}
    f_{\pm}^z
    \ 
    :
    \ 
      \left\{ 0,1 \right\}
      \times
      \mathcal{X}
      \times
      \mathcal{Y}
    &
    \ 
    \to
    \ 
    \R
    \\
      (T,X,Y(T))
    &
      \ 
      \mapsto
      \ 
      \left( 
      w(X)-\frac{1}{\pi(X)}
      \right)
      ^{\pm}
      T
      \left( 
        \mathbf{1}
        _{\left\{  Y(T)\,\le\,z \right\}}
        -
        F_{Y(1)}(z|X)
      \right)
      \,.
  \end{align*}
  It holds
  $\sup_{z\in\R}\left| \G_N f^z_\pm \right|\overset{\P}{\to}0$.
  Furthermore, for
 \begin{align*}
    f_z
    \ 
    :
    \ 
      \left\{ 0,1 \right\}
      \times
      \mathcal{X}
      \times
      \mathcal{Y}
    &
    \ 
    \to
    \ 
    \R
    \\
      (T,X,Y(T))
    &
      \ 
      \mapsto
      \ 
      \left( 
      w(X)-\frac{1}{\pi(X)}
      \right)
      T
      \left( 
        \mathbf{1}
        _{\left\{  Y(T)\,\le\,z \right\}}
        -
        F_{Y(1)}(z|X)
      \right)
      \,.
  \end{align*}
  it holds
  $\sup_{z\in\R}\left| \G_N f_z \right|\overset{\P}{\to}0$.
\end{lemma}
\begin{proof}
  It holds for all $z\in\R$
  $
  $
  \begin{gather*}
  f^z_\pm
      (T,X,Y(T))
      \ 
      \lesssim
      \ 
     \left(
      w(X)- \frac{1}{\pi(X)}
      \right)
      ^\pm
      \,.
  \end{gather*}
  By Theorem~\ref{aa:weights:th}
  there exists a decreasing sequence 
  $(\varepsilon_N)\subset(0,1]$
  with $\varepsilon_N\to 0$
  and
  $
  \P[
\left| 
      w(X)- 1/\pi(X)
\right|
\le
\varepsilon_N/2
  ]
  \to 1
  $
  for $N\to\infty$.
  Therefore
  \begin{gather*}
  \P
  \left[
  f^z_\pm
  \in
  \mathcal{F}_N^\pm
  \
  \forall
  z\in\R
  \right]
    =
  \P[
\left| 
      w(X)- 1/\pi(X)
\right|
\le
\varepsilon_N/2
  ]
  \to 1
  \,.
  \end{gather*}
Then, for all $\varepsilon>0$ it holds
\begin{align*}
  \P
  \left[
  \sup_{z\in\R}
  \left| 
  \G_N
  f^z_\pm
  \right|
  \le \varepsilon
  \right]
  &
  \ 
  \ge
  \ 
  \P
  \left[
  f^z_\pm
  \in
  \mathcal{F}_N^\pm
  \
  \forall
  z\in\R
  \,,
  \ 
  \norm{\G_N}^*_{\mathcal{F}_N^\pm}
  \le \varepsilon
  \right]
  \\
  &
  \ 
  \ge
  \ 
  \P
  \left[
  f^z_\pm
  \in
  \mathcal{F}_N^\pm
  \
  \forall
  z\in\R
  \right]
  \ 
  -
  \ 
  \P
  \left[
  \norm{\G_N}^*_{\mathcal{F}_N^\pm}
  \le \varepsilon
  \right]
  \\
  &
  \ 
  \to
  \ 
  1
  \,.
\end{align*}
The convergence of the last term to 0 is due to 
Lemma~\ref{bounded_f_lemma} and Lemma~\ref{markov_max_lemma}.
Thus
  $\sup_{z\in\R}\left| \G_N f^z_\pm \right|\overset{\P}{\to}0$.
  Note, that 
  \begin{gather*}
\G_N f_z 
=
\G_N f^z_+
-
\G_N f^z_-
\qquad
\text{for all}\ 
z\in\R
\,.
  \end{gather*}
  Thus by Slutzky's Theorem\cite[Theorem~13.18]{Klenke2020}
  it holds
  $\sup_{z\in\R}\left| \G_N f_z \right|\overset{\P}{\to}0$.
\end{proof}
%\begin{lemma}
%  Define for
%  $z\in\R$
% \begin{align*}
%    f_z
%    \ 
%    :
%    \ 
%      \left\{ 0,1 \right\}
%      \times
%      \mathcal{X}
%      \times
%      \mathcal{Y}
%&
%    \ 
%    \to
%    \ 
%      \R
%    \\
%    \left(
%      T,X,Y(T)
%    \right)
%    &
%      \ 
%      \mapsto
%      \ 
%      \frac{T}{\pi(X)}
%      \left( 
%        \mathbf{1}
%        _{\left\{  Y(T)\,\le\,z \right\}}
%        -
%        F_{Y(1)}(z|X)
%      \right)
%      \ 
%      +
%      \ 
%      \left( 
%        F_{Y(1)}(z|X)
%        -
%        F_{Y(1)}(z)
%      \right)
%      \,,
%  \end{align*}
%and consider the function class
%$
%  \mathcal{G}
%  \ 
%  :=
%  \ 
%  \left\{ 
%    f_z
%  \ 
%    |
%  \ 
%    z\in\R
%  \right\}
%$
%\begin{gather}
%    \,.
%\end{gather}
%  Then $\mathcal{G}$ is a class of measurable functions 
%  and it holds
%  \begin{gather}
%    N_{[\,]}(\varepsilon,\mathcal{G}, L^2(\P))
%    \le
%    ??
%    \qquad
%    \text{for all}
%    \ 
%    \varepsilon>0
%    \,.
%  \end{gather}
%\end{lemma}
%\begin{lemma}
%  \label{lemma_max_ineq}
%  Consider a function class $\mathcal{F}$ with unit ball
%  $
%  B_{\mathcal{F}}
%  :=
%  \left\{ 
%    f\in \mathcal{F}
%    \colon
%    \norm{f}_\infty
%    \le
%    1
%  \right\}
%  $.
%  Let $(\varepsilon_N)$ be a sequence converging to 0
%  and let 
%  $
%  \left( 
%    \mathcal{F}_N
%  \right)
%    :=
%    \left( 
%      C\cdot
%    \varepsilon_N\cdot B_\mathcal{F}
%    \right)
%  $
%  denote the sequence of scaled unit balls in $\mathcal{F}$.
%  Assume that 
%  there exists
%  $k<2$ such that 
%  the covering number of the unit ball in $\mathcal{F}$
%  satisfies
%  \begin{gather}
%        \log 
%      N_{[\,]}
%\left( \varepsilon, B_\mathcal{F}, L^2(\P) \right)
%\ 
%\lesssim
%\ 
%\left( \frac{1}{\varepsilon} \right)^k
%\qquad
%\text{for all}\ 
%\varepsilon>0
%\,.
%  \end{gather}
%  Then it holds
%  $
%      \norm{G_N}^*_{\mathcal{F}_N}
%    \ 
%    \overset{\P}
%    {\to}
%    \ 
%    0
%  $
%  for $N\to \infty$. 
%\end{lemma}
%\begin{proof}
%  By maximal inequalities it holds
%  \begin{align*}
%    \E^*
%    \left[ 
%      \norm{G_N}_{\mathcal{F}_N}
%    \right]
%    &
%      \ 
%      \lesssim
%      \ 
%      J_{[\,]}\left( \varepsilon_N, \mathcal{F}_N, L^2(\P) \right)
%      \\
%    &
%      \ 
%      =
%      \ 
%      \int_0^{\varepsilon_N}
%      \sqrt{
%        \log 
%      N_{[\,]}
%\left( \varepsilon, \mathcal{F}_N, L^2(\P) \right)
%    }
%    \,
%    d\varepsilon
%    \\
%    &
%    \ 
%    =
%    \ 
%      \int_0^{\varepsilon_N}
%      \sqrt{\log 
%      N_{[\,]}
%\left( \varepsilon/(C\cdot \varepsilon_N), B_\mathcal{F}, L^2(\P) \right)
%    }
%    \,
%    d\varepsilon
%    \\
%    &
%    \ 
%    \lesssim
%    \ 
%      \int_0^{\varepsilon_N}
%      \left( 
%      \frac{\varepsilon_N}{\varepsilon}
%    \right)^{k/2}
%    \,
%    d\varepsilon
%    \\
%    &
%    \ 
%    =
%    \ 
%  \varepsilon_N^{k/2}
%  \frac{1}{1-k/2}
%  \varepsilon_N^{1-k/2}
%  \\
%    &
%    \ 
%  \lesssim
%    \ 
%  \varepsilon_N
%  \\
%    &
%    \ 
%  \to
%  \ 
%  0
%  \qquad
%  \text{for}\ 
%  N\to
%  \infty
%  \,.
%  \end{align*}
%  Note, that $k<2$.
%  By the boundedness of $\E^*$ there is no measurability problem.
%  By Markov's Inequality it holds
%  \begin{align*}
%    \P
%    \left[ 
%      \norm{G_N}^*_{\mathcal{F}_N}\ge \varepsilon
%    \right]
%    \le
%    \varepsilon^{-1}
%    \,
%    \E^*
%    \left[ 
%      \norm{G_N}_{\mathcal{F}_N}
%    \right]
%    \ 
%    \to
%    \ 
%    0
%    \qquad
%    \text{for}\ 
%    N\to\infty
%    \,.
%  \end{align*}
%\end{proof}
%The next two lemmas connect $R_3$ to the theory of empirical processes.
%\begin{lemma}
%  \label{aa:mean:l:fz}
%  Consider 
%  the (random) function
%  $
%  f_D^z
%  $ given by
%  \begin{gather}
%    f_{D}^z(T,X,Y(T))
%    :=
%    T
%    \left( 
%    w(D,X)- \frac{1}{\pi(X)}
%    \right)
%    \left( 
%    \mathbf{1}_{\left\{ Y(T) \le z \right\}}
%    -
%  F_{Y(1)}(z|X)
%    \right)
%    \,.
%  \end{gather}
%  Assume that 
%  there exists a function class $\mathcal{F}$ satisfying the requirements of Lemma~\ref{lemma_max_ineq} and that
%  $
%  f_D^z
%  \in \mathcal{F}
%  $
%  for all $z\in\R$ almost surely.
%  It then holds
%  $
%  \sup_{z\in\R} \left| G_Nf_D^z \right|
%  \overset{\P}{\to}0$ for 
%  $N\to\infty$.
%\end{lemma}
%\begin{proof}
%  By the consistency of the weights there exists a learning rate $(\varepsilon_N)$ such that
%  \begin{gather}
%    \P
%    \left[ 
%      \left| 
%      w(X,D)
%      -
%      \frac{1}{\pi(X)}
%      \right|
%      \le
%      \varepsilon_N
%    \right]
%    \to 1 
%    \qquad
%    \text{for}\ 
%    N\to \infty
%    \,.
%  \end{gather}
%  Let
%  $\mathcal{F}_N:=\varepsilon_NB_\mathcal{F}$ as in 
%  Lemma~\ref{lemma_max_ineq}.
%  It holds
%\begin{gather}
%      \sup_{z\in\R}
%      \left| 
%    f_D^z
%      \right|
%      \lesssim
%      \left| 
%      w(X,D)
%      -
%      \frac{1}{\pi(X)}
%      \right|
%      \le
%      \varepsilon_N
%\end{gather}
%with probability going to 1 as $N\to\infty$.
%Thus
% \begin{gather}
%    \P
%    \left[ 
%    f_D^z
%    \in
%  \mathcal{F}_N
%  \ \forall\,z\in\R
%    \right]
%    =
%    \P
%    \left[ 
%      \sup_{z\in\R}
%      \left| 
%    f_D^z
%      \right|
%      \lesssim
%      \varepsilon_N
%    \right]
%    \to 1
%    \qquad
%    \text{as}
%    \ 
%    N\to\infty
%    \,.
%  \end{gather}
%  Then it holds
%  for all $\varepsilon>0$ 
%  \begin{align*}
%    \P
%    \left[ 
%      \sup_{z\in\R}
%  \left| G_Nf_D^z \right|
%  \le
%  \varepsilon
%    \right]
%    &
%    \ 
%    \ge
%    \ 
%    \P
%    \left[ 
%      \sup_{z\in\R}
%  \left| G_Nf_D^z \right|
%  \le
%  \norm{G_N}^*_{\mathcal{F}_N}
%  \le
%  \varepsilon
%    \right]
%    \\
%    &
%    \ 
%    \ge
%    \ 
%    \P
%    \left[ 
%    f_D^z
%    \in
%  \mathcal{F}_N
%  \ \forall\,z\in\R
%      \ 
%      \text{and}\ 
%  \norm{G_N}^*_{\mathcal{F}_N}
%  \le
%  \varepsilon
%    \right]
%    \\
%    &
%    \ 
%    \ge
%    \ 
%    \P
%    \left[ 
%    f_D^z
%    \in
%  \mathcal{F}_N
%  \ \forall\,z\in\R
%    \right]
%    -
%    \P
%    \left[ 
%  \norm{G_N}^*_{\mathcal{F}_N}
%    \ 
%  \ge
%    \ 
%  \varepsilon
%    \right]
%    \\
%    &
%    \ 
%    \to
%    \ 
%    1
%    \,.
%  \end{align*}
%  The convergence of the second term is due to Lemma~\ref{lemma_max_ineq}.
%
%\end{proof}
%
\begin{lemma}
  \label{aa:mean:l:r3}
  Assume conditional unconfoundedness, that is,
  \begin{gather}
  (Y(0),Y(1))\perp T \ |\ X
  \,.
  \end{gather}
  Then for all
  $z\in\R$
  it holds
  $
  G_Nf_z=R_3(z)
  $.
  Furthermore, under conditions it holds
  $\sup_{z\in\R}\left| R_3(z) \right|\overset{\P}{\to}0$.
\end{lemma}
\begin{proof}
  A standard computation shows
  \begin{gather}
    \E
    \left[ 
    \frac{T}{\pi(X)}
    \left( 
    \mathbf{1}_{\left\{ Y(T) \le z \right\}}
    -
  F_{Y(1)}(z|X)
    \right)
    \right]
    =0
    \,.
  \end{gather}
  Furthermore
  \begin{align*}
    &
    \E
    \left[ 
      Tw(X,D)
    \left( 
    \mathbf{1}_{\left\{ Y(T) \le z \right\}}
    -
  F_{Y(1)}(z|X)
    \right)
    \right]
    \\
    &
    \ 
    =
    \ 
    \E
    \left[ 
      \E
      \left[ 
      w(X,D)
      \left( 
    \mathbf{1}_{\left\{ Y(1) \le z \right\}}
    -
  F_{Y(1)}(z|X)
      \right)
  |T=1,X,D
      \right]
    \right]
    \\
    &
    \ 
    =
    \ 
    \E
    \left[ 
      w(X,D)
      \E
      \left[ 
    \mathbf{1}_{\left\{ Y(1) \le z \right\}}
    -
  F_{Y(1)}(z|X)
  |X,D
      \right]
    \right]
    \\
    &
    \ 
    =
    \ 
    \E
    \left[ 
      w(X,D)
      \E
      \left[ 
    \mathbf{1}_{\left\{ Y(1) \le z \right\}}
    -
  F_{Y(1)}(z|X)
  |X
      \right]
    \right]
    \\
    &
    \ 
    =
    \ 
    0
  \end{align*}
  The second equality is due to the assumption of 
  $(Y(0),Y(1))\perp T |X$.
  The third equality is due to $X\perp D$.
  Thus
  $
    \E
    f_D^z
    =
    0
  $
\end{proof}

Until now, all parts of the error decomposition converge to 0.
The last term $R_4$ will decide the profile of the limiting process.
To this end we need the following concept.
\begin{definition}
  We call a class 
  $\mathcal{F}$ of measurable functions 
$\P$-Donsker
if the sequence of processes 
$\left\{ G_N f \colon f\in\mathcal{F}\right\}$
converges in
$l^\infty(\mathcal{F})$
to a tight limit process.
\end{definition}

\begin{theorem}
  Every class $\mathcal{F}$ of measurable functions 
  with
  $
    J
    _{[]}
    (
    1
    ,
    \mathcal{F}
    ,
    L_2(\P)
    )
    <\infty
  $
  is

  $\P$-Donsker, that is,
  the sequence of processes 
$\left\{ G_N f \colon f\in\mathcal{F}\right\}$
  converges 
  in
$l^\infty(\mathcal{F})$
to a Gaussian process with mean 0 and covariance function given by
\begin{gather}
  \mathrm{Cov}(f,g)
  :=
  \E[fg]-\E[f]\E[g]
  \,.
\end{gather}
\end{theorem}
\begin{proof}
  \cite[Theorem~19.5]{Vaart2000}
\end{proof}



\begin{lemma}
  \label{aa:mean:l:r4}
  Let
  $1/\pi(X)\in L^2(\P)$.
  $R_4$ converges to a gaussian process.
\end{lemma}
\begin{proof}
But then $\mathcal{G}$ is $\P$-Donsker.
By the Donsker Theorem \cite[Theorem~19.5]{Vaart2000}
the process $R_4$ converges in $l^\infty(\R)$ to a Gaussian process, called $\P$-Brownian bridge, with mean 0 and covariance?
\subsubsection*{Covariance}
\begin{align*}
  &
  \E
  \left[
  \left( 
  f^{z_1}_{1/\pi}
  +
  F_{Y(1)}(z_1|X)
  -
F_{Y(1)}(z_1)
  \right)
  \left( 
  f^{z_2}_{1/\pi}
  +
  F_{Y(1)}(z_2|X)
  -
F_{Y(1)}(z_2)
  \right)
  \right]
  \\
  &
  \ 
  =
  \ 
\E
\left[
  f^{z_1}_{1/\pi}
  \cdot
  f^{z_2}_{1/\pi}
\right]
\\
  &
  \quad
  +
  \ 
  \E
  \left[
  f^{z_1}_{1/\pi}
  \left( 
  F_{Y(1)}(z_2|X)
  -
F_{Y(1)}(z_2)
  \right)
  \right]
  \ 
  +
  \ 
  \E
  \left[
  f^{z_2}_{1/\pi}
  \left( 
  F_{Y(1)}(z_1|X)
  -
F_{Y(1)}(z_1)
  \right)
  \right]
  \\
  &
  \quad
  +
  \ 
  \E
  \left[
  \left( 
  F_{Y(1)}(z_1|X)
  -
F_{Y(1)}(z_1)
  \right)
  \left( 
  F_{Y(1)}(z_2|X)
  -
F_{Y(1)}(z_2)
  \right)
  \right]
  \\
  &
  \ 
  =:
  \ 
  C_0
  \quad 
  +
  \quad 
  C_1
  +
  C_2
  \quad 
  +
  \quad 
  C_3
  \,.
\end{align*}
It holds
\begin{align*}
  C_0 
  &
  \ 
  =
  \ 
\E
\left[
  f^{z_1}_{1/\pi}
  \cdot
  f^{z_2}_{1/\pi}
\right]
\\
&
\ 
=
\ 
\E
\left[
\frac{1}{\pi(X)}
\frac{T}{\pi(X)}
\left( 
\mathbf{1}_{\left\{ Y(T)\,\le\, z_1 \right\}}
-
F_{Y(1)}(z_1|X)
\right)
\left( 
\mathbf{1}_{\left\{ Y(T)\,\le\, z_2 \right\}}
-
F_{Y(1)}(z_2|X)
\right)
\right]
\\
&
\ 
=
\ 
\E
\left[
\frac{1}{\pi(X)}
\left( 
\mathbf{1}_{\left\{ Y(1)\,\le\, z_1 \right\}}
-
F_{Y(1)}(z_1|X)
\right)
\left( 
\mathbf{1}_{\left\{ Y(1)\,\le\, z_2 \right\}}
-
F_{Y(1)}(z_2|X)
\right)
\right]
\\
&
\ 
=
\ 
\E
\left[
\frac{1}{\pi(X)}
\left( 
F_{Y(1)}(z_1\land z_2|X)
\ 
-
\ 
F_{Y(1)}(z_1|X)
\cdot
F_{Y(1)}(z_2|X)
\right)
\right]
\,.
\end{align*}
\begin{align*}
  C_1
  &
  \ 
  =
  \ 
 \E
  \left[
  f^{z_1}_{1/\pi}
  \left( 
  F_{Y(1)}(z_2|X)
  -
F_{Y(1)}(z_2)
  \right)
  \right]
  \\
  &
  \ 
  =
  \ 
 \E
  \left[
\frac{T}{\pi(X)}
\left( 
\mathbf{1}_{\left\{ Y(T)\,\le\, z_1 \right\}}
-
F_{Y(1)}(z_1|X)
\right)
  \left( 
  F_{Y(1)}(z_2|X)
  -
F_{Y(1)}(z_2)
  \right)
  \right]
  \\
  &
  \ 
  =
  \ 
 \E
  \left[
\left( 
\mathbf{1}_{\left\{ Y(1)\,\le\, z_1 \right\}}
-
F_{Y(1)}(z_1|X)
\right)
  \left( 
  F_{Y(1)}(z_2|X)
  -
F_{Y(1)}(z_2)
  \right)
  \right]
  \\
  &
  \ 
  =
  \ 
  0
  \,.
\end{align*}
In the same way we see $C_2=0$.
\begin{align*}
  C_3
  &
  \ 
  =
  \ 
  \E
  \left[
  \left( 
  F_{Y(1)}(z_1|X)
  -
F_{Y(1)}(z_1)
  \right)
  \left( 
  F_{Y(1)}(z_2|X)
  -
F_{Y(1)}(z_2)
  \right)
  \right]
  \\
  &
  \ 
  =
  \ 
  \E
  \left[
  F_{Y(1)}(z_1|X)
  \cdot
  F_{Y(1)}(z_2|X)
  \right]
  \ 
  -
  \ 
  F_{Y(1)}(z_1)
  \cdot
  F_{Y(1)}(z_2)
  \,.
\end{align*}
Adding up the results gives us \eqref{cov:lp}.
\end{proof}
We have gathered all the results to prove Theorem~\ref{aa:mean:th}.
\begin{proof}
  \emph{(Theorem~\ref{aa:mean:th})}
  We connect the statement of the theorem to the error decomposition by Lemma~\ref{aa:mean:lemma_decomp}.
  By Lemma~\ref{aa:mean:l:r1}, Lemma~\ref{aa:mean:l:r2},
   Lemma~\ref{aa:mean:l:fz} and Lemma~\ref{aa:mean:l:r3}
   it follows 
   $\sup_{z\in\R}|R_i(z)|\overset{\P}{\to}0$ for $i=1,2,3$.
   Thus, by Slutzky's theorem (cf.\cite[Theorem~13.8]{Klenke2020})
   the behaviour of the limiting process is the one of Lemma~\ref{aa:mean:l:r4}.
\end{proof}
