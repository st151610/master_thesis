\begin{ftheorem}
  \label{dual_solution_th}
  The dual of 
  Problem~\ref{bw:1:primal}
  is the unconstrained optimization problem 
\begin{gather*}
  \underset
  {\lambda_0,\ldots,\lambda_{N}\in \R}
  {
    \mathrm{minimize}
  }
  \quad
  \frac{1}{N}
\sum_{i=1} 
  ^N
  \left[ 
    \,
  T_i
  \cdot
  f^*
  \!
  \left( 
\lambda_0
+
\inner
{B(X_i)}
{\lambda}
  \right)
  \ 
-
\ 
  \left( 
\lambda_0
+
\inner
{B(X_i)}
{\lambda}
  \right)
  \,
  \right]
  \ 
+
\ 
\inner
{\delta}
{|\lambda|}
  \,.
\end{gather*}
  where
  \begin{gather*}
  f^*
  \,
  \colon
  \, 
  \R
  \ 
  \to
  \ 
  \R
  \,
  ,
  \qquad 
  x^*
  \ 
  \mapsto
  \ 
    x^*
    \!
    \cdot
    (f^{'})^{-1}(x^*)
  \ 
    -
  \ 
    f
    \left( 
      (f^{'})^{-1}(x^*)
    \right)
  \end{gather*}
  is the Legendre transformation of $f$,
  the vector
  $
    B(X_i)
    =
    \left[ 
      B_1(X_i)
      ,
      \ldots
      ,
      B_n(X_i)
    \right]
    ^\top
  $
  denotes 
  the $N$ basis functions of the covariates 
  of unit $i\in \left\{ 1, \ldots, N \right\}$
  and
  $
    \left| \lambda \right|
    =
    \left[ 
      \left| \lambda_1 \right|
      ,
      \ldots
      ,
      \left| \lambda_N \right|
    \right]
    ^\top
    ,
  $
  where $\left| \,\cdot\, \right|$
  is the absolute value of a real-valued scalar.
  Moreover,
  if $\lambda^\dagger$
  is an optimal solution of the above problem 
  then the optimal solution to problem
  Problem~\ref{bw:1:primal}
  is given by
  \begin{gather*}
    w_i^\dagger
    \ 
    =
    \ 
    (f^{'})^{-1}
    \left( 
      \inner
      {B(X_i)}
      {\lambda^\dagger}
      +
      \lambda_0^\dagger
    \right)
    \qquad
    \text{for}\ 
    i\in \left\{ 1\ldots,n \right\}
    \,.
  \end{gather*}
\end{ftheorem}

\newpage
We introduce some more notation. Let $\mathbf{I}_n$ be the $n$-dimensional unit matrix, $\mathrm{0}_n$ and $\mathrm{1}_n$ the $n$-dimensional vectors containing only zeros or ones.
Also we define the vector of basis functions of the covariates of unit 
$i\in \left\{ 1,\ldots,N \right\}$ to be
\begin{gather*}
     B(X_i)
    \ 
    :=
    \ 
    \left[ 
      B_1(X_i)
      ,
      \ldots
      ,
      B_N(X_i)
    \right]
    ^\top
    \in
    \R^N
    \,.
\end{gather*}
Furthermore, we define the matrix of basis functions \textbf{for the treated} to be
\begin{gather*}
    \mathbf{B}(\mathbf{X})
    \ 
    :=
    \ 
    \begin{bmatrix}
      B(X_1), \ldots, B(X_n)
    \end{bmatrix}
    \in\R^{N\times n}
    \,.
\end{gather*}
Note, that these are random quantities and that the size of $\mathbf{B}(\mathbf{X})$ depends on the random size $n\in\mathbb{N}$ of the treatment group in the sample.
\begin{lemma}
  \label{matrix_notation}
  A matrix formulation of Problem~\ref{bw:1:primal} is 
\begin{align}
  \label{cv:ts:primal}
  %%%% objective %%%%
    &\underset{w \in \R^n}
    {\mathrm{minimize}}
    &&\qquad\qquad
    \varphi(w)
    &&&
    \\
    %%%% Ax >= b %%%%
    \nonumber
    &\mathrm{subject}\ \mathrm{to} 
    &&\qquad\qquad
    \mathbf{U}w
    \ 
    \ge
    \ 
    d
    \,,
    \\
    \nonumber
    &
    &&\qquad\qquad
    \mathbf{A}w
    \ 
    =
    \ 
    a
    \,,
\end{align}
with objective function
\begin{gather*}
  \varphi
  \ 
  :
  \ 
  \R^n
  \to
  \ 
  \overline{\R}
  \,
  ,
  \qquad
  [w_1,\ldots,w_n]^\top
  \ 
  \mapsto
  \ 
  \sum_{i=1}^n f(w_i)
  \,,
\end{gather*}
inequality matrix and vector
\begin{alignat*}{2}
    \mathbf{U}
    &
    \ 
    :=
    \ 
    \begin{bmatrix}
      \mathbf{I}_n
      \\
      \pm\,\mathbf{B}(\mathbf{X})
    \end{bmatrix}
    \in
    \R^{(n+2N)\times n}
        \qquad
    &&
d
    \ 
    :=
    \ 
    \begin{bmatrix}
      0_n
      \\
      -N\delta 
      \ 
      \pm\ 
      \sum_{i = 1}^{N} B(X_i)
    \end{bmatrix}
    \in
    \R^{(n+2N)}
    \,,
    \intertext{and equality matrix and vector}
    \mathbf{A}
    &
    \ 
    :=
    \ 
      \mathrm{1}_n
      ^\top
      \in\R^{1\times n}
      \qquad
    &&
    a
  \ 
    :=
    \ 
    N
    \in\mathbb{N}
    \,.
\end{alignat*}
\end{lemma}
\begin{proof}
  A simple calculation yields the result. We omit the details.
\end{proof}
\begin{remark}
  The inequality constraints of
  Lemma~\ref{matrix_notation} differ from its counterpart
  \cite[Proof of Lemma~1]{Wang2019}.
  We don't transform the variable $w$, but shift to $d$ what prevents us from keeping $w$.
  Note, that the choice of
  \cite[Proof of Lemma~1]{Wang2019} leads to a mistake on page 21.
  The mistake is most obvious in the second display, where the first implication follows from dividing by 0. I discussed this with the authors and proposed a version of Lemma\ref{matrix_notation} to solve the problem. I think it's best not to transform variables, because the mistake comes from (wrongly) calculating the convex conjugate of the (more complicated) transformed version of the objective function. The subsequent analysis even simplifies with my version.
\end{remark}

