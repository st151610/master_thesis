\begin{example}
  Consider a proper convex function
  $
    f:
    \R \to ( -\infty, \infty]
  $
  We would like to compute the convex conjugate of the function 
  $h$ defined by 
  $
    h(x) = f \left( x - \frac{1}{n} \right)
  $.
  For this we notice that
  \begin{gather*}
    h = f \circ L
  \end{gather*}
  where 
  \begin{gather*}
   L := x \mapsto x - \frac{1}{n} 
  \end{gather*}
  is a linear map.
  Since 
  $
    \text{Im}(L) = \R
  $
  and 
  $\text{dom}(f) \neq \emptyset$
  we have
  $
    \text{Im}(L) 
    \cap
    \text{ri}(\text{dom}(f) )
    \neq
    \emptyset
  $.
  Furthermore
  \begin{gather}
    (L^*)^{-1}(x^*)
    =
    \left\{ 
     \left( x \mapsto x + \frac{1}{n} \right) (x^*)
    \right\}
    =
    \left\{ x^* + \frac{1}{n} \right\}
  \end{gather}
  Then Theorem~\ref{cvxa_conjugate_chain_rule}
  yields
  \begin{gather}
    h^* = f^* \circ L^{-1} 
        =  f^* \circ  
        \left( x^* \mapsto x^* + \frac{1}{n} \right)
  \end{gather}

\end{example}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
We employ 
Theorem~\ref{cvxa_fenchel_theorem}
together with the box constraints in 
Problem~\eqref{primal_weighting_binary}
to obtain Proposition~\ref{ch_1_dual}.

To prove Proposition~\ref{ch_1_near_oracle}
we employ
Proposition~\ref{syu_1_result}
and 
Corollary~\ref{syu_taylor_corollary}
to get
\begin{align}
  \begin{split}
    & 
    G(\lambda^*_1 + \Delta) 
    -
    G(\lambda^*_1)
\\
    &\ge
    \frac{1}{n}
    \sum_{j = 1}^{n} 
      \left[ 
        -T_j n 
        \rho^{'} 
        \left( 
          B(X_j)^T \lambda^*_1
        \right)
        +
        1
      \right]
      \Delta^T B(X_j)
\\
    & +
    \frac{1}{2}
    \sum_{j = 1}^{n} 
      -T_j  
      \rho^{''} 
      \left( 
        B(X_j)^T 
        (
          \lambda^*_1 + \xi \Delta
        )
      \right)
      \Delta^T
      \left( 
        B(X_j)
        B(X_j)^T
      \right)
      \Delta
\\
    &-
    |\Delta|^T \delta
\\
    &\ge
    - \norm{\Delta}_2
    \left( 
      \norm{
        \frac{1}{n}
        \sum_{j = 1}^{n} 
          \left[ 
            -T_j n 
            \rho^{'} 
            \left( 
              B(X_j)^T \lambda^*_1
            \right)
            +
            1
          \right]
        B(X_j)
      }_2
      +
      \norm{\delta}_2
    \right)
\\
    &+
    n
    \norm{\Delta}^2_2
    \varphi_{\rho^{''}}
    \underline{\varphi_{aa^T}}
  \end{split}
\end{align}

Next we employ Bernstein inequality~\ref{rmineq_bernstein} to bound
\begin{align}
    \norm{
      \frac{1}{n}
      \sum_{j = 1}^{n} 
      \left[ 
        -T_j n 
        \rho^{'} 
        \left( 
          B(X_j)^T \lambda^*_1
        \right)
      +
      1
      \right]
      B(X_j)
    }_2
    \le
    \CP \Ctau \LearnRate
\end{align}
with probability $1 - \tau$.
Then for 
$\norm{\Delta}_2$ large enough it holds
\begin{gather}
  G(\lambda^*_1 + \Delta) 
  -
  G(\lambda^*_1)
  >
  0
\end{gather}
with probability $1 - \tau$.
Thus by Proposition~\ref{syu_1_result}
  \begin{gather}
    \P
    \left( 
      \norm{
        \lambda^\dagger
        -
        \lambda^*_1
      }_2
      \le
      \norm{\Delta}_2
    \right)
    \ge 
    1 - \tau
    .
  \end{gather}
%Hartman-Winter Theorem
\begin{theorem*}
  Let 
  $
    X_1,
    X_2,
    \ldots
  $
  be i.i.d. real random variables with 
  $
    \E[X_1]=0
    \ 
    \text{and}
    \ 
    \mathbf{Var}
    [X_1]
    = 1.
  $
  Let
  $
    S_n
    :=
    X_1
    +
    \ldots
    +
    X_n,
    \ 
    n\in \mathbb{N}
    .
  $
  Then
  \begin{gather}
    \limsup_{n\to\infty}
    \,
    \frac{S_n}{
      \sqrt{
        \,
        2n
        \log
        \log
        n
      }
    }
    \ 
    =
    \ 
    1
    \qquad
    \text{a.s.}
  \end{gather}
\end{theorem*}
\begin{lemma}
  For any proper function
  $
    f:\R^n\to\overline{\R}
  $
  we have
  \begin{gather}
    f^*(x^*) 
    =
    \sigma_{\mathrm{epi}(f)}
    (x^*,-1)
    \qquad
    \text{for}
    \ 
    x^* \in \R^n.
  \end{gather}
\end{lemma}
\begin{proof}
  Let $x^*\in\R^n$
  and
  $
    (x,\lambda)\in \mathrm{epi}(f).
  $
  Then
  $
    x \in \mathrm{dom}(f)
  $
  and
  $
    f(x)\le \lambda.
  $
  Thus
  \begin{gather}
    \inner{x^*}{x} - f(x)
    \ge
    \inner{x^*}{x} - \lambda
    \qquad
    \text{for all}\ 
    (x,\lambda)\in \mathrm{epi}(f).
  \end{gather}
  On the other hand 
  $
    (x,f(x))\in \mathrm{epi}(f)
  $
  for all
  $
    x \in \mathrm{dom}(f).
  $
  It follows
  \begin{gather}
    \inner{x^*}{x} - f(x)
    \le
    \sup_{(x,\lambda)\in\mathrm{epi}(f)}
    \inner{x^*}{x} - \lambda
    \qquad
    \text{for all}\ 
    x \in \mathrm{dom}(f).
  \end{gather}
  Taking the supremum in the last two displays yields
  \begin{align}
    f^*(x^*)
    =
    \sup_{x\in\mathrm{dom}(f)}
    \inner{x^*}{x} - f(x)
    &=
    \sup_{(x,\lambda)\in\mathrm{epi}(f)}
    \inner{x^*}{x} - \lambda
    \\
    &=
    \sup_{(x,\lambda)\in\mathrm{epi}(f)}
    \inner{(x^*,-1)}{(x,\lambda)} 
    =
    \sigma_{\mathrm{epi}(f)}
    (x^*,-1).
  \end{align}
\end{proof}
\begin{proof}
  Let $x^*\in\R^n$ and fix $x_1^*,x_2^*\in\R^n$ such that
  $x^*=x^*_1+x^*_2$.
  We get
  \begin{align*}
    f^*(x^*_1)+g^*(x^*_2)
    &=
    \sup_{x\in\R^n}
    \inner{x^*_1}{x}-f(x)
    +
    \sup_{x\in\R^n}
    \inner{x^*_2}{x}-g(x)
    \\
    &\ge
    \sup_{x\in\R^n}
    \inner{x^*_1}{x}-f(x)
    +
    \inner{x^*_2}{x}-g(x)
    =
    \sup_{x\in\R^n}
    \inner{x^*_1+x^*_2}{x}-(f(x)+g(x))
    \\
    &=
    \sup_{x\in\R^n}
    \inner{x^*}{x}-(f+g)(x)
    =(f+g)^*(x^*)
  \end{align*}
  Taking the infimum over $x_1^*,x_2^*\in\R^n$ in the above display gives 
  $
  (f^*\square g^*)(x^*)
  \ge
  (f+g)^*(x^*).
  $
  Let us prove now $\le$ under the condition
  $
  \text{ri}\left( \text{dom}(f) \right)
  \cap
  \text{ri}\left( \text{dom}(g) \right)
  \neq 
  \emptyset
  .
  $
  The only case we need to consider is
  $
    (f+g)^*(x^*)<\infty.
  $
  Define two convex sets by
  \begin{align}
    \Omega_1
    &:=
    \left\{ 
      (x,\alpha,\beta)\in\R^{n+2}
      \colon
      \alpha\ge f(x)
    \right\}
    =
    \mathrm{epi}(f)\times \R,
    \\
    \Omega_2
    &:=
    \left\{ 
      (x,\alpha,\beta)\in\R^{n+2}
      \colon
      \beta\ge g(x)
    \right\}.
  \end{align}
  Similar to Lemma we get the representation
  \begin{gather}
    (f+g)^*(x^*)
    =
    \sigma_{\Omega_1\cap\Omega_2}
    (x^*,-1,-1).
  \end{gather}
  Indeed, the only thing we need to verify is
  $
    \mathrm{dom}(f)\cap\mathrm{dom}(g)
    =
    \mathrm{dom}(f+g).
  $
  The inclusion $\subseteq$ is clear.
  Assume towards a contradiction that
  $
    (f+g)(x)<\infty
  $
  and
  $
    f(x)=\infty.
  $
  Since $g(x)>-\infty$ it holds
  \begin{gather}
    \infty
    =
    \infty+g(x)
    =f(x)+g(x)
    =(f+g)(x)
    <
    \infty.
  \end{gather}
  This is a contradiction. The same holds for $f$ and $g$ reversed. It follows the inclusion $\supseteq$ and equality.
  By the support function intersection rule there exist triples
  \begin{gather}
    (x^*_1,-\alpha_1,-\beta_1),
    (x^*_2,-\alpha_2,-\beta_2)
    \in \R^{n+2}
    \quad
    \text{such that}
    \quad
    (x^*,-1,-1)
    =
    (x^*_1+x^*_2,-(\alpha_1+\alpha_2),-(\beta_1+\beta_2))
  \end{gather}
  and
  \begin{gather}
    (f+g)^*(x^*)
    =
    \sigma_{\Omega_1\cap\Omega_2}
    (x^*,-1,-1)
    =
    \sigma_{\Omega_1}
    (x^*_1,-\alpha_1,-\beta_1)
    +
    \sigma_{\Omega_2}
    (x^*_2,-\alpha_2,-\beta_2).
  \end{gather}
  Next we show
  $\beta_1=\alpha_2=0.$
  Suppose towards a contradiction that 
  $\beta_1\neq 0.$ 
  We fix 
  $(\overline{x},\overline{\alpha})\in\mathrm{epi}(f).$
  Then
  \begin{gather}
    \sigma_{\Omega_1}
    (x^*_1,-\alpha_1,-\beta_1)
    =
    \sup_{(x,\alpha,\beta)\in \mathrm{epi}(f)\times \R}
    \inner{x^*}{x}-\alpha \alpha_1 -\beta \beta_1
    \ge
    \sup_{\beta\in \R}
    \inner{x^*}{\overline{x}}-\overline{\alpha} \alpha_1 -\beta \beta_1
    =\infty.
  \end{gather}
  This contradicts
  $
    (f+g)^*(x^*)<\infty.
  $
  In a similar fashion we can derive a contradiction for $\alpha_2\neq0.$
  Employing Lemma and taking into account the structures of the sets 
  $\Omega_1$ and $\Omega_2$ this implies
  \begin{align}
    (f+g)^*(x^*)
    &=
    \sigma_{\Omega_1\cap\Omega_2}
    (x^*,-1,-1)
    =
    \sigma_{\Omega_1}
    (x^*_1,-1,0)
    +
    \sigma_{\Omega_2}
    (x^*_2,0,-1)
    \\
    &=
    \sigma_{\mathrm{epi}(f)}(x^*_1,-1)
    +
    \sigma_{\mathrm{epi}(g)}(x^*_2,-1)
    =
    f^*(x^*_1)
    +
    g^*(x^*_2)
    \ge
    (f^*\square g^*)(x^*).
  \end{align}
  This finishes the proof.
\end{proof}
It holds
  \begin{align*}
    - f^*(A^\top y^*) - g^*(y^*) 
    &=
      -\sup_{x\in \R^n}\inner{A^\top y^*}{x}-f(x)
      -\sup_{y\in \R^m}\inner{-y^*}{y}-g(y)
      \\
    &=
    \inf_{x\in \R^n}f(x)-\inner{y^*}{Ax}
      +\inf_{y\in \R^m}g(y)+\inner{y^*}{y}
      \\
    &\le
      \inf_{x\in \R^n}f(x)-\inner{y^*}{Ax}
      +\inf_{x\in \R^n}g(Ax)+\inner{y^*}{Ax}
      \\
    &\le
      \inf_{x\in \R^n}f(x)-\inner{y^*}{Ax} + g(Ax) + \inner{y^*}{Ax}
      \\
    &=  
      \inf_{x\in \R^n}f(x)+g(Ax)
    =
      \widehat{p}
  \end{align*}
  The first equality is due to the definition of convex conjugates, the second equality due to $\inner{A^\top y}{x}=\inner{y}{Ax}$ and $\inf \left\{ -B \right\}=-\sup \left\{ B \right\}$ for all $B \subseteq \overline{\R}$ and the first inequality due to $\mathrm{Im}(A)\subseteq \R^m$.
  Taking the supremum with respect to all 
  $y^*\in\R^m$
  yields the result.
\begin{proposition}
  Let
  $
  f,g: I\to \R
  $
  be real-valued functions on an interval $I\subseteq\R,$ 
  and let
  $\mathbf{A}\in \mathbb{H}_d$
  be a Hermitian matrix
  whose eigenvalues are contained in $I.$

  \begin{enumerate}[label={(\roman*)}]
    \item
      If $\lambda$ is an eigenvalue of of $\mathbf{A},$
      then $f(\lambda)$ is an eigenvalue of $f(\mathbf{A}).$
    \item
      $
        f(a)
        \le
        g(a)
        \quad
        \text{for all}\ 
        a\in I
        \quad
        \text{implies}
        \quad
        f(\mathbf{A})
        \preccurlyeq
        g(\mathbf{A})
        .
      $
  \end{enumerate}
\end{proposition}


\begin{proposition}
  \emph{(Hölder inequality for trace)}
  Let 
  $p$ and $q$
  be Hölder conjugate indices.
  Then
  \begin{gather}
    \mathrm{tr}
    (
    \mathbf{BC}
    )
    \le
    \norm{\mathbf{B}}_p
    \norm{\mathbf{C}}_q
    \qquad
    \text{for all}
    \ 
    \mathbf{B}
    ,
    \mathbf{C}
    \in 
    \mathbb{M}_d
    .
  \end{gather}
\end{proposition}
\begin{proof}
  \cite[Corollary~IV.2.6]{Bhatia1997}
\end{proof}
We are now ready to prove the auxiliary theorem.

\begin{theorem}
  \emph{(Matrix BDG inequality)}
  Let
  $
    p = 1
    \ 
    \text{or}\ 
    p \ge 3/2
    .
  $
  Suppose that 
  $
  (
    \mathbf{X}
   , 
   \mathbf{X}^{'}
  )
  $
  is a matrix Stein pair where
  $
   \E
   [ 
    \norm{\mathbf{X}}
    _{2p}^{2p}
   ]
   <
   \infty
   .
  $
  Then
  \begin{gather}
   \E
   [ 
    \norm{\mathbf{X}}
    _{2p}^{2p}
   ]
   ^{1/(2p)}
   \le
   \sqrt{2p - 1}
   \ 
   \E
   [ 
    \norm{\mathbf{\Delta_X}}
    _{p}^{p}
   ]
   ^{1/(2p)}
   ,
  \end{gather}
  where 
  $
    \mathbf{\Delta_X}
  $
  is the conditional variance
  .
\end{theorem}
\begin{proof}
  \emph{\cite[§7.3]{Mackey2014}}
  Suppose that
  $
  (
    \mathbf{X}
    ,
    \mathbf{X}^{'}
  )
  $
  is a matrix Stein pair with scale factor $\alpha.$
  First, observe that the result for $p=1$ already follows from 
  $
    \E[\mathbf{\Delta_X}]
    =
    \E[\mathbf{X}^2]
    .
  $
  Therefore we may assume that $p\ge 3/2.$
  We introduce the notation for the quantity of interest,
  \begin{gather}
    E
    :=
    \E[
    \norm{\mathbf{X}}_{2p}^{2p}
    ]
    =
    \E
    [
    \mathrm{tr}
    (
    \left| \mathbf{X} \right|^{2p}
    )
    ].
  \end{gather}
  We rewrite the expression for $E$
  by peeling off a copy of $\left| \mathbf{X} \right|.$
  This yields
  \begin{gather}
    E
    =
    \E
    [
    \mathrm{tr}
    (
    \left| \mathbf{X} \right|
    \cdot
    \left| \mathbf{X} \right|^{2p-1}
    )
    ]
    =
    \E
    [
    \mathrm{tr}
    (
    \mathbf{X}
    \cdot
    \mathrm{sgn}
    (
      \mathbf{X}
    )
    \cdot
    \left| \mathbf{X} \right|^{2p-1}
    )
    ]
    .
  \end{gather}
  Apply the method of exchangable pairs with 
  $
    \mathbf{F}
    (\mathbf{X})
    =
    \mathrm{sgn}
    (\mathbf{X})
    \cdot
    \left| \mathbf{X} \right|^{2p-1}
  $
  to reach
  \begin{gather}
    E
    =
    \frac{1}{2\alpha}
    \E
    [
    \mathrm{tr}
    (
    (
    \mathbf{X}
    -
    \mathbf{X}^{'}
    )
    \cdot
    (
    \mathrm{sgn}
    (
      \mathbf{X}
    )
    \cdot
    \left| \mathbf{X} \right|^{2p-1}
    -
    \mathrm{sgn}
    (
    \mathbf{X}^{'}
    )
    \cdot
    | \mathbf{X}^{'} |^{2p-1}
    )
    )
    ]
  \end{gather}



  Apply method of exchangeable pairs,
  generalized Klein inequality,
  trace Hölder
\end{proof}





\todo[color=red!40,inline]{Add outer probability calculus. \cite{vaart2013} p.6}

Let 
$
(
\mathbb{D}
,
d
)
$
be a metric space, and let 
$
(\P_n)_{n\in \mathbb{N}}
  \P
$
be (Borel) probability measures
on
$
(
\mathbb{D}
,
\mathcal{D}
)
,
$
where 
$\mathcal{D}$
is the Borel $\sigma$-algebra on $\mathbb{D},$
the smallest $\sigma$-algebra
containing all open sets.
Then the sequence 
$\P_n$
\textbf{converges weakly}
to 
$\P,$
which we denote as $\P_n \rightsquigarrow \P,$
if and only if 
\begin{gather}
  \int_\mathbb{D}
  f
  \text{d}
  \P_n
  \to
  \int_\mathbb{D}
  f
  \text{d}
  \P
  \qquad
  \text{for all}
  \ 
  f
  \in
  C_b(\mathbb{D})
  .
\end{gather}
Here 
$
  C_b(\mathbb{D})
$
denotes the set of all bounded, continuous, real functions on $\mathbb{D}.$
Equivalently, if 
$X_n$ and $X$
are 
$\mathbb{D}$-valued
random variables with distribution 
$\P_n$ and $\P$
respectively, then 
$X_n \to X$
if and only if 
\begin{gather}
  \E[f(X_n)]
  \to
  \E[f(X)]
  \qquad
  \text{for all}
  \ 
  f
  \in
  C_b(\mathbb{D})
  .
\end{gather}

This definitions yield the classical theory of weak convergence.
For a modern treatment see \cite{Klenke2020}.

The classical theory requires that 
$\P_n$
is defined, for each $n\in \mathbb{N},$
on the Borel $\sigma$-algebra $\mathcal{D},$
or, equivalently, that $X_n$ is a Borel measurable map for each $n\in \mathbb{N}.$
If 
$
(
\Omega_n,
\mathcal{A}_n,
\P_n
)
$
are the underlying probability spaces on which the maps 
$X_n$
are defined, this means that
$X_n^{-1}(D)\in \mathcal{A}_n$
for every Borel set $D \in \mathcal{D}.$
This required measurability usually holds when $\mathbb{D}$
is a separable metric space such as $\R^k$
or $C([0,1])$ with the supremum metric.

However, this apparently modest requirement can and does easily fail when the metric space $\mathbb{D}$ is not separable.

\begin{example}
  \emph{\cite[Problem~1.7.3]{vaart2013}}
  Let $\mathbb{D}=D([0,1])$ be the \textbf{Skorohod space} of all right-continuous functions on $[0,1]$
  with left limits endowed with the metric induced by the supremum norm.
  Define 
  $
    X:
    [0,1]
    \to
    \mathbb{D}
    ,\ 
    \omega
    \mapsto
    \mathbf{1}_{[\omega,1]}
    .
  $
  If we equip $[0,1]$ with the Borel $\sigma$-algebra 
  $\mathcal{B}([0,1])$, then 
  $X$ is not measurable. To see this, let $B_s$ be the open ball of radius $1/2$ in $\mathbb{D}$ around the function $\mathbf{1}_{[s,1]}.$
  Now $X(\omega)\in B_s$
  if and only if $\omega=s.$ Indeed, if $\omega\neq s$ there exists an $x$ between $\omega$ and $s$ such that the difference of the indicator functions is 1 at $x$. Conversely, if the distance is greater than
  $1/2$ at a point $x\in [0,1]$, it is because $x$ lies between $\omega$ and $s$ and the indicator functions have difference 1.
  Since arbitrary (even uncountable) unions of open sets are open,
  we get for every $S\subseteq [0,1]$ the open set 
  $
  G
  :=
  \bigcup_{s\in S}
    B_s
    \in \mathcal{D}
    .
  $
  It follows
  $
  X^{-1}(G)=S 
  \ 
  \text{for all}
  \ 
  S \subseteq [0,1]
  .
  $
  Since not all subsets of $[0,1]$ are measurable, we have
  $
  X^{-1}(\mathcal{D})\nsubseteq \mathcal{B}([0,1])
.
$
But then $X$ is not measurable. The $\sigma$-algebra $\mathcal{D}$ is to large.
\end{example}
