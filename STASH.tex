\begin{example}
  Consider a proper convex function
  $
    f:
    \R \to ( -\infty, \infty]
  $
  We would like to compute the convex conjugate of the function 
  $h$ defined by 
  $
    h(x) = f \left( x - \frac{1}{n} \right)
  $.
  For this we notice that
  \begin{gather*}
    h = f \circ L
  \end{gather*}
  where 
  \begin{gather*}
   L := x \mapsto x - \frac{1}{n} 
  \end{gather*}
  is a linear map.
  Since 
  $
    \text{Im}(L) = \R
  $
  and 
  $\text{dom}(f) \neq \emptyset$
  we have
  $
    \text{Im}(L) 
    \cap
    \text{ri}(\text{dom}(f) )
    \neq
    \emptyset
  $.
  Furthermore
  \begin{gather}
    (L^*)^{-1}(x^*)
    =
    \left\{ 
     \left( x \mapsto x + \frac{1}{n} \right) (x^*)
    \right\}
    =
    \left\{ x^* + \frac{1}{n} \right\}
  \end{gather}
  Then Theorem~\ref{cvxa_conjugate_chain_rule}
  yields
  \begin{gather}
    h^* = f^* \circ L^{-1} 
        =  f^* \circ  
        \left( x^* \mapsto x^* + \frac{1}{n} \right)
  \end{gather}

\end{example}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
We employ 
Theorem~\ref{cvxa_fenchel_theorem}
together with the box constraints in 
Problem~\eqref{primal_weighting_binary}
to obtain Proposition~\ref{ch_1_dual}.

To prove Proposition~\ref{ch_1_near_oracle}
we employ
Proposition~\ref{syu_1_result}
and 
Corollary~\ref{syu_taylor_corollary}
to get
\begin{align}
  \begin{split}
    & 
    G(\lambda^*_1 + \Delta) 
    -
    G(\lambda^*_1)
\\
    &\ge
    \frac{1}{n}
    \sum_{j = 1}^{n} 
      \left[ 
        -T_j n 
        \rho^{'} 
        \left( 
          B(X_j)^T \lambda^*_1
        \right)
        +
        1
      \right]
      \Delta^T B(X_j)
\\
    & +
    \frac{1}{2}
    \sum_{j = 1}^{n} 
      -T_j  
      \rho^{''} 
      \left( 
        B(X_j)^T 
        (
          \lambda^*_1 + \xi \Delta
        )
      \right)
      \Delta^T
      \left( 
        B(X_j)
        B(X_j)^T
      \right)
      \Delta
\\
    &-
    |\Delta|^T \delta
\\
    &\ge
    - \norm{\Delta}_2
    \left( 
      \norm{
        \frac{1}{n}
        \sum_{j = 1}^{n} 
          \left[ 
            -T_j n 
            \rho^{'} 
            \left( 
              B(X_j)^T \lambda^*_1
            \right)
            +
            1
          \right]
        B(X_j)
      }_2
      +
      \norm{\delta}_2
    \right)
\\
    &+
    n
    \norm{\Delta}^2_2
    \varphi_{\rho^{''}}
    \underline{\varphi_{aa^T}}
  \end{split}
\end{align}

Next we employ Bernstein inequality~\ref{rmineq_bernstein} to bound
\begin{align}
    \norm{
      \frac{1}{n}
      \sum_{j = 1}^{n} 
      \left[ 
        -T_j n 
        \rho^{'} 
        \left( 
          B(X_j)^T \lambda^*_1
        \right)
      +
      1
      \right]
      B(X_j)
    }_2
    \le
    \CP \Ctau \LearnRate
\end{align}
with probability $1 - \tau$.
Then for 
$\norm{\Delta}_2$ large enough it holds
\begin{gather}
  G(\lambda^*_1 + \Delta) 
  -
  G(\lambda^*_1)
  >
  0
\end{gather}
with probability $1 - \tau$.
Thus by Proposition~\ref{syu_1_result}
  \begin{gather}
    \P
    \left( 
      \norm{
        \lambda^\dagger
        -
        \lambda^*_1
      }_2
      \le
      \norm{\Delta}_2
    \right)
    \ge 
    1 - \tau
    .
  \end{gather}
%Hartman-Winter Theorem
\begin{theorem*}
  Let 
  $
    X_1,
    X_2,
    \ldots
  $
  be i.i.d. real random variables with 
  $
    \E[X_1]=0
    \ 
    \text{and}
    \ 
    \mathbf{Var}
    [X_1]
    = 1.
  $
  Let
  $
    S_n
    :=
    X_1
    +
    \ldots
    +
    X_n,
    \ 
    n\in \mathbb{N}
    .
  $
  Then
  \begin{gather}
    \limsup_{n\to\infty}
    \,
    \frac{S_n}{
      \sqrt{
        \,
        2n
        \log
        \log
        n
      }
    }
    \ 
    =
    \ 
    1
    \qquad
    \text{a.s.}
  \end{gather}
\end{theorem*}
\begin{lemma}
  For any proper function
  $
    f:\R^n\to\overline{\R}
  $
  we have
  \begin{gather}
    f^*(x^*) 
    =
    \sigma_{\mathrm{epi}(f)}
    (x^*,-1)
    \qquad
    \text{for}
    \ 
    x^* \in \R^n.
  \end{gather}
\end{lemma}
\begin{proof}
  Let $x^*\in\R^n$
  and
  $
    (x,\lambda)\in \mathrm{epi}(f).
  $
  Then
  $
    x \in \mathrm{dom}(f)
  $
  and
  $
    f(x)\le \lambda.
  $
  Thus
  \begin{gather}
    \inner{x^*}{x} - f(x)
    \ge
    \inner{x^*}{x} - \lambda
    \qquad
    \text{for all}\ 
    (x,\lambda)\in \mathrm{epi}(f).
  \end{gather}
  On the other hand 
  $
    (x,f(x))\in \mathrm{epi}(f)
  $
  for all
  $
    x \in \mathrm{dom}(f).
  $
  It follows
  \begin{gather}
    \inner{x^*}{x} - f(x)
    \le
    \sup_{(x,\lambda)\in\mathrm{epi}(f)}
    \inner{x^*}{x} - \lambda
    \qquad
    \text{for all}\ 
    x \in \mathrm{dom}(f).
  \end{gather}
  Taking the supremum in the last two displays yields
  \begin{align}
    f^*(x^*)
    =
    \sup_{x\in\mathrm{dom}(f)}
    \inner{x^*}{x} - f(x)
    &=
    \sup_{(x,\lambda)\in\mathrm{epi}(f)}
    \inner{x^*}{x} - \lambda
    \\
    &=
    \sup_{(x,\lambda)\in\mathrm{epi}(f)}
    \inner{(x^*,-1)}{(x,\lambda)} 
    =
    \sigma_{\mathrm{epi}(f)}
    (x^*,-1).
  \end{align}
\end{proof}
\begin{proof}
  Let $x^*\in\R^n$ and fix $x_1^*,x_2^*\in\R^n$ such that
  $x^*=x^*_1+x^*_2$.
  We get
  \begin{align*}
    f^*(x^*_1)+g^*(x^*_2)
    &=
    \sup_{x\in\R^n}
    \inner{x^*_1}{x}-f(x)
    +
    \sup_{x\in\R^n}
    \inner{x^*_2}{x}-g(x)
    \\
    &\ge
    \sup_{x\in\R^n}
    \inner{x^*_1}{x}-f(x)
    +
    \inner{x^*_2}{x}-g(x)
    =
    \sup_{x\in\R^n}
    \inner{x^*_1+x^*_2}{x}-(f(x)+g(x))
    \\
    &=
    \sup_{x\in\R^n}
    \inner{x^*}{x}-(f+g)(x)
    =(f+g)^*(x^*)
  \end{align*}
  Taking the infimum over $x_1^*,x_2^*\in\R^n$ in the above display gives 
  $
  (f^*\square g^*)(x^*)
  \ge
  (f+g)^*(x^*).
  $
  Let us prove now $\le$ under the condition
  $
  \text{ri}\left( \text{dom}(f) \right)
  \cap
  \text{ri}\left( \text{dom}(g) \right)
  \neq 
  \emptyset
  .
  $
  The only case we need to consider is
  $
    (f+g)^*(x^*)<\infty.
  $
  Define two convex sets by
  \begin{align}
    \Omega_1
    &:=
    \left\{ 
      (x,\alpha,\beta)\in\R^{n+2}
      \colon
      \alpha\ge f(x)
    \right\}
    =
    \mathrm{epi}(f)\times \R,
    \\
    \Omega_2
    &:=
    \left\{ 
      (x,\alpha,\beta)\in\R^{n+2}
      \colon
      \beta\ge g(x)
    \right\}.
  \end{align}
  Similar to Lemma we get the representation
  \begin{gather}
    (f+g)^*(x^*)
    =
    \sigma_{\Omega_1\cap\Omega_2}
    (x^*,-1,-1).
  \end{gather}
  Indeed, the only thing we need to verify is
  $
    \mathrm{dom}(f)\cap\mathrm{dom}(g)
    =
    \mathrm{dom}(f+g).
  $
  The inclusion $\subseteq$ is clear.
  Assume towards a contradiction that
  $
    (f+g)(x)<\infty
  $
  and
  $
    f(x)=\infty.
  $
  Since $g(x)>-\infty$ it holds
  \begin{gather}
    \infty
    =
    \infty+g(x)
    =f(x)+g(x)
    =(f+g)(x)
    <
    \infty.
  \end{gather}
  This is a contradiction. The same holds for $f$ and $g$ reversed. It follows the inclusion $\supseteq$ and equality.
  By the support function intersection rule there exist triples
  \begin{gather}
    (x^*_1,-\alpha_1,-\beta_1),
    (x^*_2,-\alpha_2,-\beta_2)
    \in \R^{n+2}
    \quad
    \text{such that}
    \quad
    (x^*,-1,-1)
    =
    (x^*_1+x^*_2,-(\alpha_1+\alpha_2),-(\beta_1+\beta_2))
  \end{gather}
  and
  \begin{gather}
    (f+g)^*(x^*)
    =
    \sigma_{\Omega_1\cap\Omega_2}
    (x^*,-1,-1)
    =
    \sigma_{\Omega_1}
    (x^*_1,-\alpha_1,-\beta_1)
    +
    \sigma_{\Omega_2}
    (x^*_2,-\alpha_2,-\beta_2).
  \end{gather}
  Next we show
  $\beta_1=\alpha_2=0.$
  Suppose towards a contradiction that 
  $\beta_1\neq 0.$ 
  We fix 
  $(\overline{x},\overline{\alpha})\in\mathrm{epi}(f).$
  Then
  \begin{gather}
    \sigma_{\Omega_1}
    (x^*_1,-\alpha_1,-\beta_1)
    =
    \sup_{(x,\alpha,\beta)\in \mathrm{epi}(f)\times \R}
    \inner{x^*}{x}-\alpha \alpha_1 -\beta \beta_1
    \ge
    \sup_{\beta\in \R}
    \inner{x^*}{\overline{x}}-\overline{\alpha} \alpha_1 -\beta \beta_1
    =\infty.
  \end{gather}
  This contradicts
  $
    (f+g)^*(x^*)<\infty.
  $
  In a similar fashion we can derive a contradiction for $\alpha_2\neq0.$
  Employing Lemma and taking into account the structures of the sets 
  $\Omega_1$ and $\Omega_2$ this implies
  \begin{align}
    (f+g)^*(x^*)
    &=
    \sigma_{\Omega_1\cap\Omega_2}
    (x^*,-1,-1)
    =
    \sigma_{\Omega_1}
    (x^*_1,-1,0)
    +
    \sigma_{\Omega_2}
    (x^*_2,0,-1)
    \\
    &=
    \sigma_{\mathrm{epi}(f)}(x^*_1,-1)
    +
    \sigma_{\mathrm{epi}(g)}(x^*_2,-1)
    =
    f^*(x^*_1)
    +
    g^*(x^*_2)
    \ge
    (f^*\square g^*)(x^*).
  \end{align}
  This finishes the proof.
\end{proof}
It holds
  \begin{align*}
    - f^*(A^\top y^*) - g^*(y^*) 
    &=
      -\sup_{x\in \R^n}\inner{A^\top y^*}{x}-f(x)
      -\sup_{y\in \R^m}\inner{-y^*}{y}-g(y)
      \\
    &=
    \inf_{x\in \R^n}f(x)-\inner{y^*}{Ax}
      +\inf_{y\in \R^m}g(y)+\inner{y^*}{y}
      \\
    &\le
      \inf_{x\in \R^n}f(x)-\inner{y^*}{Ax}
      +\inf_{x\in \R^n}g(Ax)+\inner{y^*}{Ax}
      \\
    &\le
      \inf_{x\in \R^n}f(x)-\inner{y^*}{Ax} + g(Ax) + \inner{y^*}{Ax}
      \\
    &=  
      \inf_{x\in \R^n}f(x)+g(Ax)
    =
      \widehat{p}
  \end{align*}
  The first equality is due to the definition of convex conjugates, the second equality due to $\inner{A^\top y}{x}=\inner{y}{Ax}$ and $\inf \left\{ -B \right\}=-\sup \left\{ B \right\}$ for all $B \subseteq \overline{\R}$ and the first inequality due to $\mathrm{Im}(A)\subseteq \R^m$.
  Taking the supremum with respect to all 
  $y^*\in\R^m$
  yields the result.
\begin{proposition}
  Let
  $
  f,g: I\to \R
  $
  be real-valued functions on an interval $I\subseteq\R,$ 
  and let
  $\mathbf{A}\in \mathbb{H}_d$
  be a Hermitian matrix
  whose eigenvalues are contained in $I.$

  \begin{enumerate}[label={(\roman*)}]
    \item
      If $\lambda$ is an eigenvalue of of $\mathbf{A},$
      then $f(\lambda)$ is an eigenvalue of $f(\mathbf{A}).$
    \item
      $
        f(a)
        \le
        g(a)
        \quad
        \text{for all}\ 
        a\in I
        \quad
        \text{implies}
        \quad
        f(\mathbf{A})
        \preccurlyeq
        g(\mathbf{A})
        .
      $
  \end{enumerate}
\end{proposition}


\begin{proposition}
  \emph{(Hölder inequality for trace)}
  Let 
  $p$ and $q$
  be Hölder conjugate indices.
  Then
  \begin{gather}
    \mathrm{tr}
    (
    \mathbf{BC}
    )
    \le
    \norm{\mathbf{B}}_p
    \norm{\mathbf{C}}_q
    \qquad
    \text{for all}
    \ 
    \mathbf{B}
    ,
    \mathbf{C}
    \in 
    \mathbb{M}_d
    .
  \end{gather}
\end{proposition}
\begin{proof}
  \cite[Corollary~IV.2.6]{Bhatia1997}
\end{proof}
We are now ready to prove the auxiliary theorem.

\begin{theorem}
  \emph{(Matrix BDG inequality)}
  Let
  $
    p = 1
    \ 
    \text{or}\ 
    p \ge 3/2
    .
  $
  Suppose that 
  $
  (
    \mathbf{X}
   , 
   \mathbf{X}^{'}
  )
  $
  is a matrix Stein pair where
  $
   \E
   [ 
    \norm{\mathbf{X}}
    _{2p}^{2p}
   ]
   <
   \infty
   .
  $
  Then
  \begin{gather}
   \E
   [ 
    \norm{\mathbf{X}}
    _{2p}^{2p}
   ]
   ^{1/(2p)}
   \le
   \sqrt{2p - 1}
   \ 
   \E
   [ 
    \norm{\mathbf{\Delta_X}}
    _{p}^{p}
   ]
   ^{1/(2p)}
   ,
  \end{gather}
  where 
  $
    \mathbf{\Delta_X}
  $
  is the conditional variance
  .
\end{theorem}
\begin{proof}
  \emph{\cite[§7.3]{Mackey2014}}
  Suppose that
  $
  (
    \mathbf{X}
    ,
    \mathbf{X}^{'}
  )
  $
  is a matrix Stein pair with scale factor $\alpha.$
  First, observe that the result for $p=1$ already follows from 
  $
    \E[\mathbf{\Delta_X}]
    =
    \E[\mathbf{X}^2]
    .
  $
  Therefore we may assume that $p\ge 3/2.$
  We introduce the notation for the quantity of interest,
  \begin{gather}
    E
    :=
    \E[
    \norm{\mathbf{X}}_{2p}^{2p}
    ]
    =
    \E
    [
    \mathrm{tr}
    (
    \left| \mathbf{X} \right|^{2p}
    )
    ].
  \end{gather}
  We rewrite the expression for $E$
  by peeling off a copy of $\left| \mathbf{X} \right|.$
  This yields
  \begin{gather}
    E
    =
    \E
    [
    \mathrm{tr}
    (
    \left| \mathbf{X} \right|
    \cdot
    \left| \mathbf{X} \right|^{2p-1}
    )
    ]
    =
    \E
    [
    \mathrm{tr}
    (
    \mathbf{X}
    \cdot
    \mathrm{sgn}
    (
      \mathbf{X}
    )
    \cdot
    \left| \mathbf{X} \right|^{2p-1}
    )
    ]
    .
  \end{gather}
  Apply the method of exchangable pairs with 
  $
    \mathbf{F}
    (\mathbf{X})
    =
    \mathrm{sgn}
    (\mathbf{X})
    \cdot
    \left| \mathbf{X} \right|^{2p-1}
  $
  to reach
  \begin{gather}
    E
    =
    \frac{1}{2\alpha}
    \E
    [
    \mathrm{tr}
    (
    (
    \mathbf{X}
    -
    \mathbf{X}^{'}
    )
    \cdot
    (
    \mathrm{sgn}
    (
      \mathbf{X}
    )
    \cdot
    \left| \mathbf{X} \right|^{2p-1}
    -
    \mathrm{sgn}
    (
    \mathbf{X}^{'}
    )
    \cdot
    | \mathbf{X}^{'} |^{2p-1}
    )
    )
    ]
  \end{gather}



  Apply method of exchangeable pairs,
  generalized Klein inequality,
  trace Hölder
\end{proof}





\todo[color=red!40,inline]{Add outer probability calculus. \cite{vaart2013} p.6}

Let 
$
(
\mathbb{D}
,
d
)
$
be a metric space, and let 
$
(\P_n)_{n\in \mathbb{N}}
  \P
$
be (Borel) probability measures
on
$
(
\mathbb{D}
,
\mathcal{D}
)
,
$
where 
$\mathcal{D}$
is the Borel $\sigma$-algebra on $\mathbb{D},$
the smallest $\sigma$-algebra
containing all open sets.
Then the sequence 
$\P_n$
\textbf{converges weakly}
to 
$\P,$
which we denote as $\P_n \rightsquigarrow \P,$
if and only if 
\begin{gather}
  \int_\mathbb{D}
  f
  \text{d}
  \P_n
  \to
  \int_\mathbb{D}
  f
  \text{d}
  \P
  \qquad
  \text{for all}
  \ 
  f
  \in
  C_b(\mathbb{D})
  .
\end{gather}
Here 
$
  C_b(\mathbb{D})
$
denotes the set of all bounded, continuous, real functions on $\mathbb{D}.$
Equivalently, if 
$X_n$ and $X$
are 
$\mathbb{D}$-valued
random variables with distribution 
$\P_n$ and $\P$
respectively, then 
$X_n \to X$
if and only if 
\begin{gather}
  \E[f(X_n)]
  \to
  \E[f(X)]
  \qquad
  \text{for all}
  \ 
  f
  \in
  C_b(\mathbb{D})
  .
\end{gather}

This definitions yield the classical theory of weak convergence.
For a modern treatment see \cite{Klenke2020}.

The classical theory requires that 
$\P_n$
is defined, for each $n\in \mathbb{N},$
on the Borel $\sigma$-algebra $\mathcal{D},$
or, equivalently, that $X_n$ is a Borel measurable map for each $n\in \mathbb{N}.$
If 
$
(
\Omega_n,
\mathcal{A}_n,
\P_n
)
$
are the underlying probability spaces on which the maps 
$X_n$
are defined, this means that
$X_n^{-1}(D)\in \mathcal{A}_n$
for every Borel set $D \in \mathcal{D}.$
This required measurability usually holds when $\mathbb{D}$
is a separable metric space such as $\R^k$
or $C([0,1])$ with the supremum metric.

However, this apparently modest requirement can and does easily fail when the metric space $\mathbb{D}$ is not separable.

\begin{example}
  \emph{\cite[Problem~1.7.3]{vaart2013}}
  Let $\mathbb{D}=D([0,1])$ be the \textbf{Skorohod space} of all right-continuous functions on $[0,1]$
  with left limits endowed with the metric induced by the supremum norm.
  Define 
  $
    X:
    [0,1]
    \to
    \mathbb{D}
    ,\ 
    \omega
    \mapsto
    \mathbf{1}_{[\omega,1]}
    .
  $
  If we equip $[0,1]$ with the Borel $\sigma$-algebra 
  $\mathcal{B}([0,1])$, then 
  $X$ is not measurable. To see this, let $B_s$ be the open ball of radius $1/2$ in $\mathbb{D}$ around the function $\mathbf{1}_{[s,1]}.$
  Now $X(\omega)\in B_s$
  if and only if $\omega=s.$ Indeed, if $\omega\neq s$ there exists an $x$ between $\omega$ and $s$ such that the difference of the indicator functions is 1 at $x$. Conversely, if the distance is greater than
  $1/2$ at a point $x\in [0,1]$, it is because $x$ lies between $\omega$ and $s$ and the indicator functions have difference 1.
  Since arbitrary (even uncountable) unions of open sets are open,
  we get for every $S\subseteq [0,1]$ the open set 
  $
  G
  :=
  \bigcup_{s\in S}
    B_s
    \in \mathcal{D}
    .
  $
  It follows
  $
  X^{-1}(G)=S 
  \ 
  \text{for all}
  \ 
  S \subseteq [0,1]
  .
  $
  Since not all subsets of $[0,1]$ are measurable, we have
  $
  X^{-1}(\mathcal{D})\nsubseteq \mathcal{B}([0,1])
.
$
But then $X$ is not measurable. The $\sigma$-algebra $\mathcal{D}$ is to large.
\end{example}
%
%et 
%  $(\varepsilon_N)\subset(0,1]$ be a decreasing sequence with 
%  $\varepsilon_N\to 0$ as $N\to\infty$ and define for
%  $N\in\mathbb{N}$ and $z\in\R$ the function
% \begin{align*}
%    f_{\varepsilon_N,\pm}^z
%    \ 
%    :
%    \ 
%    &
%      \left\{ 0,1 \right\}
%      \times
%      \mathcal{X}
%      \times
%      \mathcal{Y}
%    \ 
%    \to
%    \ 
%    \R
%    \\
%    &
%      (T,X,Y(T))
%      \ 
%      \mapsto
%      \ 
%      \mathbf{1}
%      _
%      {\left\{ 
%          \left|
%      w(X)-\frac{1}{\pi(X)}
%          \right|
%          \,
%          \le
%          \,
%          \varepsilon_N/2
%      \right\}}
%      \left( 
%      w(X)-\frac{1}{\pi(X)}
%      \right)
%      ^{\pm}
%      T
%      \left( 
%        \mathbf{1}
%        _{\left\{  Y(T)\,\le\,z \right\}}
%        -
%        F_{Y(1)}(z|X)
%      \right)
%      \,,
%  \end{align*}
%  and define 
%  \begin{gather*}
%    \mathcal{F}_N^\pm
%    :=
%    \left\{ 
%    f_{\varepsilon_N,\pm}^z
%    |
%    z\in\R
%    \right\}
%    \,.
%  \end{gather*}
%\begin{lemma}
%  Consider the weights function
%  \begin{align*}
%    w
%    \colon
%    \mathcal{X}
%    \times
%    \R^{N+1}
%    \ 
%    \to
%    \ 
%    \R
%    \,,
%    \quad
%    (X,(\lambda,\lambda_0))
%    \ 
%    \mapsto
%    \ 
%    (
%    f^{'}
%    )^{-1}
%    \left( 
%      \inner{B(X)}{\lambda}
%      +
%      \lambda_0
%    \right)
%    \,.
%  \end{align*}
%    If the optimal solution 
%    $
%    (\lambda^\dagger,\lambda_0^\dagger)
%    $ to Problem? exists and is measurable for all $N\in\mathbb{N}$ 
%    then $
%    w
%    \circ
%    (
%    X
%    ,
%    (\lambda^\dagger,\lambda_0^\dagger)
%    )
%    $
%    is measurable.
%\end{lemma}
%\begin{lemma}
%  \label{bounded_f_lemma}
%    If the optimal solution $(\lambda^\dagger,\lambda_0^\dagger)$ to Problem? exists and is measurable for all $N\in\mathbb{N}$ 
%  the function class
%  $
%    \mathcal{F}_N^\pm
%  $
%    is measurable for all $N\in\mathbb{N}$.
%    Furthermore, $\mathcal{F}_N^\pm$ is bounded above by $\varepsilon_N$ and
%    \begin{gather*}
%     J_{[\, ]}
%    \left( 
%      \varepsilon_N
%    ,
%    \mathcal{F}_N^\pm
%    ,
%     L^2(\P)
%    \right)
%    \ 
%    \to
%    \ 
%    0
%    \qquad
%    \text{for}
%    \ 
%    N
%    \to
%    \infty
%\,. 
%    \end{gather*}
%\end{lemma}
%\begin{proof}
%  If the optimal solution $(\lambda^\dagger,\lambda_0^\dagger)$ to Problem? exists and is measurable for all $N\in\mathbb{N}$ 
%  then the weights function $w$ is measurable. Indeed,
%  this follows from
%  \begin{gather*}
%    w(X)=
%    (
%    f^{'}
%    )
%    ^{-1}
%    \left( \inner{B(X)}{\lambda^\dagger}+\lambda^\dagger_0 \right)
%    \,,
%  \end{gather*}
%  the measurability of the basis functions $B$ and the continuity of $
%    (
%    f^{'}
%    )
%    ^{-1}
%    $. Thus $f_{\varepsilon_N,\pm}^z$ are measurable for all $N\in\mathbb{N}$ and all $z\in\R$. Clearly, 
%    $\mathcal{F}_N^\pm$ is bounded above by $\varepsilon_N$.
%    Since 
%    \begin{gather*}
%       \mathbf{1}
%      _
%      {\left\{ 
%          \left|
%      w(X)-\frac{1}{\pi(X)}
%          \right|
%          \,
%          \le
%          \,
%          \varepsilon_N/2
%      \right\}}
%      \left( 
%      w(X)-\frac{1}{\pi(X)}
%      \right)
%      ^{\pm}
%    \end{gather*}
%    is non-negative and bounded above by $\varepsilon_N$
%    it follows from Lemma~\ref
%    {lem:brack_n}
%  \begin{gather*}
%    N_{[\,]}
%    (
%    \varepsilon
%    ,
%    \mathcal{F}_N^\pm, L^2(\P))
%    \ 
%    \lesssim
%    \ 
%    \left( 
%    \frac
%    {\varepsilon_N}
%    {\varepsilon}
%    \right)^2
%    \ 
%    \lesssim
%    \ 
%    \left( 
%    \frac
%    {1}
%    {\varepsilon}
%    \right)^2
%    \qquad
%    \text{for all}
%    \ 
%    \varepsilon>0
%    \,.
%  \end{gather*}
%Thus
%    \begin{align*}
%     J_{[\, ]}
%    \left( 
%      \varepsilon_N
%    ,
%    \mathcal{F}_N^\pm
%    ,
%     L^2(\P)
%    \right)
%    &
%    \ 
%=
%\  
%\int^{\varepsilon_N}_0
%\sqrt{
%\log
%    N_{[\,]}
%    (
%    \varepsilon
%    ,
%    \mathcal{F}_N^\pm, L^2(\P))
%}
%\, 
%d\varepsilon
%\\
%&
%\ 
%\lesssim
%\ 
%\int^{\varepsilon_N}_0
%\log
%\left( 
%  \frac{1}{\varepsilon}
%\right)
%\, 
%d\varepsilon
%\ 
%=
%\ 
%\varepsilon_N
%-
%\varepsilon_N \log(\varepsilon_N)
%\ 
%\to
%\ 
%0 
%\qquad
%\text{for}\ 
%N\to\infty
%\,.
%    \end{align*}
%\end{proof}
%In the next lemma we get rid of the $\varepsilon_N$ bound
%\begin{lemma}
%
%  \label{lemma_fpm}
% Consider for $z\in\R$ the function
% \begin{align*}
%    f_{\pm}^z
%    \ 
%    :
%    \ 
%      \left\{ 0,1 \right\}
%      \times
%      \mathcal{X}
%      \times
%      \mathcal{Y}
%    &
%    \ 
%    \to
%    \ 
%    \R
%    \\
%      (T,X,Y(T))
%    &
%      \ 
%      \mapsto
%      \ 
%      \left( 
%      w(X)-\frac{1}{\pi(X)}
%      \right)
%      ^{\pm}
%      T
%      \left( 
%        \mathbf{1}
%        _{\left\{  Y(T)\,\le\,z \right\}}
%        -
%        F_{Y(1)}(z|X)
%      \right)
%      \,.
%  \end{align*}
%  It holds
%  $\sup_{z\in\R}\left| \G_N f^z_\pm \right|\overset{\P}{\to}0$.
%  Furthermore, for
% \begin{align*}
%    f_z
%    \ 
%    :
%    \ 
%      \left\{ 0,1 \right\}
%      \times
%      \mathcal{X}
%      \times
%      \mathcal{Y}
%    &
%    \ 
%    \to
%    \ 
%    \R
%    \\
%      (T,X,Y(T))
%    &
%      \ 
%      \mapsto
%      \ 
%      \left( 
%      w(X)-\frac{1}{\pi(X)}
%      \right)
%      T
%      \left( 
%        \mathbf{1}
%        _{\left\{  Y(T)\,\le\,z \right\}}
%        -
%        F_{Y(1)}(z|X)
%      \right)
%      \,.
%  \end{align*}
%  it holds
%  $\sup_{z\in\R}\left| \G_N f_z \right|\overset{\P}{\to}0$.
%\end{lemma}
%\begin{proof}
%  It holds for all $z\in\R$
%  $
%  $
%  \begin{gather*}
%  f^z_\pm
%      (T,X,Y(T))
%      \ 
%      \lesssim
%      \ 
%     \left(
%      w(X)- \frac{1}{\pi(X)}
%      \right)
%      ^\pm
%      \,.
%  \end{gather*}
%  By Theorem~\ref{aa:weights:th}
%  there exists a decreasing sequence 
%  $(\varepsilon_N)\subset(0,1]$
%  with $\varepsilon_N\to 0$
%  and
%  $
%  \P[
%\left| 
%      w(X)- 1/\pi(X)
%\right|
%\le
%\varepsilon_N/2
%  ]
%  \to 1
%  $
%  for $N\to\infty$.
%  Therefore
%  \begin{gather*}
%  \P
%  \left[
%  f^z_\pm
%  \in
%  \mathcal{F}_N^\pm
%  \
%  \forall
%  z\in\R
%  \right]
%    =
%  \P[
%\left| 
%      w(X)- 1/\pi(X)
%\right|
%\le
%\varepsilon_N/2
%  ]
%  \to 1
%  \,.
%  \end{gather*}
%Then, for all $\varepsilon>0$ it holds
%\begin{align*}
%  \P
%  \left[
%  \sup_{z\in\R}
%  \left| 
%  \G_N
%  f^z_\pm
%  \right|
%  \le \varepsilon
%  \right]
%  &
%  \ 
%  \ge
%  \ 
%  \P
%  \left[
%  f^z_\pm
%  \in
%  \mathcal{F}_N^\pm
%  \
%  \forall
%  z\in\R
%  \,,
%  \ 
%  \norm{\G_N}^*_{\mathcal{F}_N^\pm}
%  \le \varepsilon
%  \right]
%  \\
%  &
%  \ 
%  \ge
%  \ 
%  \P
%  \left[
%  f^z_\pm
%  \in
%  \mathcal{F}_N^\pm
%  \
%  \forall
%  z\in\R
%  \right]
%  \ 
%  -
%  \ 
%  \P
%  \left[
%  \norm{\G_N}^*_{\mathcal{F}_N^\pm}
%  \le \varepsilon
%  \right]
%  \\
%  &
%  \ 
%  \to
%  \ 
%  1
%  \,.
%\end{align*}
%The convergence of the last term to 0 is due to 
%Lemma~\ref{bounded_f_lemma} and Lemma~\ref{markov_max_lemma}.
%Thus
%  $\sup_{z\in\R}\left| \G_N f^z_\pm \right|\overset{\P}{\to}0$.
%  Note, that 
%  \begin{gather*}
%\G_N f_z 
%=
%\G_N f^z_+
%-
%\G_N f^z_-
%\qquad
%\text{for all}\ 
%z\in\R
%\,.
%  \end{gather*}
%  Thus by Slutzky's Theorem\cite[Theorem~13.18]{Klenke2020}
%  it holds
%  $\sup_{z\in\R}\left| \G_N f_z \right|\overset{\P}{\to}0$.
%\end{proof}
%%\begin{lemma}
%%  Define for
%%  $z\in\R$
%% \begin{align*}
%%    f_z
%%    \ 
%%    :
%%    \ 
%%      \left\{ 0,1 \right\}
%%      \times
%%      \mathcal{X}
%%      \times
%%      \mathcal{Y}
%%&
%%    \ 
%%    \to
%%    \ 
%%      \R
%%    \\
%%    \left(
%%      T,X,Y(T)
%%    \right)
%%    &
%%      \ 
%%      \mapsto
%%      \ 
%%      \frac{T}{\pi(X)}
%%      \left( 
%%        \mathbf{1}
%%        _{\left\{  Y(T)\,\le\,z \right\}}
%%        -
%%        F_{Y(1)}(z|X)
%%      \right)
%%      \ 
%%      +
%%      \ 
%%      \left( 
%%        F_{Y(1)}(z|X)
%%        -
%%        F_{Y(1)}(z)
%%      \right)
%%      \,,
%%  \end{align*}
%%and consider the function class
%%$
%%  \mathcal{G}
%%  \ 
%%  :=
%%  \ 
%%  \left\{ 
%%    f_z
%%  \ 
%%    |
%%  \ 
%%    z\in\R
%%  \right\}
%%$
%%\begin{gather}
%%    \,.
%%\end{gather}
%%  Then $\mathcal{G}$ is a class of measurable functions 
%%  and it holds
%%  \begin{gather}
%%    N_{[\,]}(\varepsilon,\mathcal{G}, L^2(\P))
%%    \le
%%    ??
%%    \qquad
%%    \text{for all}
%%    \ 
%%    \varepsilon>0
%%    \,.
%%  \end{gather}
%%\end{lemma}
%%\begin{lemma}
%%  \label{lemma_max_ineq}
%%  Consider a function class $\mathcal{F}$ with unit ball
%%  $
%%  B_{\mathcal{F}}
%%  :=
%%  \left\{ 
%%    f\in \mathcal{F}
%%    \colon
%%    \norm{f}_\infty
%%    \le
%%    1
%%  \right\}
%%  $.
%%  Let $(\varepsilon_N)$ be a sequence converging to 0
%%  and let 
%%  $
%%  \left( 
%%    \mathcal{F}_N
%%  \right)
%%    :=
%%    \left( 
%%      C\cdot
%%    \varepsilon_N\cdot B_\mathcal{F}
%%    \right)
%%  $
%%  denote the sequence of scaled unit balls in $\mathcal{F}$.
%%  Assume that 
%%  there exists
%%  $k<2$ such that 
%%  the covering number of the unit ball in $\mathcal{F}$
%%  satisfies
%%  \begin{gather}
%%        \log 
%%      N_{[\,]}
%%\left( \varepsilon, B_\mathcal{F}, L^2(\P) \right)
%%\ 
%%\lesssim
%%\ 
%%\left( \frac{1}{\varepsilon} \right)^k
%%\qquad
%%\text{for all}\ 
%%\varepsilon>0
%%\,.
%%  \end{gather}
%%  Then it holds
%%  $
%%      \norm{G_N}^*_{\mathcal{F}_N}
%%    \ 
%%    \overset{\P}
%%    {\to}
%%    \ 
%%    0
%%  $
%%  for $N\to \infty$. 
%%\end{lemma}
%%\begin{proof}
%%  By maximal inequalities it holds
%%  \begin{align*}
%%    \E^*
%%    \left[ 
%%      \norm{G_N}_{\mathcal{F}_N}
%%    \right]
%%    &
%%      \ 
%%      \lesssim
%%      \ 
%%      J_{[\,]}\left( \varepsilon_N, \mathcal{F}_N, L^2(\P) \right)
%%      \\
%%    &
%%      \ 
%%      =
%%      \ 
%%      \int_0^{\varepsilon_N}
%%      \sqrt{
%%        \log 
%%      N_{[\,]}
%%\left( \varepsilon, \mathcal{F}_N, L^2(\P) \right)
%%    }
%%    \,
%%    d\varepsilon
%%    \\
%%    &
%%    \ 
%%    =
%%    \ 
%%      \int_0^{\varepsilon_N}
%%      \sqrt{\log 
%%      N_{[\,]}
%%\left( \varepsilon/(C\cdot \varepsilon_N), B_\mathcal{F}, L^2(\P) \right)
%%    }
%%    \,
%%    d\varepsilon
%%    \\
%%    &
%%    \ 
%%    \lesssim
%%    \ 
%%      \int_0^{\varepsilon_N}
%%      \left( 
%%      \frac{\varepsilon_N}{\varepsilon}
%%    \right)^{k/2}
%%    \,
%%    d\varepsilon
%%    \\
%%    &
%%    \ 
%%    =
%%    \ 
%%  \varepsilon_N^{k/2}
%%  \frac{1}{1-k/2}
%%  \varepsilon_N^{1-k/2}
%%  \\
%%    &
%%    \ 
%%  \lesssim
%%    \ 
%%  \varepsilon_N
%%  \\
%%    &
%%    \ 
%%  \to
%%  \ 
%%  0
%%  \qquad
%%  \text{for}\ 
%%  N\to
%%  \infty
%%  \,.
%%  \end{align*}
%%  Note, that $k<2$.
%%  By the boundedness of $\E^*$ there is no measurability problem.
%%  By Markov's Inequality it holds
%%  \begin{align*}
%%    \P
%%    \left[ 
%%      \norm{G_N}^*_{\mathcal{F}_N}\ge \varepsilon
%%    \right]
%%    \le
%%    \varepsilon^{-1}
%%    \,
%%    \E^*
%%    \left[ 
%%      \norm{G_N}_{\mathcal{F}_N}
%%    \right]
%%    \ 
%%    \to
%%    \ 
%%    0
%%    \qquad
%%    \text{for}\ 
%%    N\to\infty
%%    \,.
%%  \end{align*}
%%\end{proof}
%%The next two lemmas connect $R_3$ to the theory of empirical processes.
%%\begin{lemma}
%%  \label{aa:mean:l:fz}
%%  Consider 
%%  the (random) function
%%  $
%%  f_D^z
%%  $ given by
%%  \begin{gather}
%%    f_{D}^z(T,X,Y(T))
%%    :=
%%    T
%%    \left( 
%%    w(D,X)- \frac{1}{\pi(X)}
%%    \right)
%%    \left( 
%%    \mathbf{1}{\left\{ Y(T) \le z \right\}}
%%    -
%%  F_{Y(1)}(z|X)
%%    \right)
%%    \,.
%%  \end{gather}
%%  Assume that 
%%  there exists a function class $\mathcal{F}$ satisfying the requirements of Lemma~\ref{lemma_max_ineq} and that
%%  $
%%  f_D^z
%%  \in \mathcal{F}
%%  $
%%  for all $z\in\R$ almost surely.
%%  It then holds
%%  $
%%  \sup_{z\in\R} \left| G_Nf_D^z \right|
%%  \overset{\P}{\to}0$ for 
%%  $N\to\infty$.
%%\end{lemma}
%%\begin{proof}
%%  By the consistency of the weights there exists a learning rate $(\varepsilon_N)$ such that
%%  \begin{gather}
%%    \P
%%    \left[ 
%%      \left| 
%%      w(X,D)
%%      -
%%      \frac{1}{\pi(X)}
%%      \right|
%%      \le
%%      \varepsilon_N
%%    \right]
%%    \to 1 
%%    \qquad
%%    \text{for}\ 
%%    N\to \infty
%%    \,.
%%  \end{gather}
%%  Let
%%  $\mathcal{F}_N:=\varepsilon_NB_\mathcal{F}$ as in 
%%  Lemma~\ref{lemma_max_ineq}.
%%  It holds
%%\begin{gather}
%%      \sup_{z\in\R}
%%      \left| 
%%    f_D^z
%%      \right|
%%      \lesssim
%%      \left| 
%%      w(X,D)
%%      -
%%      \frac{1}{\pi(X)}
%%      \right|
%%      \le
%%      \varepsilon_N
%%\end{gather}
%%with probability going to 1 as $N\to\infty$.
%%Thus
%% \begin{gather}
%%    \P
%%    \left[ 
%%    f_D^z
%%    \in
%%  \mathcal{F}_N
%%  \ \forall\,z\in\R
%%    \right]
%%    =
%%    \P
%%    \left[ 
%%      \sup_{z\in\R}
%%      \left| 
%%    f_D^z
%%      \right|
%%      \lesssim
%%      \varepsilon_N
%%    \right]
%%    \to 1
%%    \qquad
%%    \text{as}
%%    \ 
%%    N\to\infty
%%    \,.
%%  \end{gather}
%%  Then it holds
%%  for all $\varepsilon>0$ 
%%  \begin{align*}
%%    \P
%%    \left[ 
%%      \sup_{z\in\R}
%%  \left| G_Nf_D^z \right|
%%  \le
%%  \varepsilon
%%    \right]
%%    &
%%    \ 
%%    \ge
%%    \ 
%%    \P
%%    \left[ 
%%      \sup_{z\in\R}
%%  \left| G_Nf_D^z \right|
%%  \le
%%  \norm{G_N}^*_{\mathcal{F}_N}
%%  \le
%%  \varepsilon
%%    \right]
%%    \\
%%    &
%%    \ 
%%    \ge
%%    \ 
%%    \P
%%    \left[ 
%%    f_D^z
%%    \in
%%  \mathcal{F}_N
%%  \ \forall\,z\in\R
%%      \ 
%%      \text{and}\ 
%%  \norm{G_N}^*_{\mathcal{F}_N}
%%  \le
%%  \varepsilon
%%    \right]
%%    \\
%%    &
%%    \ 
%%    \ge
%%    \ 
%%    \P
%%    \left[ 
%%    f_D^z
%%    \in
%%  \mathcal{F}_N
%%  \ \forall\,z\in\R
%%    \right]
%%    -
%%    \P
%%    \left[ 
%%  \norm{G_N}^*_{\mathcal{F}_N}
%%    \ 
%%  \ge
%%    \ 
%%  \varepsilon
%%    \right]
%%    \\
%%    &
%%    \ 
%%    \to
%%    \ 
%%    1
%%    \,.
%%  \end{align*}
%%  The convergence of the second term is due to Lemma~\ref{lemma_max_ineq}.
%%
%%\end{proof}
%%
%\begin{lemma}
%  \label{aa:mean:l:r3}
%  Assume conditional unconfoundedness, that is,
%  \begin{gather}
%  (Y(0),Y(1))\perp T \ |\ X
%  \,.
%  \end{gather}
%  Then for all
%  $z\in\R$
%  it holds
%  $
%  G_Nf_z=R_3(z)
%  $.
%  Furthermore, under conditions it holds
%  $\sup_{z\in\R}\left| R_3(z) \right|\overset{\P}{\to}0$.
%\end{lemma}
%\begin{proof}
%  A standard computation shows
%  \begin{gather}
%    \E
%    \left[ 
%    \frac{T}{\pi(X)}
%    \left( 
%    \mathbf{1}{\left\{ Y(T) \le z \right\}}
%    -
%  F_{Y(1)}(z|X)
%    \right)
%    \right]
%    =0
%    \,.
%  \end{gather}
%  Furthermore
%  \begin{align*}
%    &
%    \E
%    \left[ 
%      Tw(X,D)
%    \left( 
%    \mathbf{1}{\left\{ Y(T) \le z \right\}}
%    -
%  F_{Y(1)}(z|X)
%    \right)
%    \right]
%    \\
%    &
%    \ 
%    =
%    \ 
%    \E
%    \left[ 
%      \E
%      \left[ 
%      w(X,D)
%      \left( 
%    \mathbf{1}{\left\{ Y(1) \le z \right\}}
%    -
%  F_{Y(1)}(z|X)
%      \right)
%  |T=1,X,D
%      \right]
%    \right]
%    \\
%    &
%    \ 
%    =
%    \ 
%    \E
%    \left[ 
%      w(X,D)
%      \E
%      \left[ 
%    \mathbf{1}{\left\{ Y(1) \le z \right\}}
%    -
%  F_{Y(1)}(z|X)
%  |X,D
%      \right]
%    \right]
%    \\
%    &
%    \ 
%    =
%    \ 
%    \E
%    \left[ 
%      w(X,D)
%      \E
%      \left[ 
%    \mathbf{1}{\left\{ Y(1) \le z \right\}}
%    -
%  F_{Y(1)}(z|X)
%  |X
%      \right]
%    \right]
%    \\
%    &
%    \ 
%    =
%    \ 
%    0
%  \end{align*}
%  The second equality is due to the assumption of 
%  $(Y(0),Y(1))\perp T |X$.
%  The third equality is due to $X\perp D$.
%  Thus
%  $
%    \E
%    f_D^z
%    =
%    0
%  $
%\end{proof}
%

The next lemma connects objective function $\varphi$, the set of basis functions $\mathfrak{B}$ and the propensity score.

\begin{lemma}
  Let Assumptions~\ref{asu:objective_f}, \ref{asu:ps} and \ref{asu:basis} hold true.
  Then for all
  $i\in \left\{ 1,\ldots,N \right\}$ it holds
  \begin{gather*}
   \left| 
   \sum_{k=1}^{\# \mathfrak{B}} 
   B_k(X_i)
   \cdot
   \varphi^{'}
   \left( 
     \frac{1}{\pi(X_k)}
   \right)
   \ 
   -
   \ 
   \varphi^{'}
   \left( 
     \frac{1}{\pi(X_i)}
   \right)
   \right| 
   \ 
   \to
   \ 
   0
   \qquad
   \text{for}
   \ 
   N\to\infty
   \,.
  \end{gather*}
\end{lemma}

\begin{proof}
  Consider the function 
  \begin{gather*}
  g
  \ 
  :=
  \ 
  \varphi^{'}
  \ 
  \circ 
  \ 
  (x\mapsto 1/x) 
  \ 
  \circ
  \ 
  \pi
  \,.
  \end{gather*}
  Then $g$ is continuously differentiable.
  This follows from the assumption that
  $\varphi$ is twice continuously differentiable on $(0,\infty)$, $\pi\in (0,1)$ and that $\pi$ is continuously differentiable on 
  $\R^d$.
  Thus, by the chain rule we get
  \begin{gather*}
   \nabla
   g
   \ 
   =
   \ 
   x
   \ 
   \mapsto
   \ 
   \frac{\nabla \pi(x)}{\pi^2(x)}
   \cdot
   \varphi^{''}\left( \frac{1}{\pi(x)} \right)
   \,.
  \end{gather*}
  Note, that $\nabla g$ is well defined, continuous on $\R^d$
  and does not depend on $N$.
  Since we assume
      $
      \sum_{k=1}^{\# B} 
      B_k(X_i)
      \ 
      =
      \ 
      1
      $
      for all 
      $i\in \left\{ 1,\ldots ,N \right\}$
      it holds
      \begin{align*}
   &
   \left| 
   \sum_{k=1}^{\# \mathfrak{B}} 
   B_k(X_i)
   \cdot
   \varphi^{'}
   \left( 
     \frac{1}{\pi(X_k)}
   \right)
   \ 
   -
   \ 
   \varphi^{'}
   \left( 
     \frac{1}{\pi(X_i)}
   \right)
   \right| 
   \\
   &
   \ 
   =
   \ 
   \left| 
   \sum_{k=1}^{\# \mathfrak{B}} 
   B_k(X_i)
   \left( 
   \varphi^{'}
   \left( 
     \frac{1}{\pi(X_k)}
   \right)
   \ 
   -
   \ 
   \varphi^{'}
   \left( 
     \frac{1}{\pi(X_i)}
   \right)
   \right)
   \right| 
\\
   &
   \ 
   \le
   \ 
   \sum_{k=1}^{\# \mathfrak{B}} 
   B_k(X_i)
   \left| 
   \left( 
   \varphi^{'}
   \left( 
     \frac{1}{\pi(X_k)}
   \right)
   \ 
   -
   \ 
   \varphi^{'}
   \left( 
     \frac{1}{\pi(X_i)}
   \right)
   \right)
   \right| 
\\
   &
   \ 
   \le
   \ 
   \sum_{k=1}^{\# \mathfrak{B}} 
   B_k(X_i)
   \sup_{\theta\in[0,1]}
   \norm{
     \nabla g
     (
     \theta X_k
     +
     (1-\theta)
     X_i
     )
   }_2
   \norm{X_k-X_i}_2
\\
   &
   \ 
   \lesssim
   \ 
   \sum_{k=1}^{\# \mathfrak{B}} 
   B_k(X_i)
   \norm{X_k-X_i}_2
   \ 
   \to
   \ 
   0
   \qquad
   \text{for}
   \ 
   N\to\infty
   \,.
      \end{align*}
\end{proof}


